Modeling Long- and Short-Term Temporal Patterns with Deep
Neural Networks
Guokun Lai
Carnegie Mellon University
guokun@cs.cmu.edu
Wei-Cheng Chang
Carnegie Mellon University
wchang2@andrew.cmu.edu
Yiming Yang
Carnegie Mellon University
yiming@cs.cmu.edu
Hanxiao Liu
Carnegie Mellon University
hanxiaol@cs.cmu.edu
ABSTRACT
Multivariate time series forecasting is an important machine learn-
ing problem across many domains, including predictions of solar
plant energy output, electricity consumption, and traffic jam situ-
ation. Temporal data arise in these real-world applications often
involves a mixture of long-term and short-term patterns, for which
traditional approaches such as Autoregressive models and Gaussian
Process may fail. In this paper, we proposed a novel deep learning
framework, namely Long- and Short-term Time-series network
(LSTNet), to address this open challenge. LSTNet uses the Convo-
lution Neural Network (CNN) and the Recurrent Neural Network
(RNN) to extract short-term local dependency patterns among vari-
ables and to discover long-term patterns for time series trends. Fur-
thermore, we leverage traditional autoregressive model to tackle
the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive
patterns, LSTNet achieved significant performance improvements
over that of several state-of-the-art baseline methods. All the data
and experiment codes are available online.
KEYWORDS
Multivariate Time Series, Neural Network, Autoregressive models
ACM Reference Format:
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Mod-
eling Long- and Short-Term Temporal Patterns with Deep Neural Networks.
In Proceedings of ACM Conference (SIGIR’18). ACM, New York, NY, USA,
11 pages. https://doi.org/10.475/123_4
1 INTRODUCTION
Multivariate time series data are ubiquitous in our everyday life
ranging from the prices in stock markets, the traffic flows on high-
ways, the outputs of solar power plants, the temperatures across
different cities, just to name a few. In such applications, users are
often interested in the forecasting of the new trends or potential
hazardous events based on historical observations on time series
signals. For instance, a better route plan could be devised based on
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR’18, July 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 123-4567-24-567/08/06. . . $15.00
https://doi.org/10.475/123_4
the predicted traffic jam patterns a few hours ahead, and a larger
profit could be made with the forecasting of the near-future stock
market.
Multivariate time series forecasting often faces a major research
challenge, that is, how to capture and leverage the dynamics depen-
dencies among multiple variables. Specifically, real-world applica-
tions often entail a mixture of short-term and long-term repeating
patterns, as shown in Figure 1 which plots the hourly occupancy
rate of a freeway. Apparently, there are two repeating patterns, daily
and weekly. The former portraits the morning peaks vs. evening
peaks, while the latter reflects the workday and weekend patterns.
A successful time series forecasting model should be capture both
kinds of recurring patterns for accurate predictions. As another
example, consider the task of predicting the output of a solar energy
farm based on the measured solar radiation by massive sensors over
different locations. The long-term patterns reflect the difference
between days vs. nights, summer vs. winter, etc., and the short-
term patterns reflect the effects of cloud movements, wind direction
changes, etc. Again, without taking both kinds of recurrent pat-
terns into account, accurate time series forecasting is not possible.
However, traditional approaches such as the large body of work in
autoregressive methods [2, 12, 22, 32, 35] fall short in this aspect, as
most of them do not distinguish the two kinds of patterns nor model
their interactions explicitly and dynamically. Addressing such limi-
tations of existing methods in time series forecasting is the main
focus of this paper, for which we propose a novel framework that
takes advantages of recent developments in deep learning research.
Deep neural networks have been intensively studied in related
domains, and made extraordinary impacts on the solutions of a
broad range of problems. The recurrent neural networks (RNN)
models [9], for example, have become most popular in recent natu-
ral language processing (NLP) research. Two variants of RNN in
particular, namely the Long Short Term Memory (LSTM) [15] and
the Gated Recurrent Unit (GRU) [6], have significantly improved
the state-of-the-art performance in machine translation, speech
recognition and other NLP tasks as they can effectively capture
the meanings of words based on the long-term and short-term
dependencies among them in input documents [1, 14, 19].In the
field of computer vision, as another example, convolution neural
network (CNN) models [19, 21] have shown outstanding perfor-
mance by successfully extracting local and shift-invariant features
(called "shapelets" sometimes) at various granularity levels from
input images.
ar
X
iv
:1
70
3.
07
01
5v
3 
 [
cs
.L
G
] 
 1
8 
A
pr
 2
01
8
SIGIR’18, July 2018, Ann Arbor, MI, USA Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu
Mon Tue Wes Thu Fri Sat Sun Mon Tue Wes Thu Fri Sat Sun
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
O
cc
u
p
a
n
cy
 r
a
te
(%
)
Figure 1: The hourly occupancy rate of a road in the bay area
for 2 weeks
Deep neural networks have also received an increasing amount
of attention in time series analysis. A substantial portion of the
previous work has been focusing on time series classification, i.e.,
the task of automated assignment of class labels to time series input.
For instance, RNN architectures have been studied for extracting
informative patterns from health-care sequential data [5, 23] and
classifying the data with respect diagnostic categories. RNN has
also been applied to mobile data, for classifying the input sequences
with respect to actions or activities [13]. CNNmodels have also been
used in action/activity recognition [13, 20, 31], for the extraction of
shift-invariant local patterns from input sequences as the features
of classification models.
Deep neural networks have also been studied for time series
forecasting [8, 33], i.e., the task of using observed time series in the
past to predict the unknown time series in a look-ahead horizon
– the larger the horizon, the harder the problem. Efforts in this
direction range from the early work using naive RNN models [7]
and the hybrid models [16, 34, 35] combining the use of ARIMA
[3] and Multilayer Perceptron (MLP), to the recent combination
of vanilla RNN and Dynamic Boltzmann Machines in time series
forecasting [8].
In this paper, we propose a deep learning framework designed for
the multivariate time series forecasting, namely Long- and Short-
term Time-series Network (LSTNet), as illustrated in Figure 2. It
leverages the strengths of both the convolutional layer to discover
the local dependency patterns among multi-dimensional input vari-
ables and the recurrent layer to captures complex long-term de-
pendencies. A novel recurrent structure, namely Recurrent-skip, is
designed for capturing very long-term dependence patterns and
making the optimization easier as it utilizes the periodic property
of the input time series signals. Finally, the LSTNet incorporates a
traditional autoregressive linear model in parallel to the non-linear
neural network part, which makes the non-linear deep learning
model more robust for the time series with violate scale changing.
In the experiment on the real world seasonal time series datasets,
our model consistently outperforms the traditional linear models
and GRU recurrent neural network.
The rest of this paper is organized as follows. Section 2 outlines
the related background, including representative auto-regressive
methods and Gaussian Process models. Section 3 describe our pro-
posed LSTNet. Section 4 reports the evaluation results of our model
in comparison with strong baselines on real-world datasets. Finally,
we conclude our findings in Section 5.
2 RELATED BACKGROUND
One of the most prominent univariate time series models is the
autoregressive integrated moving average (ARIMA) model. The
popularity of the ARIMA model is due to its statistical properties as
well as the well-known Box-Jenkins methodology [2] in the model
selection procedure. ARIMAmodels are not only adaptive to various
exponential smoothing techniques [25] but also flexible enough to
subsume other types of time series models including autoregression
(AR), moving average (MA) and Autoregressive Moving Average
(ARMA). However, ARIMA models, including their variants for
modeling long-term temporal dependencies [2], are rarely used in
high dimensional multivariate time series forecasting due to their
high computational cost.
On the other hand, vector autoregression (VAR) is arguably the
most widely used models in multivariate time series [2, 12, 24] due
to its simplicity. VAR models naturally extend AR models to the
multivariate setting, which ignores the dependencies between out-
put variables. Significant progress has been made in recent years
in a variety of VAR models, including the elliptical VAR model [27]
for heavy-tail time series and structured VAR model [26] for better
interpretations of the dependencies between high dimensional vari-
ables, and more. Nevertheless, the model capacity of VAR grows
linearly over the temporal window size and quadratically over the
number of variables. This implies, when dealing with long-term
temporal patterns, the inherited large model is prone to overfit-
ting. To alleviate this issue, [32] proposed to reduce the original
high dimensional signals into lower dimensional hidden represen-
tations, then applied VAR for forecasting with a variety choice of
regularization.
Time series forecasting problems can also be treated as standard
regression problems with time-varying parameters. It is therefore
not surprising that various regression models with different loss
functions and regularization terms are applied to time series fore-
casting tasks. For example, linear support vector regression (SVR)
[4, 17] learns a max margin hyperplane based on the regression loss
with a hyper-parameter ϵ controlling the threshold of prediction
errors. Ridge regression is yet another example which can be re-
covered from SVR models by setting ϵ to zeros. Lastly, [22] applied
LASSO models to encourage sparsity in the model parameters so
that interesting patterns among different input signals could be
manifest. These linear methods are practically more efficient for
multivariate time series forecasting due to high-quality off-the-shelf
solvers in the machine learning community. Nonetheless, like VARs,
those linear models may fail to capture complex non-linear relation-
ships of multivariate signals, resulting in an inferior performance
at the cost of its efficiency.
Gaussian Processes (GP) is a non-parametric method for mod-
eling distributions over a continuous domain of functions. This
contrasts with models defined by a parameterized class of functions
Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks SIGIR’18, July 2018, Ann Arbor, MI, USA
Time
Autoregresssive
Prediction
Multivariate Time series Convolutional Layer Recurrent and Recurrent-skip layer Fully connected and element-wise sum output
Linear 
Bypass
Figure 2: An overview of the Long- and Short-term Time-series network (LSTNet)
such as VARs and SVRs. GP can be applied to multivariate time se-
ries forecasting task as suggested in [28], and can be used as a prior
over the function space in Bayesian inference. For example, [10]
presented a fully Bayesian approach with the GP prior for nonlinear
state-space models, which is capable of capturing complex dynam-
ical phenomena. However, the power of Gaussian Process comes
with the price of high computation complexity. A straightforward
implementation of Gaussian Process for multivariate time-series
forecasting has cubic complexity over the number of observations,
due to the matrix inversion of the kernel matrix.
3 FRAMEWORK
In this section, we first formulate the time series forecasting prob-
lem, and then discuss the details of the proposed LSTNet archi-
tecture (Figure 2) in the following part. Finally, we introduce the
objective function and the optimization strategy.
3.1 Problem Formulation
In this paper, we are interested in the task of multivariate time series
forecasting. More formally, given a series of fully observed time
series signals Y = {y1,y2, . . . ,yT } where yt ∈ Rn , and n is the
variable dimension, we aim at predicting a series of future signals
in a rolling forecasting fashion. That being said, to predict yT+h
whereh is the desirable horizon ahead of the current time stamp, we
assume {y1,y2, . . . ,yT } are available. Likewise, to predict the value
of the next time stamp yT+h+1, we assume {y1,y2, . . . ,yT ,yT+1}
are available. We hence formulate the input matrix at time stampT
as XT = {y1,y2, . . . ,yT } ∈ Rn×T .
In the most of cases, the horizon of the forecasting task is chosen
according to the demands of the environmental settings, e.g. for the
traffic usage, the horizon of interest ranges from hours to a day; for
the stock market data, even seconds/minutes-ahead forecast can be
meaningful for generating returns.
Figure 2 presents an overview of the proposed LSTnet architec-
ture. The LSTNet is a deep learning framework specifically designed
for multivariate time series forecasting tasks with a mixture of long-
and short-term patterns. In following sections, we introduce the
building blocks for the LSTNet in detail.
3.2 Convolutional Component
The first layer of LSTNet is a convolutional network without pool-
ing, which aims to extract short-term patterns in the time dimension
as well as local dependencies between variables. The convolutional
layer consists of multiple filters of width ω and height n (the height
is set to be the same as the number of variables). The k-th filter
sweeps through the input matrix X and produces
hk = RELU (Wk ∗ X + bk ) (1)
where ∗ denotes the convolution operation and the outputhk would
be a vector, and the RELU function is RELU (x) = max(0,x). We
make each vector hk of length T by zero-padding on the left of
input matrix X . The output matrix of the convolutional layer is of
size dc ×T where dc denotes the number of filters.
3.3 Recurrent Component
The output of the convolutional layer is simultaneously fed into
the Recurrent component and Recurrent-skip component (to be de-
scribed in subsection 3.4). The Recurrent component is a recurrent
layer with the Gated Recurrent Unit (GRU) [6] and uses the RELU
function as the hidden update activation function. The hidden state
of recurrent units at time t is computed as,
rt = σ (xtWxr + ht−1Whr + br )
ut = σ (xtWxu + ht−1Whu + bu )
ct = RELU (xtWxc + rt ⊙ (ht−1Whc ) + bc )
ht = (1 − ut ) ⊙ ht−1 + ut ⊙ ct
(2)
SIGIR’18, July 2018, Ann Arbor, MI, USA Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu
where ⊙ is the element-wise product, σ is the sigmoid function
and xt is the input of this layer at time t . The output of this layer is
the hidden state at each time stamp. While researchers are accus-
tomed to using tanh function as hidden update activation function,
we empirically found RELU leads to more reliable performance,
through which the gradient is easier to back propagate.
3.4 Recurrent-skip Component
The Recurrent layers with GRU [6] and LSTM [15] unit are carefully
designed to memorize the historical information and hence to be
aware of relatively long-term dependencies. Due to gradient vanish-
ing, however, GRU and LSTM usually fail to capture very long-term
correlation in practice. We propose to alleviate this issue via a novel
recurrent-skip component which leverages the periodic pattern in
real-world sets. For instance, both the electricity consumption and
traffic usage exhibit clear pattern on a daily basis. If we want to
predict the electricity consumption at t o’clock for today, a classical
trick in the seasonal forecasting model is to leverage the records at t
o’clock in historical days, besides the most recent records. This type
of dependencies can hardly be captured by off-the-shelf recurrent
units due to the extremely long length of one period (24 hours) and
the subsequent optimization issues. Inspired by the effectiveness
of this trick, we develop a recurrent structure with temporal skip-
connections to extend the temporal span of the information flow
and hence to ease the optimization process. Specifically, skip-links
are added between the current hidden cell and the hidden cells in
the same phase in adjacent periods. The updating process can be
formulated as,
rt = σ (xtWxr + ht−pWhr + br )
ut = σ (xtWxu + ht−pWhu + bu )
ct = RELU (xtWxc + rt ⊙ (ht−pWhc ) + bc )
ht = (1 − ut ) ⊙ ht−p + ut ⊙ ct
(3)
where the input of this layer is the output of the convolutional
layer, and p is the number of hidden cells skipped through. The
value of p can be easily determined for datasets with clear periodic
patterns (e.g. p = 24 for the hourly electricity consumption and
traffic usage datasets), and has to be tuned otherwise. In our exper-
iments, we empirically found that a well-tuned p can considerably
boost the model performance even for the latter case. Furthermore,
the LSTNet could be easily extended to contain variants of the skip
length p.
We use a dense layer to combine the outputs of the Recurrent and
Recurrent-skip components. The inputs to the dense layer include
the hidden state of Recurrent component at time stamp t , denoted
by hRt , and p hidden states of Recurrent-skip component from time
stamp t − p + 1 to t denoted by hSt−p+1,h
S
t−p+2 . . . ,h
S
t . The output
of the dense layer is computed as,
hDt =W
RhRt +
p−1∑
i=0
W Si h
S
t−i + b (4)
where hDt is the prediction result of the neural network (upper)
part in the Fig.2 at time stamp t .
3.5 Temporal Attention Layer
However, the Recurrent-skip layer requires a predefined hyper-
parameter p, which is unfavorable in the nonseasonal time series
prediction, or whose period length is dynamic over time. To alle-
viate such issue, we consider an alternative approach, attention
mechanism [1], which learns the weighted combination of hidden
representations at each window position of the input matrix. Specif-
ically, the attention weights α t ∈ Rq at current time stamp t are
calculated as
α t = AttnScore(HRt ,hRt−1)
where HRt = [hRt−q , . . . ,hRt−1] is a matrix stacking the hidden repre-
sentation of RNN column-wisely and AttnScore is some similarity
functions such as dot product, cosine, or parameterized by a simple
multi-layer perceptron.
The final output of temporal attention layer is the concatenation
of the weighted context vector ct = Htα t and last window hidden
representation hRt−1, along with a linear projection operation
hDt =W [ct ;hRt−1] + b .
3.6 Autoregressive Component
Due to the non-linear nature of the Convolutional and Recurrent
components, one major drawback of the neural network model
is that the scale of outputs is not sensitive to the scale of inputs.
Unfortunately, in specific real datasets, the scale of input signals
constantly changes in a non-periodic manner, which significantly
lowers the forecasting accuracy of the neural network model. A
concrete example of this failure is given in Section 4.6. To address
this deficiency, similar in spirit to the highway network [29], we
decompose the final prediction of LSTNet into a linear part, which
primarily focuses on the local scaling issue, plus a non-linear part
containing recurring patterns. In the LSTNet architecture, we adopt
the classical Autoregressive (AR) model as the linear component.
Denote the forecasting result of the AR component as hLt ∈ Rn , and
the coefficients of the ARmodel asW ar ∈ Rqar andbar ∈ R, where
qar is the size of input window over the input matrix. Note that in
our model, all dimensions share the same set of linear parameters.
The AR model is formulated as follows,
hLt,i =
qar−1∑
k=0
W ark yt−k,i + b
ar (5)
The final prediction of LSTNet is then obtained by by integrating
the outputs of the neural network part and the AR component:
Ŷ t = h
D
t + h
L
t (6)
where Ŷ t denotes the model’s final prediction at time stamp t .
3.7 Objective function
The squared error is the default loss function for many forecasting
tasks, the corresponding optimization objective is formulated as,
minimize
Θ
∑
t ∈ΩT rain
| |Y t − Ŷ t−h | |2F (7)
where Θ denotes the parameter set of our model, ΩT rain is the
set of time stamps used for training, | | · | |F is the Frobenius norm,
and h is the horizon as mentioned in Section 3.1. The traditional
Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks SIGIR’18, July 2018, Ann Arbor, MI, USA
linear regression model with the square loss function is named
as Linear Ridge, which is equivalent to the vector autoregressive
model with ridge regularization. However, experiments show that
the Linear Support Vector Regression (Linear SVR) [30] dominates
the Linear Ridge model in certain datasets. The only difference
between Linear SVR and Linear Ridge is the objective function. The
objective function for Linear SVR is,
minimize
Θ
1
2
| |Θ| |2F +C
∑
t ∈ΩT rain
n−1∑
i=0
ξt,i
subject to |Ŷ t−h,i −Y t,i | ≤ ξt,i + ϵ, t ∈ ΩT rain
ξt,i ≥ 0
(8)
where C and ϵ are hyper-parameters. Motivated by the remarkable
performance of the Linear SVR model, we incorporate its objective
function in the LSTNet model as an alternative of the squared loss.
For simplicity, we assume ϵ = 01, and the objective function above
reduces to absolute loss (L1-loss) function as follows:
minimize
Θ
∑
t ∈ΩT rain
n−1∑
i=0
|Y t,i − Ŷ t−h,i | (9)
The advantage of the absolute loss function is that it is more
robust to the anomaly in the real time series data. In the experiment
section, we use the validation set to decide to use which objective
function, square loss Eq.7 or absolute one Eq.9.
3.8 Optimization Strategy
In this paper, our optimization strategy is the same as that in
the traditional time series forecasting model. Supposing the in-
put time series is Y t = {y1,y2, . . . ,yt }, we define a tunable win-
dow size q, and reformulate the input at time stamp t as X t =
{yt−q+1,yt−q+2, . . . ,yt }. The problem then becomes a regression
task with a set of feature-value pairs {X t ,Y t+h }, and can be solved
by Stochastic Gradient Decent (SGD) or its variants such as Adam
[18].
4 EVALUATION
We conducted extensive experiments with 9 methods (including our
new methods) on 4 benchmark datasets for time series forecasting
tasks. All the data and experiment codes are available online 2.
4.1 Methods for Comparison
The methods in our comparative evaluation are the follows.
• AR stands for the autoregressive model, which is equivalent
to the one dimensional VAR model.
• LRidge is the vector autoregression (VAR) model with L2-
regularization, which has been most popular for multivariate
time series forecasting.
• LSVR is the vector autoregression (VAR) model with Support
Vector Regression objective function [30] .
• TRMF is the autoregressive model using temporal regular-
ized matrix factorization by [32].
1One could keep ϵ to make the objective function more faithful to the Linear SVR
model without modifying the optimization strategy. We leave this for future study.
2the link is anonymous due to the double blind policy
• GP is the Gaussian Process for time series modeling. [11, 28]
• VAR-MLP is the model proposed in [35] that combines Mul-
tilayer Perception (MLP) and autoregressive model.
• RNN-GRU is the Recurrent Neural Network model using
GRU cell.
• LSTNet-skip is our proposed LSTNet model with skip-RNN
layer.
• LSTNet-Attn is our proposed LSTNet model with temporal
attention layer.
For the single output methods above such as AR, LRidge, LSVR and
GP, we just trained n models independently, i.e., one model for each
of the n output variables.
4.2 Metrics
We used three conventional evaluation metrics defined as:
• Root Relative Squared Error (RSE):
RSE =
√∑
(i,t )∈ΩT est (Yit − Ŷit )2√∑
(i,t )∈ΩT est (Yit −mean(Y ))2
(10)
• Empirical Correlation Coefficient (CORR)
CORR =
1
n
n∑
i=1
∑
t
(
Yit −mean(Y i )
) (
Ŷit −mean(Ŷ i )
)√∑
t
(
Yit −mean(Y i )
)2 (
Ŷit −mean(Ŷ i )
)2 (11)
where Y , Ŷ ∈ Rn×T are ground true signals and system prediction
signals, respectively. The RSE are the scaled version of the widely
used Root Mean Square Error(RMSE), which is design to make more
readable evaluation, regardless the data scale. For RSE lower value
is better, while for CORR higher value is better.
4.3 Data
We used four benchmark datasets which are publicly available.
Table 1 summarizes the corpus statistics.
• Traffic3: A collection of 48 months (2015-2016) hourly data
from the California Department of Transportation. The data
describes the road occupancy rates (between 0 and 1) mea-
sured by different sensors on San Francisco Bay area free-
ways.
• Solar-Energy4 : the solar power production records in the
year of 2006, which is sampled every 10 minutes from 137
PV plants in Alabama State.
• Electricity5: The electricity consumption in kWhwas recorded
every 15 minutes from 2012 to 2014, for n = 321 clients. We
converted the data to reflect hourly consumption;
• Exchange-Rate: the collection of the daily exchange rates of
eight foreign countries including Australia, British, Canada,
Switzerland, China, Japan, New Zealand and Singapore rang-
ing from 1990 to 2016.
All datasets have been split into training set (60%), validation set
(20%) and test set (20%) in chronological order. To facilitate future
research in multivariate time series forecasting, we publicize all
raw datasets and the one after preprocessing in the website.
3http://pems.dot.ca.gov
4http://www.nrel.gov/grid/solar-power-data.html
5https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
SIGIR’18, July 2018, Ann Arbor, MI, USA Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu
Datasets T D L
Traffic 17,544 862 1 hour
Solar-Energy 52,560 137 10 minutes
Electricity 26,304 321 1 hour
Exchange-Rate 7,588 8 1 day
Table 1: Dataset Statistics, whereT is length of time series,D
is number of variables, L is the sample rate.
In order to examine the existence of long-term and/or short-
term repetitive patterns in time series data, we plot autocorrelation
graph for some randomly selected variables from the four datasets
in Figure 3. Autocorrelation, also known as serial correlation, is the
correlation of a signal with a delayed copy of itself as a function of
delay defined below
R(τ ) = E[(Xt − µ)(Xt+τ − µ)]
σ 2
where Xt is the time series signals, µ is mean and σ 2 is variance. In
practice, we consider the empirical unbiased estimator to calculate
the autocorrelation.
We can see in the graphs (a), (b) and (c) of Figure 3, there are repet-
itive patterns with high autocorrelation in the Traffic, Solar-Energy
and Electricity datasets, but not in the Exchange-Rate dataset. Fur-
thermore, we can observe a short-term daily pattern (in every 24
hours) and long-term weekly pattern (in every 7 days) in the graph
of the Traffic and Electricity dataset, which perfect reflect the ex-
pected regularity in highway traffic situations and electricity con-
sumptions. On the other hand, in graph (d) of the Exchange-Rate
dataset, we hardly see any repetitive long-term patterns, expect
some short-term local continuity. These observations are important
for our later analysis on the empirical results of different methods.
That is, for the methods which can properly model and success-
fully leverage both short-term and long-term repetitive patterns
in data, they should outperform well when the data contain such
repetitive patterns (like in Electricity, Traffic and Solar-Energy).
On the other hand, if the dataset does not contain such patterns
(like in Exchange-Rate), the advantageous power of those methods
may not lead a better performance than that of other less powerful
methods. We will revisit this point in Section 4.7 with empirical
justifications.
4.4 Experimental Details
We conduct grid search over all tunable hyper-parameters on the
held-out validation set for each method and dataset. Specifically,
all methods share the same grid search range of the window size q
ranging from {20, 21, . . . , 29} if applied. For LRidge and LSVR, the
regularization coefficient λ is chosen from {2−10, 2−8, . . . , 28, 210}.
For GP, the RBF kernel bandwidth σ and the noise level α are cho-
sen from {2−10, 2−8, . . . , 28, 210}. For TRMF, the hidden dimension
is chosen from {22, . . . , 26} and the regularization coefficient λ is
chosen from {0.1, 1, 10}. For LST-Skip and LST-Attn, we adopted the
training strategy described in Section 3.8. The hidden dimension of
the Recurrent and Convolutional layer is chosen from {50, 100, 200},
and {20, 50, 100} for Recurrent-skip layer. The skip-length p of
(a) Traffic dataset (b) Solar-Energy dataset
(c) Electricity dataset (d) Exchange-Rate dataset
Figure 3: Autocorrelation graphs of sampled variables form
four datasets.
Recurrent-skip layer is set as 24 for the Traffic and Electricity
dataset, and tuned range from 21 to 26 for the Solar-Energy and
Exchange-Rate datasets. The regularization coefficient of the AR
component is chosen from {0.1, 1, 10} to achieve the best perfor-
mance. We perform dropout after each layer, except input and
output ones, and the rate usually is set to 0.1 or 0.2. The Adam[18]
algorithm is utilized to optimize the parameters of our model.
4.5 Main Results
Table 2 summarizes the evaluation results of all the methods (8)
on all the test sets (4) in all the metrics (3). We set horizon =
{3, 6, 12, 24}, respectively, which means the horizons was set from 3
to 24 hours for the forecasting over the Electricity and Traffic data,
from 30 to 240 minutes over the Solar-Energy data, and from 3 to
24 days over the Exchange-Rate data. The larger the horizons, the
harder the prediction tasks. The best result for each (data, metric)
pair is highlighted in bold face in this table. The total count of the
bold-faced results is 17 for LSTNet-Skip (one version of the pro-
posed LSTNet), 7 for LSTNet-Attn (the other version of our LSTNet),
and between 0 to 3 for the rest of the methods.
Clearly, the two proposed models, LSTNet-skip and LSTNet-Attn,
consistently enhance over state-of-the-art on the datasets with pe-
riodic pattern, especially in the settings of large horizons. Besides,
LSTNet outperforms the strong neural baseline RNN-GRU by 9.2%,
11.7%, 22.2% in RSE metric on Solar-Energy, Traffic and Electricity
dataset respectively when the horizon is 24, demonstrating the
effectiveness of the framework design for complex repetitive pat-
terns. What’s more, when the periodic pattern q is not clear from
applications, users may consider LSTNet-attn as alternative over
LSTNet-skip, given the former still yield considerable improvement
over the baselines. But the proposed LSTNet is slightly worse than
AR and LRidge on the Exchange-Rate dataset. Why? Recall that in
Section 4.3 and Figure 3 we used the autocorrelation curves of these
Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks SIGIR’18, July 2018, Ann Arbor, MI, USA
Dataset Solar-Energy Traffic Electricity Exchange-Rate
Horizon Horizon Horizon Horizon
Methods Metrics 3 6 12 24 3 6 12 24 3 6 12 24 3 6 12 24
AR RSE 0.2435 0.3790 0.5911 0.8699 0.5991 0.6218 0.6252 0.6293 0.0995 0.1035 0.1050 0.1054 0.0228 0.0279 0.0353 0.0445
(3) CORR 0.9710 0.9263 0.8107 0.5314 0.7752 0.7568 0.7544 0.7519 0.8845 0.8632 0.8591 0.8595 0.9734 0.9656 0.9526 0.9357
LRidge RSE 0.2019 0.2954 0.4832 0.7287 0.5833 0.5920 0.6148 0.6025 0.1467 0.1419 0.2129 0.1280 0.0184 0.0274 0.0419 0.0675
(3) CORR 0.9807 0.9568 0.8765 0.6803 0.8038 0.8051 0.7879 0.7862 0.8890 0.8594 0.8003 0.8806 0.9788 0.9722 0.9543 0.9305
LSVR RSE 0.2021 0.2999 0.4846 0.7300 0.5740 0.6580 0.7714 0.5909 0.1523 0.1372 0.1333 0.1180 0.0189 0.0284 0.0425 0.0662
(1) CORR 0.9807 0.9562 0.8764 0.6789 0.7993 0.7267 0.6711 0.7850 0.8888 0.8861 0.8961 0.8891 0.9782 0.9697 0.9546 0.9370
TRMF RSE 0.2473 0.3470 0.5597 0.9005 0.6708 0.6261 0.5956 0.6442 0.1802 0.2039 0.2186 0.3656 0.0351 0.0875 0.0494 0.0563
(0) CORR 0.9703 0.9418 0.8475 0.5598 0.6964 0.7430 0.7748 0.7278 0.8538 0.8424 0.8304 0.7471 0.9142 0.8123 0.8993 0.8678
GP RSE 0.2259 0.3286 0.5200 0.7973 0.6082 0.6772 0.6406 0.5995 0.1500 0.1907 0.1621 0.1273 0.0239 0.0272 0.0394 0.0580
(1) CORR 0.9751 0.9448 0.8518 0.5971 0.7831 0.7406 0.7671 0.7909 0.8670 0.8334 0.8394 0.8818 0.8713 0.8193 0.8484 0.8278
VARMLP RSE 0.1922 0.2679 0.4244 0.6841 0.5582 0.6579 0.6023 0.6146 0.1393 0.1620 0.1557 0.1274 0.0265 0.0304 0.0407 0.0578
(0) CORR 0.9829 0.9655 0.9058 0.7149 0.8245 0.7695 0.7929 0.7891 0.8708 0.8389 0.8192 0.8679 0.8609 0.8725 0.8280 0.7675
RNN-GRU RSE 0.1932 0.2628 0.4163 0.4852 0.5358 0.5522 0.5562 0.5633 0.1102 0.1144 0.1183 0.1295 0.0192 0.0264 0.0408 0.0626
(0) CORR 0.9823 0.9675 0.9150 0.8823 0.8511 0.8405 0.8345 0.8300 0.8597 0.8623 0.8472 0.8651 0.9786 0.9712 0.9531 0.9223
LST-Skip RSE 0.1843 0.2559 0.3254 0.4643 0.4777 0.4893 0.4950 0.4973 0.0864 0.0931 0.1007 0.1007 0.0226 0.0280 0.0356 0.0449
(17) CORR 0.9843 0.9690 0.9467 0.8870 0.8721 0.8690 0.8614 0.8588 0.9283 0.9135 0.9077 0.9119 0.9735 0.9658 0.9511 0.9354
LST-Attn RSE 0.1816 0.2538 0.3466 0.4403 0.4897 0.4973 0.5173 0.5300 0.0868 0.0953 0.0984 0.1059 0.0276 0.0321 0.0448 0.0590
(7) CORR 0.9848 0.9696 0.9397 0.8995 0.8704 0.8669 0.8540 0.8429 0.9243 0.9095 0.9030 0.9025 0.9717 0.9656 0.9499 0.9339
Table 2: Results summary (in RSE and CORR) of all methods on four datasets: 1) each row has the results of a specific method
in a particular metric; 2) each column compares the results of all methods on a particular dataset with a specific horizon value;
3) bold face indicates the best result of each column in a particular metric; and 4) the total number of bold-faced results of
each method is listed under the method name within parentheses.
datasets to show the existence of repetitive patterns in the Solar-
Energy, Traffic and Electricity datasets but not in Exchange-Rate.
The current results provide empirical evidence for the success of
LSTNet models in modeling long-term and short-term dependency
patterns when they do occur in data. Otherwise, LSTNet performed
comparably with the better ones (AR and LRidge) among the repre-
sentative baselines.
Compared the results of univariate AR with that of the multivari-
ate baseline methods (LRidge, LSVR and RNN), we see that in some
datasets, i.e. Solar-Energy and Traffic, the multivariate approaches
is stronger, but weaker otherwise, which means that the richer
input information would causes overfitting in the traditional multi-
variate approaches. In contrast, the LSTNet has robust performance
in different situations, partly due to its autoregressive component,
which we will discuss further in Section 4.6.
4.6 Ablation Study
To demonstrate the efficiency of our framework design, a careful
ablation study is conducted. Specifically, we remove each compo-
nent one at a time in our LSTNet framework. First, we name the
LSTNet without different components as follows.
• LSTw/oskip: The LSTNet models without the Recurrent-
skip component and attention component.
• LSTw/oCNN: The LSTNet-skip models without the Convo-
lutional component.
• LSTw/oAR: The LSTNet-skip models without the AR com-
ponent.
For different baselines, we tune the hidden dimension of models
such that they have similar numbers of model parameters to the
completed LSTNet model, removing the performance gain induced
by model complexity.
The test results measured using RSE and CORR are shown in
Figure 56. Several observations from these results are worth high-
lighting:
• The best result on each dataset is obtained with either LST-
Skip or LST-Attn.
• Removing the AR component (in LSTw/oAR) from the full
model caused the most significant performance drops on
most of the datasets, showing the crucial role of the AR
component in general.
• Removing the Skip and CNN components in (LSTw/oCNN or
LSTw/oskip) caused big performance drops on some datasets
but not all. All the components of LSTNet together leads to
the robust performance of our approach on all the datasets.
The conclusion is that our architecture design is most robust
across all experiment settings, especially with the large horizons.
As for why the AR component would have such an important
role, our interpretation is that AR is generally robust to the scale
changing in data. To empirically validate this intuition we plot one
dimension (one variable) of the time series signals in the electricity
consumption dataset for the duration from 1 to 5000 hours in Figure
6, where the blue curve is the true data and the red curve is the
system-forecasted signals. We can see that the true consumption
suddenly increases around the 1000th hour, and that LSTNet-Skip
successfully captures this sudden change but LSTw/oAR fails to
react properly.
6We omit the results in RAE as it shows similar comparison with respect to the relative
performance among the methods.
SIGIR’18, July 2018, Ann Arbor, MI, USA Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu
Figure 4: Simulation Test: Left side is the training set and
right side is test set.
In order to better verify this assumption, we conduct a simulation
experiment. First, we randomly generate an autoregressive process
with the scale changing by the following steps. Firstly, we randomly
sample a vector, w ∼ N (0, I ),w ∈ Rp , where p is a given window
size. Then the generated autoregressive process xt can be described
as
xt =
p∑
i=1
wixt−i + ϵ (12)
where ϵ ∼ N (µ, 1). To inject the scale changing, we increase the
mean of Gaussian noise by µ0 everyT timestamp. Then the Gaussian
noise of time series xt can be written as
ϵ ∼ N (⌊t/T ⌋ µ0, 1) (13)
where the ⌊·⌋ denotes the floor function. We split the time series as
the training set and test in chronological order, and test the RNN-
GRU and the LSTNet models. The result is illustrated in Figure
4. Both RNN and LSTNet can memorize the pattern in training
set (left side). But, the RNN-GRU model cannot follow the scale
changing pattern in the test set (right side). Oppositely, the LSTNet
model fits the test set much better. In other words, the normal
RNN module, or says the neural-network component in LSTNet,
may not be sufficiently sensitive to violated scale fluctuations in
data (which is typical in Electricity data possibly due to random
events for public holidays or temperature turbulence, etc.), while
the simple linear AR model can make a proper adjustment in the
forecasting.
In summary, this ablation study clearly justifies the efficiency of
our architecture design. All components have contributed to the
excellent and robust performance of LSTNet.
4.7 Mixture of long- and short-term patterns
To illustrate the success of LSTNet in modeling the mixture of
short-term and long-term recurring patterns in time series data,
Figure 7 compares the performance of LSTNet and VAR on an
specific time series (one of the output variables) in the Traffic dataset.
As discussed in Section 4.3, the Traffic data exhibit two kinds of
repeating patterns, i.e. the daily ones and the weekly ones. We can
see in Figure 7 that the true patterns (in blue) of traffic occupancy
are very different on Fridays and Saturdays, and another on Sunday
and Monday. The Figure 7 is the prediction result of the VAR model
(part (a)) and LSTNet (part (b)) of a traffic flow monitor sensor,
where their hyper-parameters are chosen according to the RMSE
result on the validation set. The figure shows that the VAR model
is only capable to deal with the short-term patterns. The pattern
of prediction results of the VAR model only depend on the day
before the predictions. We can clearly see that the results of it in
Saturday (2rd and 9th peaks) and Monday (4th and 11th peaks) is
different from the ground truth, where the ground truth of Monday
(weekday) has two peaks, one peak for Saturday (weekend). In
the contrary, our proposed LSTNet model performs two patterns
for weekdays and weekends respectfully. This example proves the
ability of LSTNet model to memorize short-term and long-term
recurring patterns simultaneously, which the traditional forecasting
model is not equipped, and it is crucial in the prediction task of the
real world time series signals.
5 CONCLUSION
In this paper, we presented a novel deep learning framework (LST-
Net) for the task of multivariate time series forecasting. By combin-
ing the strengths of convolutional and recurrent neural networks
and an autoregressive component, the proposed approach signifi-
cantly improved the state-of-the-art results in time series forecast-
ing on multiple benchmark datasets. With in-depth analysis and
empirical evidence, we show the efficiency of the architecture of
LSTNet model, and that it indeed successfully captures both short-
term and long-term repeating patterns in data, and combines both
linear and non-linear models for robust prediction.
For future research, there are several promising directions in
extending the work. Firstly, the skip length p of the skip-recurrent
layer is a crucial hyper-parameter. Currently, we manually tune
it based on the validation dataset. How to automatically choose
p according to data is an interesting problem. Secondly, in the
convolution layer we treat each variable dimension equally, but in
the real world dataset, we usually have rich attribute information.
Integrating them into the LSTNet model is another challenging
problem.
REFERENCES
[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
[2] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time series analysis:
forecasting and control. John Wiley & Sons, 2015.
[3] G. E. Box and D. A. Pierce. Distribution of residual autocorrelations in
autoregressive-integrated moving average time series models. Journal of the
American statistical Association, 65(332):1509–1526, 1970.
[4] L.-J. Cao and F. E. H. Tay. Support vector machine with adaptive parameters in
financial time series forecasting. IEEE Transactions on neural networks, 14(6):1506–
1518, 2003.
[5] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks
for multivariate time series with missing values. arXiv preprint arXiv:1606.01865,
2016.
[6] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated
recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555,
2014.
[7] J. Connor, L. E. Atlas, and D. R. Martin. Recurrent networks and narma modeling.
In NIPS, pages 301–308, 1991.
[8] S. Dasgupta and T. Osogami. Nonlinear dynamic boltzmann machines for time-
series prediction. AAAI-17. Extended research report available at goo. gl/Vd0wna,
2016.
[9] J. L. Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.
[10] R. Frigola, F. Lindsten, T. B. Schön, and C. E. Rasmussen. Bayesian inference and
learning in gaussian process state-space models with particle mcmc. In Advances
Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks SIGIR’18, July 2018, Ann Arbor, MI, USA
3 6 12 24
Horizon
0.1
0.2
0.3
0.4
0.5
0.6
R
S
E
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
3 6 12 24
Horizon
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
C
o
rr
e
la
ti
o
n
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
(a) Solar-Energy dataset
3 6 12 24
Horizon
0.46
0.48
0.50
0.52
0.54
0.56
R
S
E
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
3 6 12 24
Horizon
0.82
0.83
0.84
0.85
0.86
0.87
C
o
rr
e
la
ti
o
n
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
(b) Traffic dataset
3 6 12 24
Horizon
0.080
0.085
0.090
0.095
0.100
0.105
0.110
0.115
0.120
R
S
E
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
3 6 12 24
Horizon
0.84
0.86
0.88
0.90
0.92
C
o
rr
e
la
ti
o
n
LSTw/oskip
LSTw/oCNN
LSTw/oAR
LSTNet-skip
LSTNet-attn
(c) Electricity dataset
Figure 5: Results of LSTNet in the ablation tests on the Solar-Energy, Traffic and Electricity dataset
in Neural Information Processing Systems, pages 3156–3164, 2013.
[11] R. Frigola-Alcade. Bayesian Time Series Learning with Gaussian Processes. PhD
thesis, PhD thesis, University of Cambridge, 2015.
[12] J. D. Hamilton. Time series analysis, volume 2. Princeton university press
Princeton, 1994.
[13] N. Y. Hammerla, S. Halloran, and T. Ploetz. Deep, convolutional, and recur-
rent models for human activity recognition using wearables. arXiv preprint
arXiv:1604.08880, 2016.
[14] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Van-
houcke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic
modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 29(6):82–97, 2012.
[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997.
SIGIR’18, July 2018, Ann Arbor, MI, USA Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu
0 1000 2000 3000 4000 5000
Time(hours)
20
40
60
80
100
120
140
160
180
200
E
le
ct
ri
ci
ty
 C
o
n
su
m
p
ti
o
n
(k
w
h
) data
forecast
(a)
0 1000 2000 3000 4000 5000
Time(hours)
20
40
60
80
100
120
140
160
180
200
E
le
ct
ri
ci
ty
 C
o
n
su
m
p
ti
o
n
(k
w
h
) data
forecast
(b)
Figure 6: The predicted time series (red) by LSTw/oAR (a) and by LST-Skip (b) vs. the true data (blue) on Electricity dataset with
horizon = 24
Fri Sat Sun Mon Tue Wes Thu Fri Sat Sun Mon Tue
0.00
0.05
0.10
0.15
0.20
O
cc
u
p
a
n
cy
 r
a
te
(%
)
data
forecast
(a)
Fri Sat Sun Mon Tue Wes Thu Fri Sat Sun Mon Tue
0.00
0.05
0.10
0.15
0.20
O
cc
u
p
a
n
cy
 r
a
te
(%
)
data
forecast
(b)
Figure 7: The true time series (blue) and the predicted ones (red) by VAR (a) and by LSTNet (b) for one variable in the Traffic
occupation dataset. The X axis indicates the week days and the forecasting horizon = 24. VAR inadequately predicts similar
patterns for Fridays and Saturdays, and ones for Sundays andMondays, while LSTNet successfully captures both the daily and
weekly repeating patterns.
[16] A. Jain and A. M. Kumar. Hybrid neural network models for hydrologic time
series forecasting. Applied Soft Computing, 7(2):585–592, 2007.
[17] K.-j. Kim. Financial time series forecasting using support vector machines.
Neurocomputing, 55(1):307–319, 2003.
[18] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep
convolutional neural networks. In Advances in neural information processing
systems, pages 1097–1105, 2012.
[20] C. Lea, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networks:
A unified approach to action segmentation. In Computer Vision–ECCV 2016
Workshops, pages 47–54. Springer, 2016.
[21] Y. LeCun and Y. Bengio. Convolutional networks for images, speech, and time
series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.
[22] J. Li and W. Chen. Forecasting macroeconomic time series: Lasso-based ap-
proaches and their forecast combinations with dynamic factor models. Interna-
tional Journal of Forecasting, 30(4):996–1015, 2014.
[23] Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzell. Learning to diagnose with lstm
recurrent neural networks. arXiv preprint arXiv:1511.03677, 2015.
[24] H. Lütkepohl. New introduction to multiple time series analysis. Springer Science
& Business Media, 2005.
[25] E. McKenzie. General exponential smoothing and the equivalent arma process.
Journal of Forecasting, 3(3):333–344, 1984.
[26] I. Melnyk and A. Banerjee. Estimating structured vector autoregressive model.
arXiv preprint arXiv:1602.06606, 2016.
[27] H. Qiu, S. Xu, F. Han, H. Liu, and B. Caffo. Robust estimation of transition
matrices in high dimensional heavy-tailed vector autoregressive processes. In
Proceedings of the 32nd International Conference on Machine L
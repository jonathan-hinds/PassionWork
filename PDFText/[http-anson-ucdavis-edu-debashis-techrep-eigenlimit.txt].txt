Asymptotics of the leading sample eigenvalues for a spiked covariance model
Debashis Paul
Department of Statistics, Stanford University
390 Serra Mall, Stanford, CA 94305
debashis@stat.stanford.edu
Abstract : We consider a multivariate Gaussian observation model where the covariance matrix is
diagonal and the diagonal entries are all equal to one except for a finite number which are bigger. We address
the question of asymptotic behaviour of the eigenvalues of the sample covariance matrix when the sample
size and the dimension of the observations both grow to infinity in such a way that their ratio converges to a
positive constant. We establish almost sure limits of the largest few sample eigenvalues. We also show that
when a population eigenvalue is above a certain threshold and of multiplicity one, the corresponding sample
eigenvalue has a Gaussian limiting distribution. We also demonstrate a phase transition phenomenon of the
sample eigenvectors in the same setting.
Keywords : Principal component analysis, Eigenvalue distribution, Random matrix theory.
1 Introduction
Study of eigenvalues of sample covariance matrices has a long history. When the dimension N is
fixed, the distributional aspects for both Gaussian and non-Gaussian observations have been dealt
with at length by various authors. Anderson (1963), Muirhead (1982) and Tyler (1983) are among
standard references. In fixed dimension scenario much of the study of the eigenstructure of sample
covariance matrix utilizes the fact that it is a good approximation of the population covariance
matrix when sample size is large. However this is no longer the case when Nn â†’ Î³ âˆˆ (0,âˆ) as
n â†’ âˆ, where n is the sample size. Under these circustances it is known (see Bai (1999) for a
review) that, if the true covariance is the identity matrix, then the Empirical Spectral Distribution
(ESD) converges almost surely to the Marchenko-Pastur distribution, henceforth denoted by FÎ³ .
When Î³ â‰¤ 1, the support FÎ³ is the set [(1 âˆ’
âˆš
Î³)2, (1 +
âˆš
Î³)2] and when Î³ > 1 an isolated point
zero is added to the support. It is known (Bai and Yin, 1993) that when the population covariance
is identity, the largest and the smallest eigenvalues, when Î³ â‰¤ 1, converge almost surely to the
respective boundaries of the support of FÎ³ . Johnstone (2001) and Soshnikov (2002) have derived
asymptotic distribution for largest, second largest etc sample eigenvalues under the same setting.
However, in recent years researchers in various fields have been using different versions of non-
identity covariance matrices of growing dimension. Among these, a particularly interesting model
is when all except a few of the eigenvalues equal one and the few that are not are well-separated
1
from the rest. This has been referred to as a â€œspiked population modelâ€ by Johnstone (2001). It
has also been observed that for certain types of data e.g. in speech recognition (Buja et al., 1995),
wireless communication (Telatar, 1999), statistical learning (Hoyle and Rattray, 2003, 2004), a few
of the sample eigenvalues have limiting behaviour that is different from the behaviour under identity
covariance scenario. This paper attempts to contribute towards understanding these phenomena.
The literature on the asymptotics of sample eigenvalues for the non-identity covariance scenario
is relatively recent. Silverstein and Choi (1995) derived almost sure limit of the ESD under fairly
general conditions. Bai and Silverstein (2004) derived the asymptotic distribution of certain linear
spectral statistics. However, a systematic study of the individual eigenvalues has been conducted
only recently by PeÌcheÌ (2003), Baik, Ben Arous and PeÌcheÌ (2004) (henceforth Baik et al., 2004).
These authors deal with the situation when the observations are complex Gaussian and the co-
variance matrix is a finite rank perturbation of identity. When this paper was being written the
author came to know about the work by Baik and Silverstein (2004), which studies the almost sure
limits of sample eigenvalues, when the observations are either real or complex, and under fairly
weak distributional assumptions. They give almost sure limits of the M largest and M smallest
(non-zero) sample eigenvalues where M is the number of non-unit population eigenvalues.
A crucial aspect of the work of last three sets of authors is the discovery of a phase transition
phenomenon. Simply put, if the non-unit eigenvalues are close to one, then their sample versions
will behave in roughly the same way as if the true covariance were identity. However, when the true
eigenvalues are larger than 1 +
âˆš
Î³, the sample eigenvalues have a different asymptotic property.
The results of Baik et al. (2004) show a n2/3 scaling for the asymptotic distribution when a non-
unit population eigenvalue lies below the threshold 1 +
âˆš
Î³, and a n1/2 scaling for those above that
threshold.
In this paper we focus our attention on the case where we have independently and identically
distributed real-valued observations X1, . . . , Xn from an N -variate normal distribution with mean
zero and covariance matrix Î£ = diag(`1, `2, . . . , `M , 1, . . . , 1) where `1 â‰¥ `2 â‰¥ . . . â‰¥ `M > 1. We
treat the N Ã— n matrix X = (X1 : . . . : Xn) as a double array indexed by both n and N = N(n)
on the same probability space, such that N/n â†’ Î³, where Î³ is a positive constant. Throughout
we shall assume that 0 < Î³ < 1 although much of the analysis can be extended to the case Î³ â‰¥ 1
with a little extra work. Our aim is to study asymptotic behaviour of the large eigenvalues of the
sample covariance matrix S = 1nXX
T as nâ†’âˆ. In this context we get the same almost sure limits
for the M largest eigenvalues as those obtained by Baik and Silverstein (2004). However, while
they derive these limits by studying the Stieltjes transform of the distribution which serves as the
almost sure limit of the ESD, we rely on a matrix analysis approach and use properties of Gaussian
distribution, including various concentration inequalities, as well as several known results about the
limiting behaviour of the ESD for the null (or identity covariance) model. The advantage of this
approach is that it gives a different perspective to the limits, in particular their identification as
2
certain linear functionals of the limiting Marchenko-Pastur law when the true eigenvalue is above
1 +
âˆš
Î³. This analysis also allows us to derive distributional limits of the sample eigenvalues Ì‚Ì€Î½
when `Î½ > 1 +
âˆš
Î³. We do this only for the case when `Î½ has multiplicity one. A comprehensive
study of all possible scenarios is beyond the scope of this paper. Another aspect of our approach is
that it throws light on the behaviour of the eigenvectors associated with the M largest eigenvalues.
We show that the sample eigenvectors also undergo a phase transition. We would like to emphasize
that, even though our method is not suitable for analyzing the distributional limits for the case
`Î½ â‰¤ 1 +
âˆš
Î³, it does afford a more probabilistic interpretation of the results in the other scenario,
and may be applied to study similar problems in other contexts.
The results derived in this paper contain two important messages about the inferential aspect
of dealing with large dimensional multivariate data. First, and most notably, the phase transition
phenomenon described in this paper means that some commonly used tests for the hypothesis Î£ = I,
like the largest root test (Roy, 1953), may not be able to detect comparatively small departures
from idenitity covariance when the ratio N/n is significantly larger than zero. At the same time,
our distributional convergence result (Theorem 3 ) can be used to approximate the power of the
largest root test against alternatives where the departure from the null model of identity covariance
is through perturbations by positive semidefinite matrices of finite rank. We discuss this further in
Section 2.2. At a more practical level, these results show that exploratory data analytic techniques
like â€œscree plotâ€ to determine number of significant eigenvalues may be of rather limited use when
dealing with certain types of near-isotropic high dimensional data. In such circumstances, even the
somewhat more sophisticated technique of comparing the sample eigenvalues with the quantiles
of the limiting Marchenko-Pastur law, as advocated by Wachter (1976), may not be particularly
successful because of the phase transition. The second important consequence of our results is that
it gives some insight as to why it might not be such a good idea to use Principal Component Analysis
(PCA) for dimension reduction in a high dimensional setting, at least not in its standard form. This
has already been observed by Johnstone and Lu (2004) who show that when N/nâ†’ Î³ âˆˆ (0,âˆ), the
sample principal components are inconsistent estimates of the population principal components.
Theorem 4 says exactly how bad this inconsistency is. Moreover, our method of proof clearly
demonstrates how this inconsistency originates.
The rest of the paper is organized as follows. In Section 2 we describe the main results and
point to their salient features. In Section 3 we define the key quantities and expressions that will
help us derive the results. Section 4 is devoted to proving the almost sure limits of eigenvalues.
In Section 5 we derive the asymptotic distribution result (Theorem 3 ). Section 6 describes the
matrix perturbation analysis approach which is a key ingredient in the proof of Theorem 3 and
Theorem 4. Proofs of some of the auxilliary results are given in the two appendices (Appendix A
and Appendix B).
3
2 Main results
In this section we describe the four main results of this paper. The first two pertain to the
almost sure limits of sample eigenvalues, the third describes their asymptotic distribution under
certain restrictions, while the fourth describes a result about the asymptotic behaviour of sample
eigenvectors. We use Ì‚Ì€Î½ to denote the Î½-th largest eigenvalue of S.
2.1 Almost sure limit of M largest eigenvalues
We have the following results about the almost sure limits of M largest sample eigenvalues. These
were independently derived by Baik and Silverstein (2004) for non-Gaussian observations.
Theorem 1 : Suppose `Î½ â‰¤ 1 +
âˆš
Î³, then with Nn â†’ Î³ âˆˆ (0, 1) as nâ†’âˆ we have
Ì‚Ì€
Î½ â†’ (1 +
âˆš
Î³)2, almost surely as nâ†’âˆ. (1)
Theorem 2 : Suppose `Î½ > 1 +
âˆš
Î³, then with Nn â†’ Î³ âˆˆ (0, 1) as nâ†’âˆ we have
Ì‚Ì€
Î½ â†’ `Î½
(
1 +
Î³
`Î½ âˆ’ 1
)
, almost surely as nâ†’âˆ. (2)
Let us discuss a little about the limits appearing in (1) and (2). We shall denote the limit in (2) by
ÏÎ½ := `Î½
(
1 + Î³`Î½âˆ’1
)
. It turns out, via Lemma B.1, that ÏÎ½ appears as a solution to the following
equation
Ï = `(1 + Î³
âˆ«
x
Ïâˆ’ x
dFÎ³(x)) (3)
with ` = `Î½ . Since FÎ³ is supported on [(1 âˆ’
âˆš
Î³)2, (1 +
âˆš
Î³)2] for Î³ â‰¤ 1 (with a single isolated
point added to the support for Î³ > 1), the function on the RHS is monotonically decreasing in
Ï âˆˆ ((1 + âˆšÎ³)2,âˆ) and the LHS is obviously increasing in Ï. So a solution to (3) exists only if
`Î½ â‰¥ 1 + cÎ³ , for some cÎ³ > 0. That cÎ³ =
âˆš
Î³ is a part of Lemma B.1. Note that when `Î½ = 1 +
âˆš
Î³,
ÏÎ½ = (1 +
âˆš
Î³)2, the almost sure limit of the j-th largest eigenvalue (for j fixed) in the identity
covariance case.
2.2 Asymptotic normality of sample eigenvalues
When a non-unit eigenvalue of Î£ is simple, i.e. of multiplicity one, and above the critical value
1 +
âˆš
Î³, we show that the corresponding sample eigenvalue is asymptotically normally distributed.
While a generalization of this result for the multiplicity greater than one case seems interesting,
we do not pursue it here. We note that for the complex Gaussian case a result in the analogous
situation has been derived by Baik et al. (2004, Theorem 1.1(b)), where they showed that when the
largest eigenvalue is greater than 1 +
âˆš
Î³ and of multiplicity k, the largest sample eigenvalue, after
4
similar centering and scaling, converges in distribution to the distribution of the largest eigenvalue
of a kÃ— k GUE (Gaussian Unitary Ensemble). They also derived the limiting distributions for the
case when a (non-unit) population eigenvalue is smaller than 1 +
âˆš
Î³. Distributional aspect of a
sample eigenvalue for the real case in the latter situation is beyond the scope of this paper.
Theorem 3 : Suppose `Î½ > 1 +
âˆš
Î³ and of multiplicity 1. Then as n,N â†’ âˆ so that Nn âˆ’ Î³ =
o(nâˆ’1/2),
âˆš
n(Ì‚Ì€Î½ âˆ’ ÏÎ½) =â‡’ N(0, Ïƒ2(`Î½)), (4)
where for ` > 1 +
âˆš
Î³, and with Ï(`) = `(1 + Î³`âˆ’1),
Ïƒ2(`) =
2`Ï(`)
1 + `Î³
âˆ«
x
(Ï(`)âˆ’x)2dFÎ³(x)
=
2`Ï(`)
1 + `Î³
(`âˆ’1)2âˆ’Î³
= 2`2(1âˆ’ Î³
(`âˆ’ 1)2
) (5)
In the fixed N case, when the Î½-th eigenvalue has multiplicity 1, the Î½-th sample eigenvalue is
asymptoctically N(`Î½ , 1n2`
2
Î½). This is a special case of a more general result by Anderson (1963).
Thus the fact that the dimension to sample size ratio is positive, contributes towards the bias
and a reduction in variance. However, if Î³ is much smaller compared to `Î½ , the variance Ïƒ2(`Î½) is
approximately 2`2Î½ which is the asymptotic variance in the fixed N case. This is what we expect
intuitively, since the eigenvector associated with this sample eigenvalue, looking to maximize the
quadratic from invovling S (under orthogonality restrictions), will tend to put more mass on the
Î½-th coordinate. This is demonstrated even more clearly by Theorem 4 that we state later. But
before that, we give a brief account of the importance of Theorem 1-3 from a statistical perspective.
As we already noted in Section 1, one possible application of Theorem 3 is in the calculation of
asymptotic power for the largest root test. The latter refers to the testing problem where the null
hypothesis says that the covariance matrix is identity. And the test rejects the null hypothesis at
level Î± âˆˆ (0, 1) if the largest eigenvalue of S is above a critical level cn,N,Î±, say. Johnstone (2001)
proposed a conservative test of this type for large (n,N) data based on the quantiles of Tracy-
Widom distribution. His proposal means that the cutoff value, for large n, can be approximated
as
cn,N,Î± â‰ˆ (1 +
âˆš
N
n
)2 +Nâˆ’1/6nâˆ’1/2(1 +
âˆš
N
n
)4/3Ï„Î±, for Î± âˆˆ (0, 1)
where Ï„Î± is the (1âˆ’ Î±) quantile of Tracy-Widom law of order 1.
Now suppose we consider the alternative hypothesis that the population covariance matrix is
Î£ = diag(`1, . . . , `M , 1, . . . , 1) with `1 â‰¥ . . . â‰¥ `M > 1. If `1 > 1 +
âˆš
Î³, Theorem 2 shows that
the largest root test is asymptotically consistent. For the special case when `1 is of multiplicity
one, Theorem 3 immediately gives an expression for the asymptotic power function, assuming that
N
n converges to Î³ fast enough, as n â†’ âˆ. But one has to view this in proper context, since our
result is derived under the assumption that `1, . . . , `M are all fixed and we do not have a rate of
5
convergence for the distribution of Ì‚Ì€1 towards normality. A detailed analysis of power properties
against local alternatives is beyond the scope of this paper. However, Theorem 1 indicates that
the largest root test may fail to detect a departure from the null model of identity covariance if `1
is less than 1 +
âˆš
Î³.
It is important to point to a potential advantage of such a test as compared to some other
well-known tests for the same hypothesis. Ledoit and Wolf (2002) give a nice overview of different
tests of sphericity used in high-dimensional setting. They consider tests based on statistics U , and
W given below.
U =
1
N
trace
ï£®ï£°( S
1
N trace(S)
âˆ’ I
)2ï£¹ï£» , W = 1
N
trace[(Sâˆ’ I)2]âˆ’ N
n
[
1
N
trace(S)
]2
+
N
n
The statistic U is used to test sphericity, i.e. Î£ = cI for some c > 0 unknown. Their results
(Ledoit and Wolf, 2002, Proposition 1-7) show that if Î²N = 1N trace(Î£) and Î¸
2
N =
1
N trace(Î£âˆ’ I)
2
are fixed, at values Î² > 0 and Î¸, say, as Nn â†’ Î³ âˆˆ (0,âˆ), then the test based on W is consistent
for testing H0 : (Î² âˆ’ 1)2 + Î¸2 = 0 against HA : (Î² âˆ’ 1)2 + Î¸2 > 0. Whereas the test based on U
is consistent for H0 : Î¸2/Î²2 = 0 against HA : Î¸2/Î²2 > 0. Their results can be easily extended to
the case where Î²N â†’ Î² and Î¸N â†’ Î¸ rather than being fixed quantities. Notice that even when Î£
is a finite rank perturbation of identity, Î² = 1 and Î¸ = 0. Under this setting these tests cannot
distinguish between H0 and HA. We expect similar sort of asymptotic behvaiour from any test that
relies upon traces of powers of S and statistics derived from them. In contrast the test described in
the previous paragraph can separate the null from the alternative in the same scenario under the
rather mild requirement that Î»1(Î£) > 1 +
âˆš
Î³. We treat this comparison as a way of emphasizing
the following point: our results show that for signal detection problems in high dimension, when the
signal is rather feeble, leaning on tests based on the extreme eigenvalues may be more meaningful
than depending on tests which are based on the bulk of the eigenvalue specturm.
2.3 Angle between true and estimated eigenvectors
Hoyle and Rattray (2004) mention about a phase transition phenomenon in the asymptotic be-
haviour of the angle between the true and estimated eigenvector associated with a non-unit eigen-
value `Î½ . They term this â€œthe phenomenon of retarded learningâ€. They derived this result at a
physical level of rigour. Their result can be rephrased in our context to mean that if 1 < `Î½ â‰¤ 1+
âˆš
Î³
is a simple eigenvalue, then the cosine of the angle between the corresponding true and estimated
eigenvectors almost surely converges to zero, whereas one gets strictly positive limit if `Î½ > 1 +
âˆš
Î³.
Part (a) of Theorem 4, stated below and proved in Section 6, is a precise statement of the latter
part of their result. This also readily proves a stronger version of the result regarding inconsistency
of sample eigenvectors as stated in Johnstone and Lu (2004).
6
Theorem 4 : Let eÌƒÎ½ denote the N Ã— 1 vector with 1 in the Î½-th coordinate and zeros elsewhere,
and pÎ½ denote the eigenvector of S associated with the eigenvalue Ì‚Ì€Î½ .
(a) If `Î½ > 1 +
âˆš
Î³ and of multiplicity one,
|ã€ˆpÎ½ , eÌƒÎ½ã€‰|
a.s.â†’
âˆš(
1âˆ’ Î³
(`Î½ âˆ’ 1)2
)
/
(
1 +
Î³
`Î½ âˆ’ 1
)
as nâ†’âˆ. (6)
(b) If `Î½ â‰¤ 1 +
âˆš
Î³,
ã€ˆpÎ½ , eÌƒÎ½ã€‰
a.s.â†’ 0 as nâ†’âˆ. (7)
In order to prove this result we use a specific decmposition of the eigenvectors as explained in
Section 3. Proceeding along this line it is possible to study the behaviour of the sample eigenvectors
in more detail. But we shall give it a full treatment elsewhere and hence do not deal with this issue
in the current paper.
3 Representation of the eigenvalues of S
Throughout we assume that n is large enough so that Nn < 1. In order to proceed further we
introduce some notations that will help us in later stages. First we partition the matrix S as
S =
[
SAA SAB
SBA SBB
]
where the suffix A corresponds to the set of coordinates {1, . . . ,M} and B corresponds to the set
{M + 1, . . . , N}. As before we use Ì‚Ì€Î½ and pÎ½ to denote the Î½-th largest sample eigenvalue and the
correpsonding sample eigenvector. We shall follow the convention that the Î½-th element of pÎ½ is
nonnegative to avoid any ambiguity. We shall write pÎ½ as pTÎ½ = (p
T
A,Î½ , p
T
B,Î½) and denote the norm
â€– pB,Î½ â€– by RÎ½ . Then almost surely 0 < RÎ½ < 1.
With this setting in place, now we can express the first M eigenequations for S as
SAApA,Î½ + SABpB,Î½ = Ì‚Ì€Î½pA,Î½ , Î½ = 1, . . . ,M, (8)
SBApA,Î½ + SBBpB,Î½ = Ì‚Ì€Î½pB,Î½ , Î½ = 1, . . . ,M, (9)
pTA,Î½pA,Î½â€² + p
T
B,Î½pB,Î½â€² = Î´Î½,Î½â€² , 1 â‰¤ Î½, Î½ â€² â‰¤M. (10)
Here Î´Î½Î½â€² is the Kronecker symbol. Now denote the vector pA,Î½/ â€– pA,Î½ â€–= pA,Î½/
âˆš
1âˆ’R2Î½ by bÎ½ .
Thus â€– bÎ½ â€–= 1. Similarly define qÎ½ := pB,Î½/RÎ½ and again â€– qÎ½ â€–= 1.
With all the relevant quantities about the problem now defined we can express the eigenequa-
tions in a more suitable form that will allow us to make useful observations about the relationship
7
among the empirical eigenvalues Ì‚Ì€1, . . . , Ì‚Ì€M . First, changing sides in (9) to collect terms involving
qÎ½ , and noticing that almost surely 0 < RÎ½ < 1, and Ì‚Ì€Î½I âˆ’ SBB is invertible,
qÎ½ =
âˆš
1âˆ’R2Î½
RÎ½
(Ì‚Ì€Î½I âˆ’ SBB)âˆ’1SBAbÎ½ (11)
Now, dividing both sides of (8) by
âˆš
1âˆ’R2Î½ and substituting the expression for qÎ½ , we get
(SAA + SAB(Ì‚Ì€Î½I âˆ’ SBB)âˆ’1SBA)bÎ½ = Ì‚Ì€Î½bÎ½ , Î½ = 1, . . . ,M. (12)
This equation is quite remarkable since it shows that Ì‚Ì€Î½ is an eigenvalue of the matrix K(Ì‚Ì€Î½) where
K(x) = SAA + SAB(xI âˆ’ SBB)âˆ’1SBA
with corresponding eigenvector bÎ½ . This particular observation will be the building block for all
our analysis. However, we shall find it more convenient to express the quantities in terms of the
spectral elements of the data matrix X.
Let Î› denote the diagonal matrix diag(`1, . . . , `M ). Because of normality assumption, the
observation matrix X can be reexpressed as
XT = [ZTAÎ›
1/2 : ZTB], ZA is M Ã— n, ZB is (N âˆ’M)Ã— n,
and the entries of ZA and ZB are i.i.d. N(0, 1), and ZA and ZB are mutually independent. We can
also assume that ZA and ZB are defined on the same probability space.
Let the singular value decomposition of 1âˆš
n
ZB be given as
1âˆš
n
ZB = VM1/2HT (13)
whereM is the (N âˆ’M)Ã— (N âˆ’M) diagonal matrix of the eigenvalues of SBB in decreasing order,
V is the (Nâˆ’M)Ã—(Nâˆ’M) matrix of eigenvectors of SBB and H is the nÃ—(Nâˆ’M) matrix of right
singular vectors. We shall denote the diagonal elements of M by Âµ1 > . . . > ÂµNâˆ’M , suppressing
the dependence on n.
Let the columns of V be denoted by v1, . . . , vNâˆ’M and the columns of H be denoted by
h1, . . . , hNâˆ’M . Note that {v1, . . . , vNâˆ’M} is a complete orthonormal basis for RNâˆ’M , whereas
h1, . . . , hNâˆ’M form an orthonormal basis of an (N âˆ’M) dimensional subspace (viz. the rowspace
of ZB) of Rn.
Observe that qÎ½ = V Î¶Î½ =
âˆ‘Nâˆ’M
j=1 Î¶Î½jvj for some unit vector Î¶Î½ . Also, define by T the matrix
1âˆš
n
HTZTA. T is an (N âˆ’ M) Ã— M matrix and let its columns be denoted by t1, . . . , tM . The
most important property about T that we shall use repeatedly is that the vectors t1, . . . , tM are
distributed as i.i.d. N(0, 1nINâˆ’M ) and are independent of ZB. This is because the columns of H
form an orthonormal set of vectors and the rows of ZA are i.i.d. Nn(0, I) vectors, and moreover,
ZA and ZB are independently distributed.
8
Thus, we obtain the following equations by simple linear transformations of (11) and (12),
respectively.
Î¶Î½ =
âˆš
1âˆ’R2Î½
RÎ½
(Ì‚Ì€Î½I âˆ’M)âˆ’1M1/2TÎ›1/2bÎ½ , Î½ = 1, . . . ,M. (14)
(SAA + Î›1/2T TM(Ì‚Ì€Î½I âˆ’M)âˆ’1TÎ›1/2)bÎ½ = Ì‚Ì€Î½bÎ½ , Î½ = 1, . . . ,M. (15)
Also note that K(x) can be expressed as
K(x) = SAA + Î›1/2T TM(xI âˆ’M)âˆ’1TÎ›1/2 (16)
We conclude this section by rewriting equation (10) in terms of the vectors {bÎ½ : Î½ = 1, . . . ,M} as
bTÎ½ [I + Î›
1/2T T (Ì‚Ì€Î½I âˆ’M)âˆ’1M(Ì‚Ì€Î½â€²I âˆ’M)âˆ’1TÎ›1/2]bÎ½â€² = 11âˆ’R2Î½ Î´Î½Î½â€² , 1 â‰¤ Î½, Î½ â€² â‰¤M, (17)
which is same as
bTÎ½ [I + SAB(Ì‚Ì€Î½I âˆ’ SBB)âˆ’1(Ì‚Ì€Î½â€²I âˆ’ SBB)âˆ’1SBA]bÎ½â€² = 11âˆ’R2Î½ Î´Î½Î½â€² , 1 â‰¤ Î½, Î½ â€² â‰¤M. (18)
4 Almost sure limits
In this section we prove Theorem 1 and Theorem 2. Proofs of these two theorems depend heavily
on the asymptotic behaviour of the largest eigenvalue of a Wishart matrix in the null (i.e. identity
covariance) case, as well as on the limiting behaviour of the Empirical Spectral Distribution of
Wishart matrices. Throughout, the ESD of SBB is denoted by FÌ‚n,Nâˆ’M . Then we know that (cf.
Bai, 1999)
FÌ‚n,Nâˆ’M =â‡’ FÎ³ , almost surely as nâ†’âˆ
where =â‡’ denotes distributional convergence.
Our proof relies upon essentially showing the following fact
tTjM(Ì‚Ì€Î½I âˆ’M)âˆ’1tk â†’ 0, almost surely 1 â‰¤ j 6= k â‰¤M,
tTjM(Ì‚Ì€Î½I âˆ’M)âˆ’1tj â†’ Î³ âˆ« xÏÎ½ âˆ’ xdFÎ³(x), almost surely.
The rest of the section is organized as follows. We shall establish first Theorem 2 and then
Theorem 1. The proofs of these two results use the same technique in that they use the interlacing
inequality for eigenvalues of symmetric matrices to derive upper and lower bounds for Ì‚Ì€Î½ which
may fail to hold with negligible probability.
However, it turns out that the derivation of the lower bound for Ì‚Ì€Î½ in the proof of Theorem 2
becomes much easier if one has a suitable preliminary lower bound on Ì‚Ì€Î½ . To be more specific, one
9
needs to ensure that the set CÎ½ = {Ì‚Ì€Î½ > Âµ1} has very high probability when `Î½ > 1 +âˆšÎ³. This is
comparatively a lot easier in the case when `Î½ is in fact greater than (1 +
âˆš
Î³)2. This is established
via Proposition 1 and Proposition 2. Incidentally, Proposition 2 gives a general purpose bound for
the j-th eigenvalue Âµj of SBB for every fixed j. However, in the general case (i.e. when simply
`Î½ > 1 +
âˆš
Î³), we explicitly construct a lower bound for Ì‚Ì€Î½ using equations (14) and (15). This
requires a lot more work and to keep the exposition simpler we have deferred this result (Proposition
B.2 ) till Appendix B.
It is comparatively easier to derive sharp upper bounds for Ì‚Ì€Î½ and the same technique can be
used to derive bounds for the cases when `Î½ > 1 +
âˆš
Î³ and when 1 < `Î½ â‰¤ 1 +
âˆš
Î³. For the time
being we assume that either `Î½ â‰¤ 1 +
âˆš
Î³ or `Î½ > (1 +
âˆš
Î³)2.
4.1 Bounds on the eigenvalues Ì‚Ì€Î½
We make use of the interlacing inequality for eigenvalues of symmetric matrices, (see e.g. Section
1f of Rao, 1973). We introduce some notations for later use.
4.1.1 Notations
We denote the quantity (1 +
âˆš
Î³)2 by ÎºÎ³ . Throughout, unless otherwise specified, Î»j(C) for any
symmetric matrix C will denote the j-th largest eigenvalue of C. For any m Ã— m matrix C , if
G âŠ‚ {1, . . . ,m}, then by CG we shall denote the submatrix of C deleting the rows and columns
that are in G. Also, we shall use â€– â€– to denote both l2 norm of vectors, as well as the 2-norm, or
the largest singular value, of matrices. â€– â€–HS will mean the Hilbert-Schmidt norm for matrices.
For Ï > ÎºÎ³ , we define
Î›G(Ï) = (1 + Î³
âˆ«
x
Ïâˆ’ x
dFÎ³(x))Î›G (19)
4.1.2 Interlacing inequalities
By the interlacing inequality we have
Î»1(SG) â‰¥ Î»|G|+1(S) and Î»k(S) â‰¥ Î»k(SG), for all G âŠ‚ {1, . . . , N}, all k (20)
Define Î“Î½ = {1, . . . , Î½} and Î“Î½ = {Î½ + 1, . . . ,M} for 1 â‰¤ Î½ â‰¤ M and Î“0 = Ï† = Î“M . Then (20)
implies
Î»1(SÎ“Î½âˆ’1) â‰¥ Ì‚Ì€Î½ â‰¥ Î»Î½(SÎ“Î½ ) (21)
10
4.1.3 Eigenvalues of submatrices
Observe that Î»1(SÎ“Î½âˆ’1) and Î»Î½(SÎ“Î½ ) are some eigenvalues of KÎ“Î½âˆ’1(Î»1(SÎ“Î½âˆ’1)) and KÎ“Î½ (Î»Î½(SÎ“Î½ )),
respectively, where
KG(x) = SAA,G + Î›
1/2
G T
T
GM(xI âˆ’M)âˆ’1TGÎ›
1/2
G , for G âŠ‚ {1, . . . ,M}. (22)
Here by TG we denote the submatrix of T with all columns in set G deleted. This follows by noting
that for G âŠ‚ {1, . . . ,M},
SG =
[
SAA,G SAB,G
SBA,G SBB
]
, with SAB,G =
1
n
Î›1/2G ZA,GZ
T
B, SAA,G =
1
n
Î›1/2G ZA,GZ
T
A,GÎ›
1/2
G ,
and SBA,G = STAB,G, where ZA,G denotes the submatrix of ZA with rows in the set G deleted.
4.1.4 Preliminary bounds
To begin with let us assume that `Î½ > (1 +
âˆš
Î³)2. This situation is simpler to deal with. In the
following SAA,Î½ denotes the submatrix of SAA consisting of only the first Î½ rows and Î½ columns.
We show that the eigenvalues of SAA,Î½ concentrate around their population counterparts, and by
the interlacing inequalities we directly have
Î»Î½(SÎ“Î½ ) â‰¥ Î»Î½(SAA,Î½), (23)
Therefore the required lower bound for Ì‚Ì€Î½ is easily obtained if we apply (21). We formally state
the following:
Proposition 1 : Let ÎºÎ³ = (1+
âˆš
Î³)2 and 0 <  < `1Î½2 be any number such that `Î½ > ÎºÎ³ +2. Then,
P(Î»Î½(SÎ“Î½ ) â‰¤ ÎºÎ³ + ) â‰¤ 2Î½ exp
(
âˆ’ n
2
6`21Î½2
)
+ Î½(Î½ âˆ’ 1) exp
(
âˆ’ n
2
3`21Î½2
)
(24)
Proof : In view of (23) we only need to establish the inequality for Î»Î½(SAA,Î½). Let Î›Î½ =
diag(`1, . . . , `Î½). Then for Î½ â€² = 1, . . . , Î½,
|Î»Î½â€²(SAA,Î½)âˆ’ `Î½â€² | â‰¤â€– SAA,Î½ âˆ’ Î›Î½ â€– (25)
In certain circumstances this upper bound can be improved with a more careful analysis, but we
do not need that here. The bound appearing on the RHS can be majorized by the Hilbert-Schmidt
norm:
â€– SAA,Î½ âˆ’ Î›Î½ â€–HS=
âˆšâˆšâˆšâˆš Î½âˆ‘
k=1
|skk âˆ’ `k|2 +
Î½âˆ‘
j 6=k
|sjk|2
where sjk is the (j, k)-th element of SAA, 1 â‰¤ j, k â‰¤ M . In order to bound the terms appearing
inside square roots we use large deviation inequalities for quadratic forms of Gaussian random
11
variables. Observe that sjk =
âˆš
`j`k
1
nZ
T
A,jZA,k where Z
T
A,j is the j-th row of ZA. Taking X = ZA,j ,
Y = ZA,k, C(Z) = I and L = 1 in Lemma A.1 we get
P(|skj | >
âˆš
`j`kt) â‰¤ 2 exp(âˆ’
(1âˆ’ Î´)nt2
2
), 0 < t <
Î´
1âˆ’ Î´
(26)
Similarly, applying Lemma A.2 with X = ZA,k, C(Z) = I and L = 1 we get
P(|skk âˆ’ `k| > `kt) â‰¤ 2 exp(âˆ’
(1âˆ’ Î´)nt2
4
), 0 < t <
2Î´
1âˆ’ Î´
(27)
Thus, taking Î´ = 13 in (26) and (27) and setting t =

`1Î½
, since 0 <  < `1Î½2 we get, after using (25)
and the expression for â€– SAA,Î½ âˆ’ Î›Î½ â€–HS
P(|Î»Î½(SAA,Î½)âˆ’ `Î½ | > ) â‰¤ 2Î½ exp
(
âˆ’ n
2
6`21Î½2
)
+ Î½(Î½ âˆ’ 1) exp
(
âˆ’ n
2
3`21Î½2
)
(28)
From this (24) follows.
Next we state a result about the concentration of largest few eigenvalues of SBB around ÎºÎ³ . This
is proved in Appendix A.
Proposition 2 : For any 0 < Î´ < ÎºÎ³/2,
P(|Âµ1 âˆ’ ÎºÎ³ | â‰¥ Î´) â‰¤ 2 exp
(
âˆ’ nÎ´
2
32ÎºÎ³
)
, for n â‰¥ n0(Î³, Î´) (29)
where n0(Î³, Î´) is an integer large enough such that |Median(Âµ1)âˆ’ ÎºÎ³ | â‰¤ Î´4 for n â‰¥ n0(Î³, Î´).
Remark : The proof relies on the asymptotic distribution of the largest eigenvalue of a sample
covariance matrix in the identity covariance case (Johnstone, 2001) and a concentration inequality
for singular values of Gaussian random matrices. Soshnikov (2002) proved that when centered and
scaled by the same numbers, a similar type of limiting law holds for any leading eigenvalue (i.e.
any Âµj with j fixed). The details of these distributions can be found in Tracy and Widom (1994),
(1996). So the same proposition applies to any Âµj for j fixed.
4.2 Upper bound for Ì‚Ì€Î½
First we derive a tight upper bound for Ì‚Ì€Î½ . Our strategy is to utilize the upper bound in (21). For
this we do not need (24). However, we need the bound (29). For simplicity of notations, we shall
use Î»Ì‚1,Î½ to mean Î»1(SÎ“Î½âˆ’1). Our aim is to prove the following:
Proposition 3 : Let `Î½ > 1 +
âˆš
Î³ for some 1 â‰¤ Î½ â‰¤M . Then given  > 0 there exists n1(Î½, ,Î›, Î³)
large enough such that, for n â‰¥ n1(Î½, ,Î›, Î³),
P(Ì‚Ì€Î½ > ÏÎ½ + , Âµ1 < ÎºÎ³ + /2) â‰¤ (M âˆ’ Î½ + 1)Îµ1(n, ,Î›, Î³) + (M âˆ’ Î½)(M âˆ’ Î½ + 1)Îµ2(n, ,Î›, Î³),
(30)
12
where Îµ1 and Îµ2 are the terms appearing on the RHS of equations (47) and (39), respectively.
Proof : First, from (21) we have
P(Ì‚Ì€Î½ > ÏÎ½ + , Âµ1 < ÎºÎ³ + /2) â‰¤ P(Î»Ì‚1,Î½ > ÏÎ½ + , Âµ1 < ÎºÎ³ + /2)
Since Î»Ì‚1,Î½ is an eigenvalue of KÎ“Î½âˆ’1(Î»Ì‚1,Î½) where KG(Â·) is defined through (22),
Î»Ì‚1,Î½ â‰¤ Î»1(KÎ“Î½âˆ’1(ÏÎ½ + )) on the set J1,Î½ := {Î»Ì‚1,Î½ > ÏÎ½ + , Âµ1 < ÎºÎ³ + /2} (31)
To verify (31) observe that on J1,Î½ the inequalities
Âµj
Î»Ì‚1,Î½âˆ’Âµj
â‰¤ ÂµjÏÎ½+âˆ’Âµj hold for all j = 1, . . . , Nâˆ’M .
This implies the inequality for positive semidefinite matrices: KÎ“Î½âˆ’1(Î»Ì‚1,Î½) â‰¤ KÎ“Î½âˆ’1(ÏÎ½ + ) (since,
for any a âˆˆ RMâˆ’|G|, aTKG(x)a = aTSAA,Ga+
âˆ‘Nâˆ’M
j=1 c
2
jÂµj(xâˆ’ Âµj)âˆ’1 where c = TGÎ›
1/2
G a).
Let Î›G(Ï) be defined through (19). Then by (31) and a simple inequality for eigenvalues of
symmetric matrices, on the set J1,Î½ ,
Î»Ì‚1,Î½ â‰¤ `Î½(1 + Î³
âˆ«
x
ÏÎ½ + âˆ’ x
dFÎ³(x))+ â€– KÎ“Î½âˆ’1(ÏÎ½ + )âˆ’ Î›Î“Î½âˆ’1(ÏÎ½ + ) â€–HS (32)
since `Î½(1 + Î³
âˆ«
x
ÏÎ½+âˆ’xdFÎ³(x)) is the largest eigenvalue of Î›Î“Î½âˆ’1(ÏÎ½ + ).
Now, let Î´ := Î´() > 0 be such that
`Î½(1 + Î³
âˆ«
x
ÏÎ½ + âˆ’ x
dFÎ³(x)) + Î´ = ÏÎ½ +  (33)
Indeed we can find such a Î´ because if we define `Î½, to be `Î½, = (ÏÎ½ + )(1 + Î³
âˆ«
x
ÏÎ½+âˆ’xdFÎ³(x))
âˆ’1,
then we have `Î½, > `Î½ (since, by definition, ÏÎ½ is the solution to equation (3)), and hence
`Î½(1 + Î³
âˆ«
x
ÏÎ½ + âˆ’ x
dFÎ³(x)) =
`Î½
`Î½,
(ÏÎ½ + ) < ÏÎ½ + 
Denote the matrix appearing inside â€– â€–HS on the RHS of (32) by DÌƒÎ½ = ((DÌƒÎ½,jk))Mâˆ’Î½+1j,k=1 . From
our construction, if
|DÌƒÎ½,jk| < Î´jk, 1 â‰¤ j, k â‰¤M âˆ’ Î½ + 1, where Î´jk > 0, âˆ€ j, k, and
Mâˆ’Î½+1âˆ‘
j=1
Mâˆ’Î½+1âˆ‘
k=1
Î´2jk â‰¤ Î´2,
then from (32) we get, on J1,Î½ , Î»Ì‚1,Î½ < ÏÎ½ +  which is an impossibility. Hence after taking the union
bound,
P(J1,Î½) â‰¤
Mâˆ’Î½+1âˆ‘
j=1
P(|DÌƒÎ½,jj | â‰¥ Î´jj , Âµ1 < ÎºÎ³ + /2) +
Mâˆ’Î½+1âˆ‘
j<k
P(|DÌƒÎ½,jk| â‰¥ Î´jk, Âµ1 < ÎºÎ³ + /2) (34)
We set Î´jj = `j+Î½âˆ’1Î´Ìƒ1, j = 1, . . . ,M âˆ’ Î½ + 1 and Î´jk =
âˆš
`j+Î½âˆ’1`k+Î½âˆ’1 Î´Ìƒ2, 1 â‰¤ j < k â‰¤M âˆ’ Î½ + 1
where Î´Ìƒ1, Î´Ìƒ2 > 0 are such that Î´2 = Î´Ìƒ21
âˆ‘M
j=Î½ `
2
j + Î´Ìƒ
2
2(
âˆ‘M
j=Î½ `j)
2. To be specific, we take Î´Ìƒ2 =
Î´âˆš
2
(
âˆ‘M
j=Î½ `j)
âˆ’1 and Î´Ìƒ1 = Î´âˆš2(
âˆ‘M
j=Î½ `
2
j )
âˆ’1/2.
13
Remark : Notice that the bound we are using here is rather crude. In specific situations, e.g.
when `1, . . . , `Î½ are distinct, one may be able to get better bounds.
Observe that for j = 1, . . . ,M âˆ’ Î½ + 1,
DÌƒÎ½,jj = `j+Î½âˆ’1(
1
n
ZTA,j+Î½âˆ’1ZA,j+Î½âˆ’1 âˆ’ 1)
+`j+Î½âˆ’1(tTj+Î½âˆ’1M((ÏÎ½ + )I âˆ’M)âˆ’1tj+Î½âˆ’1 âˆ’
1
n
trace (M((ÏÎ½ + )I âˆ’M)âˆ’1))
+`j+Î½âˆ’1(
1
n
trace (M((ÏÎ½ + )I âˆ’M)âˆ’1)âˆ’ Î³
âˆ«
x
ÏÎ½ + âˆ’ x
dFÎ³(x)) (35)
whereas, for 1 â‰¤ j 6= k â‰¤M âˆ’ Î½ + 1,
DÌƒÎ½,jk =
âˆš
`j+Î½âˆ’1`k+Î½âˆ’1[
1
n
ZTA,j+Î½âˆ’1ZA,k+Î½âˆ’1 + t
T
j+Î½âˆ’1M((ÏÎ½ + )I âˆ’M)âˆ’1tk+Î½âˆ’1] (36)
Define JÎ³() := {Âµ1 â‰¤ ÎºÎ³ + }. Then to bound P(|DÌƒÎ½,jk| â‰¥ Î´jk, JÎ³(/2)), for j 6= k, observe that
from (26) we have
P(| 1
n
ZTA,j+Î½âˆ’1ZA,k+Î½âˆ’1| â‰¥ Î´Ìƒ2/2) â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
2
12
)
, for 0 < Î´Ìƒ2 < 1 (37)
Since
âˆš
ntj âˆ¼ N(0, INâˆ’M ) for j = 1, . . . ,M , and
â€– M((ÏÎ½ + )I âˆ’M)âˆ’1 â€–=
Âµ1
ÏÎ½ + âˆ’ Âµ1
â‰¤ ÎºÎ³ + /2
ÏÎ½ + /2âˆ’ ÎºÎ³
on JÎ³(/2), we can apply Lemma A.1 to conclude that (taking Î´ = 13 in the lemma), for j 6= k,
P(|tTj+Î½âˆ’1M((ÏÎ½ + )I âˆ’M)âˆ’1tk+Î½âˆ’1| â‰¥ Î´Ìƒ2/2, JÎ³(/2))
= 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ22
12(ÎºÎ³ + /2)2
)
= 2 exp
(
âˆ’1
Î³
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ22
12(ÎºÎ³ + /2)2
(1 + o(1))
)
, for 0 < Î´Ìƒ2 <
ÎºÎ³ + /2
ÏÎ½ + /2âˆ’ ÎºÎ³
(38)
Combining (36), (37) and (38), for 0 < Î´Ìƒ2 < min{1, ÎºÎ³+/2ÏÎ½+/2âˆ’ÎºÎ³ },
P(|DÌƒÎ½,jk| â‰¥ Î´jk, Âµ1 < ÎºÎ³ + /2) â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
2
12
)
+ 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ22
12(ÎºÎ³ + /2)2
)
(39)
for all 1 â‰¤ j < k â‰¤M âˆ’ Î½ + 1
In order to obtain a similar bound for DÌƒÎ½,jj , first observe that from (27),
P(| 1
n
ZTA,j+Î½âˆ’1ZA,j+Î½âˆ’1 âˆ’ 1| â‰¥ Î´Ìƒ1/4) â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
1
96
)
, for 0 < Î´Ìƒ1 < 4 (40)
14
Again, argument silimlar to that leading to (38) implies (this time using Lemma A.2 ),
P(|tTj+Î½âˆ’1M((ÏÎ½ + )I âˆ’M)âˆ’1tj+Î½âˆ’1 âˆ’
1
n
trace (M((ÏÎ½ + )I âˆ’M)âˆ’1)| â‰¥ Î´Ìƒ1/4, JÎ³(/2))
â‰¤ 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ21
96(ÎºÎ³ + /2)2
)
= 2 exp
(
âˆ’1
Î³
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ21
96(ÎºÎ³ + /2)2
(1 + o(1))
)
, for 0 < Î´Ìƒ1 <
4(ÎºÎ³ + /2)
ÏÎ½ + /2âˆ’ ÎºÎ³
(41)
To provide a bound for the remaining terms, we observe that on JÎ³(/2),
trace (M((ÏÎ½ + )I âˆ’M)âˆ’1) + (N âˆ’M) = (ÏÎ½ + ) trace (((ÏÎ½ + )I âˆ’M)âˆ’1)
= (ÏÎ½ + ) trace G1(SBB; ÏÎ½ + , Î³, /2),
where the function G1(Â·; Â·, Â·, Â·) is defined through (90) in Appendix A. Therefore we can apply
Proposition A.1 (in the Appendix ) to get
P(| 1
n
trace (M((ÏÎ½ + )I âˆ’M)âˆ’1)âˆ’
E(
1
n
(ÏÎ½ + ) trace G1(SBB; ÏÎ½ + , Î³, /2)) +
N âˆ’M
n
| > Î´Ìƒ1/4, JÎ³(/2))
= P(| 1
n
trace G1(SBB; ÏÎ½ + , Î³, /2)âˆ’ E(
1
n
trace G1(SBB; ÏÎ½ + , Î³, /2))|
> (ÏÎ½ + )âˆ’1Î´Ìƒ1/4, JÎ³(/2))
â‰¤ 2 exp
(
âˆ’ n
n+N âˆ’M
n2Î´Ìƒ21
2
(ÏÎ½ + /2âˆ’ ÎºÎ³)4
64(ÏÎ½ + )2(ÎºÎ³ + /2)
)
= 2 exp
(
âˆ’ n
2Î´Ìƒ21
2(1 + Î³)
(ÏÎ½ + /2âˆ’ ÎºÎ³)4
64(ÏÎ½ + )2(ÎºÎ³ + /2)
(1 + o(1))
)
(42)
Now to tackle the remainder we notice that
E(
1
n
trace G1(SBB; ÏÎ½ + , Î³, /2)) =
N âˆ’M
n
E
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFÌ‚n,Nâˆ’M (x)
where FÌ‚n,Nâˆ’M denotes the ESD of the matrix SBB. Note thatG1 is bounded above and monotone in
its first argument. Further, defining Fn,Nâˆ’M to be the expected ESD, by linearity of expectation,
E
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFÌ‚n,Nâˆ’M (x) =
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFn,Nâˆ’M (x). It is well-known that
Fn,Nâˆ’M =â‡’ FÎ³ as n â†’ âˆ, where FÎ³ is the Marchenko-Pastur law with parameter Î³. Bai (1993)
proved under fairly weak conditions that (Bai, 1993, Theorem 3.2) if Î¸1 â‰¤ pn â‰¤ Î¸2 where 0 < Î¸1 <
1 < Î¸2 <âˆ, then
â€– Fn,p âˆ’ Fp/n â€–âˆâ‰¤ C1(Î¸1, Î¸2)nâˆ’5/48. (43)
When 0 < Î¸1 < Î¸2 < 1, he also showed (Bai, 1993, Theorem 3.1) that
â€– Fn,p âˆ’ Fp/n â€–âˆâ‰¤ C2(Î¸1, Î¸2)nâˆ’1/4. (44)
15
Here â€– Â· â€–âˆ means the sup-norm and C1, C2 are constants with values depending on Î¸1, Î¸2.
Then utilizing the fact that FÎ³ has bounded support, G1(Â·; ÏÎ½ + , Î³, /2) is bounded, nonde-
creasing and differentiable everywhere except at x = ÎºÎ³ + /2 with Gâ€²1(x) â‰¡ 0 on (ÎºÎ³ + /2,âˆ),
and integrating by parts, we get from (43), for any 0 < Î¸1 < 1,
|
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFn,Nâˆ’M (x)âˆ’
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFÎ³(x)|
â‰¤ |
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFn,Nâˆ’M (x)âˆ’
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dF(Nâˆ’M)/n(x)|
+ |
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dF(Nâˆ’M)/n(x)âˆ’
âˆ«
G1(x; ÏÎ½ + , Î³, /2)dFÎ³(x)|
â‰¤ C3(Î¸1)(nâˆ’5/48+ â€– F(Nâˆ’M)/n âˆ’ FÎ³ â€–âˆ)
âˆ« ÎºÎ³+/2
0
|Gâ€²1(x; ÏÎ½ + , Î³, /2)|dx
â‰¤ C â€²3(Î¸1)(nâˆ’5/48 + C4(
N âˆ’M
n
âˆ’ Î³)) (ÎºÎ³ + /2)
(ÏÎ½ + /2âˆ’ ÎºÎ³)2
(45)
uniformly in Î¸1 â‰¤ Nâˆ’Mn â‰¤ 1, where C3, C
â€²
3 are constants depending on Î¸1 and C4(Â·) is a nonnegative
function converging to 0 at 0. Observe thatâˆ«
G1(x; ÏÎ½ + , Î³, /2)dFÎ³(x) =
âˆ«
1
ÏÎ½ + âˆ’ x
dFÎ³(x)
Therefore âˆƒ n1(Î½, ,Î›, Î³) â‰¥ 1 such that for n â‰¥ n1(Î½, ,Î›, Î³),
|(ÏÎ½ + )E(
1
n
trace G1(SBB; ÏÎ½ + , Î³, /2))âˆ’
N âˆ’M
n
âˆ’ Î³
âˆ«
x
ÏÎ½ + âˆ’ x
dFÎ³(x)| â‰¤ Î´Ìƒ1/4 (46)
Combining (40), (41), (42) and (46), for Î´Ìƒ1 < 4 min{1, ÎºÎ³+/2ÏÎ½+/2âˆ’ÎºÎ³ },
P(|DÌƒÎ½,jj | â‰¥ Î´jj , Âµ1 < ÎºÎ³ + /2)
â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
1
96
)
+ 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ + /2âˆ’ ÎºÎ³)2nÎ´Ìƒ21
96(ÎºÎ³ + /2)2
)
+2 exp
(
âˆ’ n
n+N âˆ’M
n2Î´Ìƒ21
2
(ÏÎ½ + /2âˆ’ ÎºÎ³)4
64(ÏÎ½ + )2(ÎºÎ³ + /2)
)
for 1 â‰¤ j â‰¤M âˆ’ Î½ + 1 (47)
for all n â‰¥ n1(Î½, ,Î›, Î³).
Remark : Since the upper bounds in (39) and (47) involve quantities Î´Ìƒ2 and Î´Ìƒ1, respectively, it is
important to clarify their behaviour vis-a-vis  when  â†’ 0. Since Î´Ìƒ1, Î´Ìƒ2 are proportional to Î´(),
defined by (33), we study the latter. From (33), we get
dÎ´()
d
= 1 + `Î½Î³
âˆ«
x
(ÏÎ½ + âˆ’ x)2
dFÎ³(x)â†’ 1 +
`Î½Î³
(`Î½ âˆ’ 1)2 âˆ’ Î³
as â†’ 0,
by Lemma B.2. This shows that âˆƒ 0 < c1 < c2 <âˆ such that c1 â‰¤ Î´() â‰¤ c2 for  small enough.
16
4.3 Lower bound for Ì‚Ì€Î½
Now, we derive a sharp lower bound for Ì‚Ì€Î½ under the restriction that {Ì‚Ì€Î½ > ÎºÎ³ + /2 > Âµ1} for
 > 0 small enough so that ÎºÎ³ + 2 < ÏÎ½ . Then utilizing the lower bound in (21) in a way very
similiar to the proof of Proposition 3, we obtain :
Proposition 4 : Let `Î½ > 1 +
âˆš
Î³ for some 1 â‰¤ Î½ â‰¤M . Let  > 0 be such that ÏÎ½ > ÎºÎ³ + 2. Then
there exists n2(Î½, ,Î›, Î³) large enough such that, for n â‰¥ n2(Î½, ,Î›, Î³),
P(Ì‚Ì€Î½ < ÏÎ½ âˆ’ , Âµ1 < ÎºÎ³ + /2, Î»Î½(SÎ“Î½ ) > ÎºÎ³ + /2) â‰¤ Î½ÎµÌƒ1(n, ,Î›, Î³) + Î½(Î½ âˆ’ 1)ÎµÌƒ2(n, ,Î›, Î³) (48)
where ÎµÌƒ1 and ÎµÌƒ2 are given by the RHS of (53) and (52), respectively.
Proof : Define Î»Ì‚Î½,Î½ = Î»Î½(SÎ“Î½ ). From (21) we have
P(Ì‚Ì€Î½ < ÏÎ½ âˆ’ , Âµ1 < ÎºÎ³ + /2, Î»Ì‚Î½,Î½ > ÎºÎ³ + /2) â‰¤ P(Î»Ì‚Î½,Î½ < ÏÎ½ âˆ’ , Âµ1 < ÎºÎ³ + /2, Î»Ì‚Î½,Î½ > ÎºÎ³ + /2)
Since Î»Ì‚Î½,Î½ is an eigenvalue of KÎ“Î½ (Î»Ì‚Î½,Î½),
Î»Ì‚Î½,Î½ â‰¥ Î»Î½(KÎ“Î½ (ÏÎ½ âˆ’ )) on the set JÎ½,Î½ := {ÎºÎ³ + /2 < Î»Ì‚Î½,Î½ < ÏÎ½ âˆ’ , Âµ1 < ÎºÎ³ + /2} (49)
Then, with Î›G(Ï) defined through (19), on the set JÎ½,Î½ ,
Î»Ì‚Î½,Î½ â‰¥ `Î½(1 + Î³
âˆ«
x
ÏÎ½ âˆ’ âˆ’ x
dFÎ³(x))âˆ’ â€– KÎ“Î½ (ÏÎ½ âˆ’ )âˆ’ Î›Î“Î½ (ÏÎ½ âˆ’ ) â€–HS (50)
since `Î½(1 + Î³
âˆ«
x
ÏÎ½âˆ’âˆ’xdFÎ³(x)) is the smallest eigenvalue of Î›Î“Î½ (ÏÎ½ âˆ’ ).
Now, let Î´ := Î´() > 0 be such that `Î½(1 + Î³
âˆ«
x
ÏÎ½âˆ’âˆ’xdFÎ³(x)) âˆ’ Î´ = ÏÎ½ âˆ’ . We can find such
a Î´ because if we define `Î½, to be `Î½, = (ÏÎ½ âˆ’ )(1 + Î³
âˆ«
x
ÏÎ½âˆ’âˆ’xdFÎ³(x))
âˆ’1, then we have `Î½, < `Î½
(since, by definition, ÏÎ½ is the solution to equation (3)), and hence
`Î½(1 + Î³
âˆ«
x
ÏÎ½ âˆ’ âˆ’ x
dFÎ³(x)) =
`Î½
`Î½,
(ÏÎ½ âˆ’ ) > ÏÎ½ âˆ’ 
Note also that Î´ â†“ 0 as  â†“ 0. Denote the matrix appearing inside â€– â€–HS on the RHS of (50) by
DÎ½ = ((DÎ½,jk))Î½j,k=1. From our construction, if
|DÎ½,jk| â‰¤ Î´jk, 1 â‰¤ j, k â‰¤ Î½, where Î´jk > 0, âˆ€ j, k, and
Î½âˆ‘
j=1
Î½âˆ‘
k=1
Î´
2
jk â‰¤ Î´
2
,
then from (50) we get, on JÎ½,Î½ , Î»Ì‚Î½,Î½ > ÏÎ½ âˆ’  which is impossible. Hence after taking the union
bound,
P(JÎ½,Î½) â‰¤
Î½âˆ‘
j=1
P(|DÎ½,jj | â‰¥ Î´jj , Âµ1 < ÎºÎ³ + /2) +
Î½âˆ‘
j<k
P(|DÎ½,jk| â‰¥ Î´jk, Âµ1 < ÎºÎ³ + /2) (51)
17
We set Î´jj = `j Î´Ìƒ3, j = 1, . . . , Î½ and Î´jk =
âˆš
`j`k Î´Ìƒ4, 1 â‰¤ j < k â‰¤ Î½ where Î´Ìƒ3, Î´Ìƒ4 > 0 are
such that Î´2 = Î´Ìƒ23
âˆ‘Î½
j=1 `
2
j + Î´Ìƒ
2
4(
âˆ‘Î½
j=1 `j)
2. To be specific, we take Î´Ìƒ4 = Î´âˆš2(
âˆ‘Î½
j=1 `j)
âˆ’1 and Î´Ìƒ3 =
Î´âˆš
2
(
âˆ‘Î½
j=1 `
2
j )
âˆ’1/2.
Therefore by derivations similar to (39), we have for 0 < Î´Ìƒ4 < min{1, ÎºÎ³+/2ÏÎ½âˆ’3/2âˆ’ÎºÎ³ },
P(|DÎ½,jk| â‰¥ Î´jk, Âµ1 < ÎºÎ³ + /2) â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
4
12
)
+ 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ âˆ’ 3/2âˆ’ ÎºÎ³)2nÎ´Ìƒ24
12(ÎºÎ³ + /2)2
)
(52)
for all 1 â‰¤ j < k â‰¤ Î½
Similarly, âˆƒ n2(Î½, ,Î›, Î³) such that for Î´Ìƒ3 < 4 min{1, ÎºÎ³+/2)ÏÎ½âˆ’3/2âˆ’ÎºÎ³ },
P(|DÎ½,jj | â‰¥ Î´jj , Âµ1 < ÎºÎ³ + /2)
â‰¤ 2 exp
(
âˆ’nÎ´Ìƒ
2
3
96
)
+ 2 exp
(
âˆ’ n
N âˆ’M
(ÏÎ½ âˆ’ 3/2âˆ’ ÎºÎ³)2nÎ´Ìƒ23
96(ÎºÎ³ + /2)2
)
+2 exp
(
âˆ’ n
n+N âˆ’M
n2Î´Ìƒ23
2
(ÏÎ½ âˆ’ 3/2âˆ’ ÎºÎ³)4
64(ÏÎ½ âˆ’ )2(ÎºÎ³ + /2)
)
for 1 â‰¤ j â‰¤ Î½ (53)
for all n â‰¥ n2(Î½, ,Î›, Î³).
Proof of Theorem 2 : The remark following Proposition 3 remains valid for Proposition 4 as well
(possibly with different constants). And so, the proof in the case when `Î½ > (1 +
âˆš
Î³)2 now follows
easily by combining Proposition 1, Proposition 2, Proposition 3 and Proposition 4 and applying
first Borel-Cantelli lemma.
Proof for the general case is deduced by combining Proposition 2, Proposition 3 and Proposition
B.2 and then applying first Borel-Cantelli lemma.
4.4 Proof of Theorem 1
By interlacing inequality, it follows that Ì‚Ì€Î½ â‰¥ ÂµÎ½ . Proposition 2 and the remark following that
ensure that ÂµÎ½ concentrates around ÎºÎ³ . In view of this we only need to show that for every  > 0
the probability P(Ì‚Ì€Î½ > ÎºÎ³ + ) is summable over n, so that an Application of Borel-Cantelli lemma
will complete the proof.
We take essentially the same approach as in proving Proposition 3. As before, we denote
Î»1(SÎ“Î½âˆ’1) by Î»Ì‚1,Î½ and use (21). So we only need to ensure that P(Î»Ì‚1,Î½ > ÎºÎ³ + , Âµ1 < ÎºÎ³ + /2) is
summable. As before, consider the set JÌƒ1,Î½ := {Î»Ì‚1,Î½ > ÎºÎ³ + , Âµ1 < ÎºÎ³ + /2}. Then, since Î»Ì‚1,Î½ is
an eigenvalue of KÎ“Î½âˆ’1(Î»Ì‚1,Î½), with KG(Â·) defined by (22), we have
Î»Ì‚1,Î½ â‰¤ Î»1(KÎ“Î½âˆ’1(ÎºÎ³ + )) on the set JÌƒ1,Î½ . (54)
Then on the set JÌƒ1,Î½ ,
Î»Ì‚1,Î½ â‰¤ `Î½(1 + Î³
âˆ«
x
ÎºÎ³ + âˆ’ x
dFÎ³(x))+ â€– KÎ“Î½âˆ’1(ÎºÎ³ + )âˆ’ Î›Î“Î½âˆ’1(ÎºÎ³ + ) â€–HS (55)
18
since `Î½(1 + Î³
âˆ«
x
ÎºÎ³+âˆ’xdFÎ³(x)) is the largest eigenvalue of Î›Î“Î½âˆ’1(ÎºÎ³ + ), where the last quantity
is defined through (19).
âˆƒ Î´ := Î´() > 0 such that
`Î½(1 + Î³
âˆ«
x
ÎºÎ³ + âˆ’ x
dFÎ³(x)) + Î´ = ÎºÎ³ +  (56)
This is because, if we define ÎºÎ³, = (ÎºÎ³ + )(1 +Î³
âˆ«
x
ÎºÎ³+âˆ’xdFÎ³(x))
âˆ’1, then ÎºÎ³, > 1 +
âˆš
Î³ â‰¥ `Î½ since
whenever Ï > ÎºÎ³ , âˆƒ a unique ` > 1 +
âˆš
Î³ which solves the equation (3), so that
`Î½(1 + Î³
âˆ«
x
ÎºÎ³ + âˆ’ x
dFÎ³(x)) =
`Î½
ÎºÎ³,
(ÎºÎ³ + ) < ÎºÎ³ + 
As should be clear by now, the proof of Proposition 3 can now be followed verbatim just by replacing
the quantity ÏÎ½ +  by ÎºÎ½ + , and replacing J1,Î½ by JÌƒ1,Î½ . Thus, skipping all the details we simply
present the final result :
Proposition 5 : Let `Î½ â‰¤ 1 +
âˆš
Î³. With Î´Ìƒ1 and Î´Ìƒ2 having the same definition as in the proof of
Proposition 3, and Î´ = Î´() defined through (56) sufficiently small, âˆƒ n3(Î½, ,Î›, Î³) large enough so
that for n â‰¥ n3(Î½, ,Î›, Î³)
P(Ì‚Ì€Î½ > ÎºÎ³ + , Âµ1 < ÎºÎ³ + /2) â‰¤ (M âˆ’ Î½ + 1)Îµ1(n, ,Î›, Î³) + (M âˆ’ Î½)(M âˆ’ Î½ + 1)Îµ2(n, ,Î›, Î³, )
(57)
where
Îµ1 = 2 exp
(
âˆ’nÎ´Ìƒ
2
1
96
)
+ 2 exp
(
âˆ’ n
N âˆ’M
nÎ´Ìƒ21
2
384(ÎºÎ³ + /2)2
)
+ 2 exp
(
âˆ’ n
n+N âˆ’M
n2Î´Ìƒ21
2
4
1024(ÎºÎ³ + )2(ÎºÎ³ + /2)
)
Îµ2 = 2 exp
(
âˆ’nÎ´Ìƒ
2
2
12
)
+ 2 exp
(
âˆ’ n
N âˆ’M
nÎ´Ìƒ22
2
48(ÎºÎ³ + /2)2
)
Remark : It is important to take note of the behaviour of Î´(). When `Î½ < 1 +
âˆš
Î³, (56) implies
that as  â†“ 0, Î´()â†’ ÎºÎ³âˆ’ `Î½(1 +Î³
âˆ«
x
ÎºÎ³âˆ’xdFÎ³(x)), by Monotone Convergence Theorem. By Lemma
B.1, the limit equals ÎºÎ³ âˆ’ `Î½(1 +
âˆš
Î³) > 0. This shows that Î´() is bounded below, and so the same
is true for Î´Ìƒ1 and Î´Ìƒ2. However, if `Î½ = 1 +
âˆš
Î³ then Î´()â†’ 0 as  â†“ 0 and therefore we need to do a
more careful analysis to ensure that the bound in (57) is meaningful. Again using Lemma B.1, we
19
can write, for 0 <  < 2Î³,
Î´() = + (1 +
âˆš
Î³)Î³
[âˆ«
x
ÎºÎ³ âˆ’ x
dFÎ³(x)âˆ’
âˆ«
x
ÎºÎ³ + âˆ’ x
dFÎ³(x)
]
= + (1 +
âˆš
Î³)Î³
âˆ« ÎºÎ³
(1âˆ’âˆšÎ³)2
x
(ÎºÎ³ + âˆ’ x)(ÎºÎ³ âˆ’ x)
1
2Ï€Î³x
âˆš
(ÎºÎ³ âˆ’ x)(xâˆ’ (1âˆ’
âˆš
Î³)2)dx
> +
(1 +
âˆš
Î³)
2Ï€
âˆ« ÎºÎ³
ÎºÎ³âˆ’
âˆš
ÎºÎ³ âˆ’ âˆ’ (1âˆ’
âˆš
Î³)2
(ÎºÎ³ + âˆ’ x)
âˆš
ÎºÎ³ âˆ’ x
dx
> +
(1 +
âˆš
Î³)
2Ï€
Â·  Â·
âˆš
4Î³ âˆ’ 
2
âˆš

>
âˆš

[âˆš
+
(1 +
âˆš
Î³)
âˆš
Î³
2
âˆš
2Ï€
]
Which means that for 0 <  < 0, say, Î´() > c
âˆš
 for some constant c > 0 and so the bound (57)
is strong enough.
Thus the proof of summability of P(Ì‚Ì€Î½ > ÎºÎ³ + ) is completed by combining Proposition 5 with
Proposition 2.
5 Proof of Theorem 3
The proof involves several parts. The first step is to utilize the eigen-equation (15) to get
Ì‚Ì€
Î½ = bTÎ½ (SAA + Î›
1/2T TM(Ì‚Ì€Î½I âˆ’M)âˆ’1TÎ›1/2)bÎ½ (58)
Next step is to show that
bÎ½ âˆ’ eÎ½ = âˆ’[RÎ½(K(ÏÎ½)âˆ’
ÏÎ½
`Î½
Î›)eÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)RÎ½KÎ½(ÏÎ½)eÎ½ ] + (Ì‚Ì€Î½ âˆ’ ÏÎ½)2OP (1) + oP (nâˆ’1/2) (59)
where K(x) is defined by (16), RÎ½ is a deterministic diagonal matrix, and K(ÏÎ½) is a stochastic
matrix with norm OP (nâˆ’1/2). This is done in Section 6. Then we can write (see Section 6.1 ), after
expanding K(Ì‚Ì€Î½) appearing in (58) around ÏÎ½ , using (59), changing sides, and finally multiplying
by
âˆš
n,
âˆš
n(Ì‚Ì€Î½ âˆ’ ÏÎ½)(1 + `Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’2tÎ½ + dÎ½) = âˆšn(sÎ½Î½ + `Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’1tÎ½ âˆ’ ÏÎ½) + oP (1)
(60)
where dÎ½ = âˆ’`Î½(Ì‚Ì€Î½ âˆ’ÏÎ½)(tTÎ½M(ÏÎ½Iâˆ’M)âˆ’2(Ì‚Ì€Î½Iâˆ’M)âˆ’1tÎ½ +OP (1)) and sÎ½Î½ is the (Î½, Î½)-th element
of S. It readily follows that dÎ½ = oP (1). We first show that the term on the RHS of (60) converges
in distribution to a Gaussian random variable with zero mean and variance given by
2`Î½ÏÎ½
(
1 + `Î½Î³
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x)
)
(61)
Next, from Proposition 6 stated below, it follows that
tTÎ½M(ÏÎ½I âˆ’M)âˆ’2tÎ½
a.s.âˆ’â†’ Î³
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x) (62)
20
Hence (4), with Ïƒ2(`) given by the first expression in (5), follows from (61), (62) and (60) once we
apply Slutskyâ€™s theorem. Application of (99) gives the second equality in (5) and the third follows
from simple algebra.
Proposition 6 : Suppose Nn â†’ Î³ âˆˆ (0, 1) as nâ†’âˆ. Let Î´,  > 0 be such that Î´ <
16(ÎºÎ³+/2)
2
, and
Ï â‰¥ ÎºÎ³ + . Then âˆƒ n4(Ï, Î´, , Î³) such that for all n â‰¥ n4(Ï, Î´, , Î³),
P(|tTjM(ÏI âˆ’M)âˆ’2tj âˆ’ Î³
âˆ«
x
(Ïâˆ’ x)2
dFÎ³(x)| > Î´, Âµ1 < ÎºÎ³ + /2)
â‰¤ 2 exp
(
âˆ’ n
N âˆ’M
n(Î´/4)2(Ïâˆ’ ÎºÎ³ âˆ’ /2)4
6(ÎºÎ³ + /2)2
)
+ 2 exp
(
âˆ’ n
n+N âˆ’M
n2(Î´/4)2
2
(Ïâˆ’ ÎºÎ³ âˆ’ /2)6
16Ï2(ÎºÎ³ + /2)
)
+2 exp
(
âˆ’ n
n+N âˆ’M
n2(Î´/4)2
2
(Ïâˆ’ ÎºÎ³ âˆ’ /2)4
4(ÎºÎ³ + /2)
)
, 1 â‰¤ j â‰¤M,
The proof of this proposition is given in Appendix B.
The main term on the RHS of (60) can be expressed as Wn +W â€²n, where
Wn =
âˆš
n(sÎ½Î½ âˆ’ (1âˆ’ Î³)`Î½ + `Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’1tÎ½ âˆ’ `Î½ÏÎ½
1
n
trace((ÏÎ½I âˆ’M)âˆ’1))
and
W â€²n =
âˆš
n`Î½(ÏÎ½
1
n
trace((ÏÎ½I âˆ’M)âˆ’1)âˆ’
Î³`Î½
`Î½ âˆ’ 1
)
Note that by (97),
Î³`Î½
`Î½ âˆ’ 1
= Î³(1 +
1
`Î½ âˆ’ 1
) = Î³
âˆ«
ÏÎ½
ÏÎ½ âˆ’ x
dFÎ³(x)
On the other hand
ÏÎ½
1
n
trace((ÏÎ½I âˆ’M)âˆ’1) =
N âˆ’M
n
âˆ«
ÏÎ½
ÏÎ½ âˆ’ x
FÌ‚n,Nâˆ’M (x)
Since the function 1ÏÎ½âˆ’z is analytic in an open set containing the interval [(1âˆ’
âˆš
Î³)2, (1+
âˆš
Î³)2], from
Bai and Silverstein (2004, Theorem 1.1) the sequence W â€²n = oP (1) once we invoke
N
n âˆ’Î³ = o(n
âˆ’1/2).
Remark : The result of Bai and Silverstein (2004) is actually much stronger than what we need.
They also show the result under fairly weak conditions. From their result one can deduce asymptotic
normality of the sequence
âˆš
nW â€²n if we replace Î³ by
Nâˆ’M
n . However, for our purpose we only need
that W â€²n = oP (1).
5.1 Asymptotic normality of Wn
First we recall that by definition of T , tÎ½ = 1âˆšnH
TZA,Î½ where ZTA,Î½ is the Î½-th row of ZA. Since
N âˆ’M < n, and columns of H are orthonormal, we can extend them to form an orthonormal basis
21
of Rn given by the matrix HÌƒ = [H : Hc] where Hc is nÃ— (nâˆ’N +M). Thus, HÌƒHÌƒT = HÌƒT HÌƒ = In.
Then writing
sÎ½Î½ = `Î½
1
n
ZTA,Î½ZA,Î½ = `Î½
1
n
ZTA,Î½HÌƒHÌƒ
TZA,Î½ = `Î½(â€–
1âˆš
n
HTZA,Î½ â€–2 + â€–
1âˆš
n
HTc ZA,Î½ â€–2)
= `Î½(â€– tÎ½ â€–2 + â€– wÎ½ â€–2)
with wÎ½ := 1âˆšnH
T
c ZA,Î½ , we have wÎ½ âˆ¼ N(0, 1nInâˆ’N+M ), tÎ½ âˆ¼ N(0,
1
nINâˆ’M ), and these are mutu-
ally independent and independent of ZB (since HÌƒ is an orthonormal basis and ZA,Î½ âˆ¼ N(0, In)).
Therefore we can decompose Wn as a sum of two independent random variables W1,n and W2,n
where
W1,n = `Î½
âˆš
n(â€– wÎ½ â€–2 âˆ’(1âˆ’ Î³)), and W2,n = `Î½ÏÎ½
âˆš
n(tTÎ½ (ÏÎ½I âˆ’M)âˆ’1tÎ½ âˆ’
1
n
trace((ÏÎ½I âˆ’M)âˆ’1))
Since n â€– wÎ½ â€–2âˆ¼ Ï‡2nâˆ’N+M and
N
n âˆ’ Î³ = o(n
âˆ’1/2), we get W1,n =â‡’ N(0, 2`2Î½(1 âˆ’ Î³)). In Section
5.2 we prove that
W2,n =â‡’ N(0, 2`2Î½Î³
âˆ«
Ï2Î½
(ÏÎ½ âˆ’ x)2
dFÎ³(x)). (63)
The asymptotic normality of Wn is therefore established. Since W â€²n = oP (1) this implies asymptotic
normality of the RHS of (60). The expression (61) for asymptotic variance is then deduced as
follows:âˆ«
Ï2Î½
(ÏÎ½ âˆ’ x)2
dFÎ³(x) = 1 + 2
âˆ«
x
(ÏÎ½ âˆ’ x)
dFÎ³(x) +
âˆ«
x2
(ÏÎ½ âˆ’ x)2
dFÎ³(x)
= 1 + 2
âˆ«
x
ÏÎ½ âˆ’ x
dFÎ³(x) + ÏÎ½
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x)âˆ’
âˆ«
x
ÏÎ½ âˆ’ x
dFÎ³(x)
= 1 +
1
`Î½ âˆ’ 1
+ ÏÎ½
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x)
where in the last step we used (97). Therefore the asymptotic variance of Wn is
2`2Î½(1âˆ’ Î³) + 2`2Î½Î³
âˆ«
Ï2Î½
(ÏÎ½ âˆ’ x)2
dFÎ³(x) = 2`2Î½(1 +
Î³
`Î½ âˆ’ 1
) + 2`2Î½ÏÎ½Î³
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x),
from which (61) follows since `Î½(1 + Î³`Î½âˆ’1) = ÏÎ½ .
5.2 Proof of (63)
Let tÎ½ = (tÎ½,1, . . . , tÎ½,Nâˆ’M )T . tÎ½,j
i.i.d.âˆ¼ N(0, 1n) and independent of M. Hence defining yj =
âˆš
ntÎ½,j ,
we have
W2,n = `Î½ÏÎ½
1âˆš
n
ï£«ï£­Nâˆ’Mâˆ‘
j=1
1
ÏÎ½ âˆ’ Âµj
y2j âˆ’
Nâˆ’Mâˆ‘
j=1
1
ÏÎ½ âˆ’ Âµj
ï£¶ï£¸ , where {yj}Nâˆ’Mj=1 i.i.d.âˆ¼ N(0, 1),
22
and {yj}Nâˆ’Mj=1 are independent of M. Thus, given M, W2,n is a weighted sum of i.i.d. mean 0
random variables. To establish (63) we need to show that
Ï†W2,n(t) := E exp(itW2,n)â†’ Ï†ÏƒÌƒ2(`Î½)(t) := exp
(
âˆ’ t
2ÏƒÌƒ2(`Î½)
2
)
, for all t âˆˆ R, as nâ†’âˆ
where ÏƒÌƒ2(`) = 2`2Î³
âˆ« Ï2(`)
(Ï2(`)âˆ’x)2dFÎ³(x) for ` > 1 +
âˆš
Î³. It is enough to show that
E
âˆ£âˆ£âˆ£âˆ£E(eitW2,n | M) exp( t2ÏƒÌƒ2(`Î½)2
)
âˆ’ 1
âˆ£âˆ£âˆ£âˆ£â†’ 0, for all t âˆˆ R, as nâ†’âˆ
where the outer expectation is with respect to the distribution of M. We break this expectation
into two parts, one over the set JÎ³(Î´) := {Âµ1 â‰¤ ÎºÎ³ + Î´} where Î´ > 0 is any number such that
ÏÎ½ > ÎºÎ³ + 2Î´, and the complementary part over the set JcÎ³(Î´) = {Âµ1 > ÎºÎ³ + Î´}. Note that JÎ³(Î´) is
a measurable set that depends on n and P(JÎ³(Î´))â†’ 1 as nâ†’âˆ. Since the inner expectation is a
bounded r.v., the second term converges to zero. Thus we only need to establish that
E
[âˆ£âˆ£âˆ£âˆ£E(eitW2,n | M) exp( t2ÏƒÌƒ2(`Î½)2
)
âˆ’ 1
âˆ£âˆ£âˆ£âˆ£ , Âµ1 â‰¤ ÎºÎ³ + Î´]â†’ 0, for all t âˆˆ R, as nâ†’âˆ
(64)
Since characteristic function of a Ï‡21 random variable at any point t is given by Ïˆ(t) :=
1âˆš
1âˆ’2it , on
the set {Âµ1 â‰¤ ÎºÎ³ + Î´} the inner conditional expectation is
Nâˆ’Mâˆ
j=1
Ïˆ
(
t`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ Âµj)
)
exp
ï£«ï£­âˆ’ it`Î½ÏÎ½âˆš
n
Nâˆ’Mâˆ‘
j=1
1
ÏÎ½ âˆ’ Âµj
ï£¶ï£¸
=
Nâˆ’Mâˆ
j=1
(
1âˆ’ 2it`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ Âµj)
)âˆ’1/2
exp
ï£«ï£­âˆ’ it`Î½ÏÎ½âˆš
n
Nâˆ’Mâˆ‘
j=1
1
ÏÎ½ âˆ’ Âµj
ï£¶ï£¸ (65)
Denoting by log z (z âˆˆ C) the principal branch of the complex logarithm we have(
1âˆ’ 2it`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ Âµj)
)âˆ’1/2
= exp
(
âˆ’1
2
log
(
1âˆ’ 2it`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ Âµj)
))
Recalling the Taylor series expansion of log(1+z) (valid for |z| < 1), we can write, for n â‰¥ nâˆ—(Î½, Î³, Î´),
large enough so that |t|`Î½ÏÎ½âˆš
n(ÏÎ½âˆ’ÎºÎ³âˆ’Î´) <
1
2 , the conditional expectation (65) as
exp
ï£«ï£­1
2
Nâˆ’Mâˆ‘
j=1
âˆâˆ‘
k=1
1
k
(
2it`Î½ÏÎ½âˆš
n
1
ÏÎ½ âˆ’ Âµj
)k
âˆ’ it`Î½ÏÎ½âˆš
n
Nâˆ’Mâˆ‘
j=1
1
ÏÎ½ âˆ’ Âµj
ï£¶ï£¸
The inner sum is dominated by a geometric series and hence finite for n â‰¥ nâˆ—(Î½, Î³, Î´) on the set
JÎ³(Î´). Interchanging the order of summations, on JÎ³(Î´), the term within exponent becomes
1
2
âˆâˆ‘
k=2
1
k
(
2it`Î½ÏÎ½âˆš
n
)k Nâˆ’Mâˆ‘
j=1
1
(ÏÎ½ âˆ’ Âµj)k
= âˆ’ t
2
2
ï£®ï£°2`2Î½Ï2Î½ 1n
Nâˆ’Mâˆ‘
j=1
1
(ÏÎ½ âˆ’ Âµj)2
ï£¹ï£»+ 1
2
âˆâˆ‘
k=3
1
k
(
2it`Î½ÏÎ½âˆš
n
)k Nâˆ’Mâˆ‘
j=1
1
(ÏÎ½ âˆ’ Âµj)k
(66)
23
Denoting the first term of (66) by an(t) and the second term by rÌƒn(t), for n â‰¥ nâˆ—(Î½, Î³, Î´), on JÎ³(Î´),
|rÌƒn(t)| â‰¤
t2
3
ï£®ï£°2`2Î½Ï2Î½ 1n
Nâˆ’Mâˆ‘
j=1
1
(ÏÎ½ âˆ’ Âµj)2
ï£¹ï£» âˆâˆ‘
k=1
(
2|t|`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ ÎºÎ³ âˆ’ Î´)
)k
=
t2
3
ï£®ï£°2`2Î½Ï2Î½ 1n
Nâˆ’Mâˆ‘
j=1
1
(ÏÎ½ âˆ’ Âµj)2
ï£¹ï£»( 2|t|`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ ÎºÎ³ âˆ’ Î´)
)(
1âˆ’ 2|t|`Î½ÏÎ½âˆš
n(ÏÎ½ âˆ’ ÎºÎ³ âˆ’ Î´)
)âˆ’1
(67)
Let G2(Â· ; Ï, Î³, Î´) to be the bounded function (defined for Ï > ÎºÎ³ + Î´) defined through (90) in the
appendix. Then on JÎ³(Î´), 1n
âˆ‘Nâˆ’M
j=1
1
(ÏÎ½âˆ’Âµj)2 =
Nâˆ’M
n
âˆ«
G2(x; ÏÎ½ , Î³, Î´)dFÌ‚n,Nâˆ’M (x) and the quantity
on the RHS converges almost surely to Î³
âˆ«
G2(x; ÏÎ½ , Î³, Î´)dFÎ³(x) = Î³
âˆ«
1
(ÏÎ½âˆ’x)2dFÎ³(x). Moreover, on
JÎ³(Î´), an(t) and rÌƒn(t) are bounded for n â‰¥ nâˆ—(Î½, Î³, Î´). Therefore, from this observation and (65)
and (66),
E
[âˆ£âˆ£âˆ£âˆ£E(eitW2,n | M) exp( t2ÏƒÌƒ2(`Î½)2
)
âˆ’ 1
âˆ£âˆ£âˆ£âˆ£ , JÎ³(Î´)] = E [| exp(an(t) + rÌƒn(t) + t2ÏƒÌƒ2(`Î½)2
)
âˆ’ 1|IJÎ³(Î´)
]
â‰¤ E
[
exp
(
an(t) +
t2ÏƒÌƒ2(`Î½)
2
)
(exp (|rÌƒn(t)|)âˆ’ 1) IJÎ³(Î´)
]
+ E
[
| exp
(
an(t) +
t2ÏƒÌƒ2(`Î½)
2
)
âˆ’ 1| IJÎ³(Î´)
]
â†’ 0 + 0, as nâ†’âˆ
by bounded convergence theorem. Since t âˆˆ R is arbitrary, (64) follows.
6 Approximation to the eigenvectors
In this section we derive a first order asymptotic expansion of the vector bÎ½ associated with the
eigenvalue Ì‚Ì€Î½ , when `Î½ is greater than 1 +âˆšÎ³ and has multiplicity 1. This expansion has already
been used in the proof of Theorem 3. We proceed with the standard perturbation analysis approach.
Our construction follows Kneip and Utikal (2001), (see also Kato, 1980, Chapter 2). First observe
that ÏÎ½ is the eigenvalue of ÏÎ½`Î½ Î› associated with the eigenvector eÎ½ . Define
RÎ½ =
Mâˆ‘
k 6=Î½
`Î½
ÏÎ½(`k âˆ’ `Î½)
ekeTk (68)
Note that RÎ½ is the resolvent of ÏÎ½`Î½ Î› â€œevaluatedâ€ at ÏÎ½ . Then utilizing the defining equation (15)
we can express
(
ÏÎ½
`Î½
Î›âˆ’ ÏÎ½I)bÎ½ = âˆ’(K(Ì‚Ì€Î½)âˆ’ ÏÎ½
`Î½
Î›)bÎ½ + (Ì‚Ì€Î½ âˆ’ ÏÎ½)bÎ½
Defining DÎ½ = K(Ì‚Ì€Î½)âˆ’ ÏÎ½`Î½ Î›Î½ , premultiplying both sides by RÎ½ and observing that RÎ½(ÏÎ½`Î½ Î›âˆ’ÏÎ½I) =
IM âˆ’ eÎ½eTÎ½ := PâŠ¥Î½ we get,
PâŠ¥Î½ bÎ½ = âˆ’RÎ½DÎ½bÎ½ + (Ì‚Ì€Î½ âˆ’ ÏÎ½)RÎ½bÎ½ (69)
24
As a convention let us suppose ã€ˆeÎ½ , bÎ½ã€‰ â‰¥ 0. Then expressing bÎ½ = ã€ˆeÎ½ , bÎ½ã€‰eÎ½ +PâŠ¥Î½ bÎ½ and observing
that RÎ½eÎ½ = 0, we get
bÎ½ âˆ’ eÎ½ = âˆ’RÎ½DÎ½eÎ½ + rÎ½ (70)
where
rÎ½ = âˆ’(1âˆ’ ã€ˆeÎ½ , bÎ½ã€‰)eÎ½ âˆ’RÎ½DÎ½(bÎ½ âˆ’ eÎ½) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)RÎ½(bÎ½ âˆ’ eÎ½)
Now, define
Î±Î½ =â€– RÎ½DÎ½ â€– +|Ì‚Ì€Î½ âˆ’ ÏÎ½ | â€– RÎ½ â€– and Î²Î½ =â€– RÎ½DÎ½eÎ½ â€– (71)
Lemma 1 : rÎ½ satisfies
â€– rÎ½ â€– â‰¤
ï£±ï£²ï£³Î²Î½
(
Î±Î½(1+Î±Î½)
1âˆ’Î±Î½(1+Î±Î½) +
Î²Î½
(1âˆ’Î±Î½(1+Î±Î½))2
)
if Î±Î½ <
âˆš
5âˆ’1
2
Î±2Î½ + 2Î±Î½ always
(72)
Proof : Rewriting (69) we get
PâŠ¥Î½ bÎ½ = âˆ’RÎ½DÎ½eÎ½ âˆ’RÎ½DÎ½(bÎ½ âˆ’ eÎ½) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)RÎ½(bÎ½ âˆ’ eÎ½) (73)
From this
yÎ½ :=â€– PâŠ¥Î½ bÎ½ â€– â‰¤ â€– RÎ½DÎ½eÎ½ â€– +(â€– RÎ½DÎ½ â€– +|Ì‚Ì€Î½ âˆ’ ÏÎ½ | â€– RÎ½ â€–) â€– bÎ½ âˆ’ eÎ½ â€–= Î²Î½ + Î±Î½ â€– bÎ½ âˆ’ eÎ½ â€–
(74)
On the other hand, from the decomposition bÎ½ = ã€ˆeÎ½ , bÎ½ã€‰eÎ½ + PâŠ¥Î½ bÎ½ , and observing that
1âˆ’ ã€ˆeÎ½ , bÎ½ã€‰ = 1âˆ’
âˆš
1âˆ’ â€– PâŠ¥Î½ bÎ½ â€–2 â‰¤â€– PâŠ¥Î½ bÎ½ â€–2,
we also have
â€– bÎ½ âˆ’ eÎ½ â€– â‰¤ â€– PâŠ¥Î½ bÎ½ â€– (1+ â€– PâŠ¥Î½ eÎ½ â€–) = yÎ½(1 + yÎ½) â‰¤ yÎ½(1 + Î±Î½)
where the last inequality is a result of the fact that from (69) one gets â€– PâŠ¥Î½ bÎ½ â€–â‰¤ Î±Î½ . Substituting
this in (74) we get yÎ½ â‰¤ Î²Î½ + Î±Î½(1 + Î±Î½)yÎ½ implying that yÎ½ â‰¤ Î²Î½1âˆ’Î±Î½(1+Î±Î½) whenever Î±Î½ <
âˆš
5âˆ’1
2 .
Therefore, if Î±Î½ <
âˆš
5âˆ’1
2 then â€– bÎ½ âˆ’ eÎ½ â€– â‰¤
Î²Î½(1+Î±Î½)
1âˆ’Î±Î½(1+Î±Î½) . Substituting this in the general bound
â€– rÎ½ â€– â‰¤ â€– PâŠ¥Î½ bÎ½ â€–2 +Î±Î½ â€– bÎ½ âˆ’ eÎ½ â€– (75)
and using the last two relationships, we get the first inequality in (72). The second inequality is a
trivial consequence of (75).
25
Next task is to establish that Î²Î½ = oP (1) and Î±Î½ = oP (1). First, consider the following
decomposition.
DÎ½ = (SAA âˆ’ Î›) + Î›1/2
(
T TM(ÏÎ½I âˆ’M)âˆ’1T âˆ’
1
n
trace(M(ÏÎ½I âˆ’M)âˆ’1)I
)
Î›1/2
+
(
1
n
trace(M(ÏÎ½I âˆ’M)âˆ’1)âˆ’ Î³
âˆ«
x
ÏÎ½ âˆ’ x
dFÎ³(x)
)
Î›
+ (ÏÎ½ âˆ’ Ì‚Ì€Î½)Î›1/2T TM(ÏÎ½I âˆ’M)âˆ’1(Ì‚Ì€Î½I âˆ’M)âˆ’1TÎ›1/2 (76)
Since Ì‚Ì€Î½ a.s.â†’ ÏÎ½ > ÎºÎ³ and Âµ1 a.s.â†’ ÎºÎ³ , in view of the analysis carried out in Section 4, it is straightfor-
ward to see that â€– DÎ½ â€–
a.s.â†’ 0. Therefore, Î±Î½
a.s.â†’ 0 and Î²Î½
a.s.â†’ 0 from the definition (71). However,
because of the special structure, we can get a much better bound for Î²Î½ . For that we need to look
at the term RÎ½DÎ½eÎ½ more closely, which we do now.
Define V (i,Î½) := T TM(ÏÎ½I âˆ’M)âˆ’iT âˆ’ 1n trace(M(ÏÎ½I âˆ’M)
âˆ’i)I for i = 1, 2. Expanding DÎ½
upto second order around ÏÎ½ , and observing that RÎ½âˆ†eÎ½ = 0 for any diagonal matrix âˆ†, we have
RÎ½DÎ½eÎ½ = RÎ½(SAA âˆ’ Î›)eÎ½ +RÎ½Î›1/2V (1,Î½)Î›1/2eÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)RÎ½Î›1/2V (2,Î½)Î›1/2eÎ½
+ (ÏÎ½ âˆ’ Ì‚Ì€Î½)2 [RÎ½Î›1/2T TM(ÏÎ½I âˆ’M)âˆ’2(Ì‚Ì€Î½I âˆ’M)âˆ’1TÎ›1/2eÎ½] (77)
= RÎ½(K(ÏÎ½)âˆ’
ÏÎ½
`Î½
Î›)eÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)RÎ½K(ÏÎ½)eÎ½ + (Ì‚Ì€Î½ âˆ’ ÏÎ½)2rÎ½
where K(ÏÎ½) = Î›1/2V (2,Î½)Î›1/2 and rÎ½ is the vector appearing inside square brackets the second line.
From this expansion and the observations (i) all except the diagonal of the matrix Î›1/2T TM(ÏÎ½Iâˆ’
M)âˆ’iTÎ›1/2eÎ½ is OP (nâˆ’1/2) for i = 1, 2 (obtained through an inequality similar to (38)), (ii) all
except the diagonal of SAA is OP (nâˆ’1/2), and (iii) RÎ½ is diagonal with (Î½, Î½)-th entry equal to 0, it
easily follows that
Î²Î½ = OP (nâˆ’1/2) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)2OP (1). (78)
Remark : The proof of (78) is somewhat long winded and deliberately so. Note that if we use (i)
and (ii) above, the condition Nn âˆ’ Î³ = o(n
âˆ’1/2), and a decomposition similar to the decomposition
of (Î½, Î½)-th element of
âˆš
n(K(ÏÎ½) âˆ’ ÏÎ½`Î½ Î›) as Wn + W
â€²
n, as in the proof of Theorem 3, then by Bai
and Silverstein (2004, Theorem 1.1) we immediately get â€– DÎ½ â€–= OP (nâˆ’1/2) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)OP (1) by
considering a second order expansion of DÎ½ in the spirit of (76). However, the way we have done it,
the bound in (78) is actually a concentration bound and does not depend on the asymptotic limit
theorem of Bai and Silverstein (2004).
As a simple consequence of (78), Lemma 1 and Theorem 3 we get the following:
Corollary 1: When `Î½ > 1 +
âˆš
Î³ and of multiplicity one, bÎ½ = eÎ½ +OP (nâˆ’1/2).
26
6.1 Explanation for expansion (60)
RHS of (58) can be written as
eTÎ½K(Ì‚Ì€Î½)eÎ½ + 2eTÎ½K(Ì‚Ì€Î½)(bÎ½ âˆ’ eÎ½) + (bÎ½ âˆ’ eÎ½)TK(Ì‚Ì€Î½)(bÎ½ âˆ’ eÎ½) (79)
First term in (79) is the major contributor in (60), since it can be written as
sÎ½Î½ + `Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’1tÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)`Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’2tÎ½
+ (ÏÎ½ âˆ’ Ì‚Ì€Î½)2`Î½tTÎ½M(ÏÎ½I âˆ’M)âˆ’2(Ì‚Ì€Î½I âˆ’M)âˆ’1tÎ½
Again, by (70), (71), (72) and (78),
(bÎ½ âˆ’ eÎ½)TK(Ì‚Ì€Î½)(bÎ½ âˆ’ eÎ½) = â€– bÎ½ âˆ’ eÎ½ â€–2 OP (1)
= Î²2Î½ OP (1) = OP (n
âˆ’1) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)2OP (nâˆ’1/2) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)4OP (1)
Finally, to check the negligibility of the second term in (79), we observe that by (70),
eTÎ½K(Ì‚Ì€Î½)(bÎ½ âˆ’eÎ½) = âˆ’eTÎ½DÎ½RÎ½DÎ½eÎ½ + eTÎ½K(Ì‚Ì€Î½)rÎ½ = âˆ’eTÎ½DÎ½RÎ½DÎ½eÎ½ + oP (nâˆ’1/2) + (Ì‚Ì€Î½ âˆ’ÏÎ½)2oP (1),
where in the last step we used (72) together with (78). Expanding DÎ½eÎ½ as in (77), and using the
definition of RÎ½ we get the expression
eTÎ½DÎ½RÎ½DÎ½eÎ½ =
Mâˆ‘
j=1
(RÎ½)jj [(DÎ½eÎ½)j ]2
=
Mâˆ‘
j 6=Î½
`Î½
ÏÎ½
(
`j`Î½
`j âˆ’ `Î½
)[
sjÎ½âˆš
`j`Î½
+ V (1,Î½)jÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)V (2,Î½)jÎ½ + (ÏÎ½ âˆ’ Ì‚Ì€Î½)2VÌƒ (3,Î½)jÎ½
]2
where VÌƒ (3,Î½) = T TM(ÏÎ½I âˆ’M)âˆ’2(Ì‚Ì€Î½I âˆ’M)âˆ’1T . Observe that for j 6= Î½, each of the terms sjÎ½ ,
V
(1,Î½)
jÎ½ and V
(2,Î½)
jÎ½ is OP (n
âˆ’1/2) and VÌƒ (3,Î½)jÎ½ = OP (1). It follows that
eTÎ½DÎ½RÎ½DÎ½eÎ½ = OP (nâˆ’1) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)2OP (nâˆ’1/2) + (Ì‚Ì€Î½ âˆ’ ÏÎ½)4OP (1)
6.2 Proof of Theorem 4
Part (a) : As a convention we choose ã€ˆpÎ½ , eÌƒÎ½ã€‰ â‰¥ 0. First note that with pA,Î½ as in (8)
ã€ˆpÎ½ , eÌƒÎ½ã€‰ = ã€ˆpA,Î½ , eÎ½ã€‰ =
âˆš
1âˆ’R2Î½ ã€ˆbÎ½ , eÎ½ã€‰
Since Î²Î½
a.s.â†’ 0, Î±Î½
a.s.â†’ 0, from (70) and (72), ã€ˆbÎ½ , eÎ½ã€‰
a.s.â†’ 1. Therefore, from (17), (62), Theorem 2
and above display, we have
1
1âˆ’R2Î½
a.s.â†’ 1 + `Î½Î³
âˆ«
x
(ÏÎ½ âˆ’ x)2
dFÎ³(x)
27
from which we get (6) after invoking Lemma B.2.
Part (b) : From (17) it is clear that in order that (7) holds, we need either bTÎ½ Î›
1/2T TM(Ì‚Ì€Î½I âˆ’
M)âˆ’2TÎ›1/2bÎ½
a.s.â†’ âˆ or ã€ˆbÎ½ , eÎ½ã€‰
a.s.â†’ 0. Clearly, we can no longer use the perturbation analysis
argument to study the behaviour of bÎ½ , since in this case Ì‚Ì€Î½ a.s.â†’ ÎºÎ³ . However we shall show that
the smallest eigenvalue of the matrix E := T TM(Ì‚Ì€Î½I âˆ’M)âˆ’2T diverges to infinity almost surely.
This will prove the result.
Our approach will be to show that given  > 0, we can find a C > 0 such that the probability
P(Î»min(E) â‰¤ C) is summable over n and that C â†’âˆ as â†’ 0.
First, denote the rows of T by tTj , j = 1, . . . , N âˆ’M (treated as an 1Ã—M vector). tj â€™s are to
be distinguished from the vectors t1, . . . , tM , the columns of T . In fact tTj = (tj1, . . . , tjM ). Then
E =
Nâˆ’Mâˆ‘
j=1
Âµj
(Ì‚Ì€Î½ âˆ’ Âµj)2 tjtTj â‰¥
Nâˆ’Mâˆ‘
j=Î½
Âµj
(Ì‚Ì€Î½ âˆ’ Âµj)2 tjtTj =: EÎ½ , say,
in the sense of inequalities between positive semi-definite matrices. Thus Î»min(E) â‰¥ Î»min(EÎ½).
Then on the set J1,Î½ := {Ì‚Ì€Î½ < ÎºÎ³ + , Âµ1 < ÎºÎ³ + /2}, we have
EÎ½ â‰¥
Nâˆ’Mâˆ‘
j=Î½
Âµj
(ÎºÎ³ + âˆ’ Âµj)2
tjtTj =: EÎ½
since by interlacing inequality Ì‚Ì€Î½ â‰¥ ÂµÎ½ . Thus, in view of Proposition 5, we only need to provide a
lower bound for the smallest eigenvalue of EÎ½ . However, it will be more convenient to work with
the matrix
E =
Nâˆ’Mâˆ‘
j=1
Âµj
(ÎºÎ³ + âˆ’ Âµj)2
tjtTj = T
TM((ÎºÎ³ + )I âˆ’M)âˆ’2T (80)
Proving summability of P(Î»min(E) â‰¤ C, J1,Î½) suffices because it is easy to see that â€– EÎ½âˆ’E â€–
a.s.â†’ 0
as nâ†’âˆ.
By Proposition 6, and calculations similar to those in deriving (38), respectively, given Î´ > 0,
such that Î´ < 16(ÎºÎ³+/2)
2
, âˆƒ n5(Î´, , Î³) such that for all n â‰¥ n5(Î´, , Î³),
P(|tTjM((ÎºÎ³ + )I âˆ’M)âˆ’2tj âˆ’ Î³
âˆ«
x
(ÎºÎ³ + âˆ’ x)2
dFÎ³(x)| > Î´, J1,Î½)
â‰¤ 2 exp
(
âˆ’ n
N âˆ’M
n(Î´/4)2(/2)4
6(ÎºÎ³ + /2)2
)
+ 2 exp
(
âˆ’ n
n+N âˆ’M
n2(Î´/4)2
2
(/2)6
16(ÎºÎ³ + )2(ÎºÎ³ + /2)
)
+2 exp
(
âˆ’ n
n+N âˆ’M
n2(Î´/4)2
2
(/2)4
4(ÎºÎ³ + /2)
)
, 1 â‰¤ j â‰¤M, (81)
and
P(|tTjM((ÎºÎ³ + )I âˆ’M)âˆ’2tk| > Î´, J1,Î½) â‰¤ 2 exp
(
âˆ’ n
N âˆ’M
nÎ´2(/2)4
3(ÎºÎ³ + /2)2
)
, 1 â‰¤ j 6= k â‰¤M.
(82)
28
If a symmetric matrix A is written as A = B + C where B is a diagonal matrix with the same
diagonal as A, then |Î»min(A)âˆ’ Î»min(B)| â‰¤â€– C â€–HS . If we denote the RHS of (81) and (82) by Îµ5
and Îµ6, respectively, then similar decomposition of the matrix E yields
P(Î»min(E) â‰¤
âˆ«
x
(ÎºÎ³ + âˆ’ x)2
dFÎ³(x)âˆ’ Î´(1 +
âˆš
M(M âˆ’ 1)), J1,Î½) â‰¤MÎµ5 +
M(M âˆ’ 1)
2
Îµ6 (83)
for 0 < Î´ < 16(ÎºÎ³+/2)
2
, and for all n â‰¥ n5(Î´, , Î³).
On the other hand, observe that if 0 <  < 2Î³, thenâˆ«
x
(ÎºÎ³ + âˆ’ x)2
dFÎ³(x) >
âˆ« ÎºÎ³âˆ’/2
ÎºÎ³âˆ’
x
(ÎºÎ³ + âˆ’ x)2
fÎ³(x)dx
=
1
2Ï€Î³
âˆ« ÎºÎ³âˆ’/2
ÎºÎ³âˆ’
âˆš
(ÎºÎ³ âˆ’ x)(xâˆ’ (1âˆ’
âˆš
Î³)2)
(ÎºÎ³ + âˆ’ x)2
dx >
1
2Ï€Î³
[
1
(2)2
âˆš

2
âˆš
ÎºÎ³ âˆ’ âˆ’ (1âˆ’
âˆš
Î³)2
]

2
=
1
16
âˆš
2Ï€Î³
âˆš
4Î³ âˆ’ âˆš

>
1
16
âˆš
Î³Ï€
1âˆš

Therefore set Î´ = (1 +
âˆš
M(M âˆ’ 1))âˆ’1 and choose  small enough so that
âˆš
Î³
16Ï€
1âˆš

âˆ’  > 0. Call
the last quantity C and observe that C satisfies the requirement : C â†’âˆ as â†’ 0. By (83) the
result follows.
7 Appendix A
7.1 Weak concentration inequalities for random quadratic forms
The following two lemmas will be referred to as weak concentration inequalities.
Suppose C : X â†’ RnÃ—n is a measureable function. Let Z be a random variable taking values
in X . Let â€– C â€– denote the operator norm of C, i.e., the largest singular value of K.
Lemma A.1 : Suppose X and Y are i.i.d. Nn(0, I) independent of Z. Then for every L > 0 and
0 < Î´ < 1,
P(
1
n
|XTC(Z)Y | > t, â€– C(Z) â€–â‰¤ L) â‰¤ 2 exp
(
âˆ’(1âˆ’ Î´)nt
2
2L2
)
, for 0 < t <
Î´
1âˆ’ Î´
L (84)
Lemma A.2 : Suppose X is distributed as Nn(0, I) independent of Z. Also let C(z) = CT (z)
for all z âˆˆ X . Let trace(B) denote the trace of a square matrix B. Then, for every L > 0 and
0 < Î´ < 1,
P(
1
n
|XTC(Z)X âˆ’ trace(C(Z))| > t, â€– C(Z) â€–â‰¤ L) â‰¤ 2 exp
(
âˆ’(1âˆ’ Î´)nt
2
4L2
)
, for 0 < t <
2Î´
1âˆ’ Î´
L
(85)
29
Proof of Lemma A.1 : In the proof for convenience we occassionally write C instead of C(Z).
Let 0 < Î» < Î´L . Then if Z âˆˆ DL with DL := {z :â€– C(z) â€–â‰¤ L},
P(
1
n
XTC(Z)Y > t|Z) â‰¤ eâˆ’nÎ»tE
[
eÎ»X
TC(Z)Y |Z
]
= eâˆ’nÎ»tE
[
exp
(
Î»2
2
â€– CY â€–2
)
|Z
]
= eâˆ’nÎ»t(2Ï€)âˆ’n/2
âˆ«
Rn
exp
(
âˆ’1
2
yT (I âˆ’ Î»2CTC)y
)
dy
= eâˆ’nÎ»t det(I âˆ’ Î»2CTC)âˆ’1/2 (86)
The last step is justified by the fact that Î»2 â€– CTC â€–â‰¤ Î»2L2 < 1 (by choice of Î») so the matrix
I âˆ’ Î»2CTC is positive definite on DL. Now, use the fact that log det(I âˆ’ Î»2CTC) =
âˆ‘n
i=1 log(1âˆ’
Î»2Ïƒ2i (C)), where Ïƒi(C) is the i-th largest singular value of C. Thus, since Z âˆˆ DL,
âˆ’ log det(Iâˆ’Î»2CTC) =
nâˆ‘
i=1
âˆâˆ‘
k=1
1
k
(Î»2Ïƒ2i (C))
k â‰¤ Î»2(
nâˆ‘
i=1
Ïƒ2i (C))
âˆâˆ‘
k=1
1
k
(Î»L)2(kâˆ’1) < nÎ»2L2
âˆâˆ‘
k=0
(Î»2L2)k
The geometric series in the last term converges for Î» < 1L and hence combining with (86) we get,
for 0 < Î´ < 1,
P(
1
n
XTC(Z)Y > t|Z) â‰¤ inf
0<Î»< Î´
L
e
âˆ’nÎ»t+ 1
2(1âˆ’Î´2)
nÎ»2L2
< inf
0<Î»< Î´
L
e
âˆ’nÎ»t+ 1
2(1âˆ’Î´)nÎ»
2L2 (87)
The function ft(Î») := âˆ’nÎ»t + 12(1âˆ’Î´)nÎ»
2L2 achieves its global minimum at Î»t =
t(1âˆ’Î´)
L2
. Therefore
if t < Î´L1âˆ’Î´ then Î»t <
Î´
L so that we get the upper bound exp(ft(Î»t)) = âˆ’
(1âˆ’Î´)nt2
2L2
in (87) for Z âˆˆ DL.
By symmetry, the same upper bound holds for P( 1nX
TC(Z)Y < âˆ’t|Z) and combining these two
and then taking expectation w.r.t. the distribution of Z over the set DL we get (84).
Proof of Lemma A.2 : As in the proof of Lemma A.1, for Z âˆˆ DL, 0 < Î» < Î´L , and t > 0,
P(
1
n
(XTC(Z)X âˆ’ trace(C(Z))) > t|Z) â‰¤ eâˆ’
Î»
2
(nt+trace(C(Z)))E
[
e
Î»
2
XTC(Z)X |Z
]
= eâˆ’
Î»
2
(nt+trace(C)) det(I âˆ’ Î»C)âˆ’1/2, (88)
where in the second step we use the fact that I âˆ’ Î»C is positive definite. Denoting the eigenvalues
of C(Z) in decreasing order by Âµi(C), we have (since by assumption â€– C â€–â‰¤ L < 1Î»)
âˆ’ log det(I âˆ’ Î»C)âˆ’ Î» trace(C) =
nâˆ‘
i=1
âˆâˆ‘
k=1
1
k
(Î»Âµi(C))k âˆ’ Î» trace(C) =
âˆâˆ‘
k=2
1
k
Î»k
nâˆ‘
i=1
(Âµi(C))k
â‰¤ nÎ»
2L2
2
âˆâˆ‘
k=2
2
k
(Î»L)kâˆ’2 â‰¤ nÎ»
2L2
2
âˆâˆ‘
k=0
(Î»L)k,
where the inequality in the third step is by â€– C(Z) â€– = max{|Âµ1|, |Âµn|}. Now the rest of the proof
simply retraces the argument of the proof of Lemma A.1 and is therefore omitted.
30
7.2 Concentration inequalities for Lipschitz functionals of random matrices
We restate Corollary 1.8(b) of Guionnet and Zeitouni (2000) in our context.
Lemma A.3 : Suppose Y is an mÃ— n matrix, m â‰¤ n, with independent (real or complex) entries
Ykl following law Pkl, 1 â‰¤ k â‰¤ m, 1 â‰¤ l â‰¤ n. Let Sâˆ† = Yâˆ†Yâˆ— be a generalized Wishart matrix
where âˆ† is a diagonal matrix with real, nonnegative diagonal entries and spectral radius Ï†âˆ† > 0.
Suppose the family {Pkl : 1 â‰¤ k â‰¤ m, 1 â‰¤ l â‰¤ n} satisfies the logarithmic Sobolev inequality with
uniformly bounded constant c. Then for any function f such that g(x) := f(x2) is Lipschitz, for
any Î´ > 0,
P
(
| 1
m
trace f(
1
m+ n
Sâˆ†)âˆ’ E(
1
m
trace f(
1
m+ n
Sâˆ†))| > Î´
)
â‰¤ 2 exp
(
âˆ’ m
2Î´2
2cÏ†âˆ†|g|2L
)
(89)
where |g|L is the Lipschitz norm of g.
In order to apply this result to our context we take m = N âˆ’M , Y = ZB and âˆ† = m+nn In,
and recall that N(0, 1) satisfies logarithmic Sobolev inequality with constant c = 1 (Bogachev,
1998, Theorem 1.6.1). Then define fk(x) = Gk(x; Ï, Î³, ), k = 1, 2, where Gk(x; Ï, Î³, ) is defined
in (90), and gk(x) = fk(x2), and notice that gk(x) is Lipschitz with |gk|L = 2k(ÎºÎ³+)
1/2
(Ïâˆ’ÎºÎ³âˆ’)k+1 . Further,
Ï†âˆ† = m+nn and Sâˆ† = (m+ n)SBB.
Gk(x; Ï, Î³, ) =
ï£±ï£²ï£³
1
(Ïâˆ’x)k x â‰¤ ÎºÎ³ + 
1
(Ïâˆ’ÎºÎ³âˆ’)k x > ÎºÎ³ + 
where Ï > ÎºÎ³ + , k = 1, 2, . . . (90)
Therefore, applying Lemma A.3 we get the following :
Proposition A.1 : For k = 1, 2, and any Î´ > 0,
P
(
| 1
n
trace Gk(SBB; Ï, Î³, )âˆ’ E(
1
n
trace Gk(SBB; Ï, Î³, ))| > Î´
)
â‰¤ 2 exp
(
âˆ’ n
n+N âˆ’M
n2Î´2
2
(Ïâˆ’ ÎºÎ³ âˆ’ )2(k+1)
4k2(ÎºÎ³ + )
)
= 2 exp
(
âˆ’ n
2Î´2
2(1 + Î³)
(Ïâˆ’ ÎºÎ³ âˆ’ )2(k+1)
4k2(ÎºÎ³ + )
(1 + o(1))
)
(91)
7.3 Proof of Proposition 2
If we denote the singular values of ZB by Ïƒ1(ZB) > Ïƒ2(ZB) > . . . > ÏƒNâˆ’M (ZB). Using a concen-
tration inequality for singular values of random matrices (Ledoux, 2001),
P(|Ïƒi(ZB)âˆ’m(Ïƒi(ZB))| > r) â‰¤ 2eâˆ’
r2
4 , r > 0, 1 â‰¤ i â‰¤ N âˆ’M (92)
where m(Ïƒi(ZB)) is a median of Ïƒi(ZB). In this case, since N âˆ’M < n (at least for sufficiently
large n), the distribution of Ïƒi(ZB) is continuous so that the median is unique.
31
Now, since Âµi := Î»i(SBB) = 1n(Ïƒi(ZB))
2 and Ïƒi(ZB) â‰¥ 0 so that
m(Âµi) := m(Î»i(SBB)) = m(Ïƒi(SBB)) =
1
n
m(Ïƒi(ZB))2,
it follows that for r > 0 and every i = 1, . . . , N âˆ’M ,
2eâˆ’
nr2
4 â‰¥ P(| 1âˆš
n
Ïƒi(ZB)âˆ’
1âˆš
n
m(Ïƒi(ZB))| > r) â‰¥ P(|Âµi âˆ’m(Âµi)| > r(2
âˆš
m(Âµi) + r)) (93)
The last inequality follows from the fact that for real numbers x, y âˆˆ R+, on the set |xâˆ’ y| â‰¤ r, we
have |x2âˆ’ y2| = |xâˆ’ y|(x+ y) â‰¤ r(2y+ r), and then taking x = 1âˆš
n
Ïƒi(ZB) and y = 1âˆšnm(Ïƒi(ZB)).
Denoting m(Âµi) by mi for convenience, set s = r(2
âˆš
mi + r) = (r +
âˆš
mi)2 âˆ’mi. Then solving
for r we get for s > 0, r =
âˆš
s+mi âˆ’
âˆš
mi. Substituting in the last display we get
P(|Âµi âˆ’mi| > s) â‰¤ 2eâˆ’
n
4
(
âˆš
s+miâˆ’
âˆš
mi)
2
, s > 0, i = 1, . . . , N âˆ’M. (94)
The next step in the proof of Proposition 2 is to use the following results on the weak convergence
of the largest eigenvalue in the identity covariance case. The limiting distribution F1 is the so-called
Tracy-Widom law of order 1.
Result [Johnstone (2001a)] : When Nn â†’ Î³ âˆˆ (0, 1), under the assumption of normality
Î³âˆ’1/2(1 +
âˆš
Î³)âˆ’4/3(N âˆ’M)2/3
ï£®ï£°Âµ1 âˆ’(1 +âˆšN âˆ’M
n
)2ï£¹ï£» =â‡’ F1. (95)
By (95) it follows that Î³âˆ’1/2(1 +
âˆš
Î³)âˆ’4/3N2/3(m1 âˆ’ (1 +
âˆš
Nâˆ’M
n )
2)â†’ m(F1) where m(F1) is the
median of F1. In particular,
m1 âˆ’ ÎºÎ³ = O(|
N
n
âˆ’ Î³|) +O(nâˆ’2/3) (96)
Now to complete the proof of Proposition 2 observe that
âˆš
s+m1 âˆ’
âˆš
m1 =
sâˆš
s+m1 +
âˆš
m1
â‰¥ s
2
âˆš
m1
>
s
2
âˆš
ÎºÎ³ + Î´/4
for n â‰¥ n0(Î³, Î´). Now since Î´ < ÎºÎ³/2 implies ÎºÎ³ + Î´4 <
9
8ÎºÎ³ , for n â‰¥ n0(Î³, Î´) we have
âˆš
s+m1 âˆ’
âˆš
m1 >
âˆš
2s
3
âˆš
ÎºÎ³
. Therfore, choosing s = 3Î´4 and substituting in (94) we get the result after applying
the condition |m1 âˆ’ ÎºÎ³ | â‰¤ Î´4 . Note also that by (96) we also get the rate at which n0(Î³, Î´) should
grow as Î´ â†“ 0.
8 Appendix B
8.1 Expression for ÏÎ½
Lemma B.1 : For `Î½ â‰¥ 1 +
âˆš
Î³, ÏÎ½ = `Î½
(
1 + Î³`Î½âˆ’1
)
solves (3).
32
Proof : We prove this by showing that for any ` > 1 +
âˆš
Î³ we haveâˆ«
x
Ï(`)âˆ’ x
fÎ³(x)dx =
1
`âˆ’ 1
(97)
where Ï(`) = `
(
1 + Î³`âˆ’1
)
, and fÎ³(x) is the density of Marchenko-Pastur law with parameter Î³(â‰¤ 1)
and is given by
fÎ³(x) =
1
2Ï€Î³x
âˆš
(b(Î³)âˆ’ x)(xâˆ’ a(Î³))I(a(Î³) â‰¤ x â‰¤ b(Î³)), where a(Î³) = (1âˆ’âˆšÎ³)2, b(Î³) = (1+âˆšÎ³)2
The LHS of (97) is equal to
1
2Ï€Î³
âˆ« b(Î³)
a(Î³)
âˆš
(b(Î³)âˆ’ x)(xâˆ’ a(Î³))
Ï(`)âˆ’ x
dx
=
1
2Ï€Î³
âˆ« 2âˆšÎ³
âˆ’2âˆšÎ³
âˆš
(2
âˆš
Î³ âˆ’ y)(y + 2âˆšÎ³)
Ï(`)âˆ’ (1 + Î³)âˆ’ y
dy, (setting y = xâˆ’ (1 + Î³))
Since Ï(`)âˆ’ (1 + Î³) = (`âˆ’ 1) + Î³`âˆ’1 , setting K = `âˆ’ 1 we can rewrite the last expression as
K
2Ï€Î³
âˆ« 2âˆšÎ³
âˆ’2âˆšÎ³
âˆš
4Î³ âˆ’ y2
K2 + Î³ âˆ’Ky
dy
=
2K
Ï€
âˆ« 1
âˆ’1
âˆš
1âˆ’ z2
K2 + Î³ âˆ’ 2KâˆšÎ³z
dz, (setting z =
y
2
âˆš
Î³
)
=
2K
Ï€
[âˆ« 1
0
âˆš
1âˆ’ z2
(
1
K2 + Î³ âˆ’ 2KâˆšÎ³z
+
1
K2 + Î³ + 2K
âˆš
Î³z
)
dz
]
=
4K(K2 + Î³)
Ï€
âˆ« 1
0
âˆš
1âˆ’ z2
(K + Î³)2 âˆ’ 4K2Î³z2
dz
=
4K(K2 + Î³)
Ï€
âˆ« Ï€/2
0
cos2 Î¸
(K + Î³)2 âˆ’ 4K2Î³ sin2 Î¸
dÎ¸, (setting sin Î¸ = z)
=
(
K2 + Î³
KÎ³
)
1
Ï€
âˆ« Ï€/2
0
(K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸ âˆ’ ((K2 + Î³)2 âˆ’ 4K2Î³)
(K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸
dÎ¸
Substituting the formula for indefinite integral (for a2 > b2)âˆ«
dx
a2 âˆ’ b2 sin2 cx
=
1
ac
âˆš
a2 âˆ’ b2
tanâˆ’1
(âˆš
a2 âˆ’ b2 tan cx
a
)
(98)
and then using the fact that tan 0 = 0 and tan Ï€2 =âˆ, the last expression equals(
K2 + Î³
KÎ³
)
1
Ï€
[
Ï€
2
âˆ’ (K
2 âˆ’ Î³)2
(K2 + Î³)(K2 âˆ’ Î³)
Ï€
2
]
=
1
2
(
K2 + Î³
KÎ³
)
(K2 + Î³)âˆ’ (K2 âˆ’ Î³)
K2 + Î³
=
1
K
thus completing the proof for the case ` > 1 +
âˆš
Î³. The case ` = 1 +
âˆš
Î³ follows from this by
applying Monotone convergence theorem to the nonnegative functions { xÏ(`)âˆ’xI(a(Î³) < x < b(Î³)) :
` â‰¥ 1 +âˆšÎ³}, since Ï(`) is monotonically increasing in ` âˆˆ [1 +âˆšÎ³,âˆ).
33
Lemma B.2 : For ` > 1 +
âˆš
Î³,âˆ«
x
(Ï(`)âˆ’ x)2
dFÎ³(x) =
1
(`âˆ’ 1)2 âˆ’ Î³
(99)
Proof : Just as in the proof of Lemma B.1, after substituting z = (2
âˆš
Î³)âˆ’1(x âˆ’ (1 + Î³)), and
letting K = `âˆ’ 1 we getâˆ«
x
(Ï(`)âˆ’ x)2
dFÎ³(x)
=
2K2
Ï€
âˆ« 1
âˆ’1
âˆš
1âˆ’ z2
(K2 + Î³ âˆ’ 2KâˆšÎ³z)2
dz
=
2K2
Ï€
âˆ« 1
0
âˆš
1âˆ’ z2
(
1
(K2 + Î³ âˆ’ 2KâˆšÎ³z)2
+
1
(K2 + Î³ + 2K
âˆš
Î³z)2
)
dz
=
2K2
Ï€
âˆ« 1
0
2((K2 + Î³)2 + 4K2Î³z2)
âˆš
1âˆ’ z2
((K2 + Î³)2 âˆ’ 4K2Î³z2)2
dz
=
4K2
Ï€
âˆ« 1
0
2(K2 + Î³)2
âˆš
1âˆ’ z2
((K2 + Î³)2 âˆ’ 4K2Î³z2)2
dz âˆ’ 4K
2
Ï€
âˆ« 1
0
âˆš
1âˆ’ z2
(K2 + Î³)2 âˆ’ 4K2Î³z2
dz
=
8K2(K2 + Î³)2
Ï€
âˆ« Ï€/2
0
cos2 Î¸dÎ¸
((K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸)2
âˆ’ 1
K2 + Î³
, setting sin Î¸ = z and by (97)
=
8K2(K2 + Î³)2
Ï€
1
4K2Î³
âˆ« Ï€/2
0
(K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸ âˆ’ ((K2 + Î³)2 âˆ’ 4K2Î³)
((K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸)2
dÎ¸ âˆ’ 1
K2 + Î³
=
2(K2 + Î³)2
Ï€Î³
[âˆ« Ï€/2
0
dÎ¸
(K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸
âˆ’
âˆ« Ï€/2
0
(K2 âˆ’ Î³)2dÎ¸
((K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸)2
]
âˆ’ 1
K2 + Î³
=
2(K2 + Î³)2
Ï€Î³
1
(K2 + Î³)(K2 âˆ’ Î³)
Ï€
2
âˆ’ 1
2(K2 + Î³)
âˆ’ 2(K
4 âˆ’ Î³2)2
Ï€Î³
âˆ« Ï€/2
0
dÎ¸
((K2 + Î³)2 âˆ’ 4K2Î³ sin2 Î¸)2
,
=
1
Î³
(
K2 + Î³
K2 âˆ’ Î³
)
âˆ’ 1
K2 + Î³
âˆ’ (K
4 âˆ’ Î³2)2
Ï€Î³
âˆ« Ï€
0
dÏ†
(K4 + Î³2 + 2K2Î³ cosÏ†)2
(100)
where eighth equality is due to (98) and in the last step we used cos 2Î¸ = 1âˆ’ 2 sin2 Î¸ before setting
Ï† = 2Î¸. Since for a > b we haveâˆ«
dx
(a+ b cosx)2
= âˆ’ b sinx
(a2 âˆ’ b2)(a+ b cosx)
+
a
a2 âˆ’ b2
âˆ«
dx
a+ b cosx
= âˆ’ b sinx
(a2 âˆ’ b2)(a+ b cosx)
+
2a
(a2 âˆ’ b2)3/2
tanâˆ’1
(âˆš
a2 âˆ’ b2 tan x2
a+ b
)
setting a = K4 + Î³2 and b = 2K2Î³ we get
(K4 âˆ’ Î³2)2
Ï€Î³
âˆ« Ï€
0
dÏ†
(K4 + Î³2 + 2K2Î³ cosÏ†)2
=
(K4 âˆ’ Î³2)2
Ï€Î³
2(K4 + Î³2)
((K4 + Î³2)2 âˆ’ 4K4Î³2)3/2
Ï€
2
=
1
Î³
(
K4 + Î³2
K4 âˆ’ Î³2
)
34
Substituting the last expression in (100) we getâˆ«
x
(Ï(`)âˆ’ x)2
dFÎ³(x) =
1
Î³
(
K2 + Î³
K2 âˆ’ Î³
)
âˆ’ 1
K2 + Î³
âˆ’ 1
Î³
(
K4 + Î³2
K4 âˆ’ Î³2
)
=
(K2 + Î³)2 âˆ’ Î³(K2 âˆ’ Î³)âˆ’ (K4 + Î³2)
Î³(K2 âˆ’ Î³)(K2 + Î³)
=
1
K2 âˆ’ Î³
8.2 Lower bound on eigenvalues in the general case
In this section we provide a lower bound for the sample eigenvalues Ì‚Ì€Î½ which holds with high
probability when `Î½ > 1 +
âˆš
Î³. It will be more useful to provide a lower bound for Î»Ì‚Î½,Î½ = Î»Î½(SÎ“Î½ ).
We do that using equations (14) and (15) and the observation that for any Î½ â‰¥ 1
Î½âˆ‘
k=1
Î»k(SÎ“Î½ ) = maxLâˆˆOÎ½,Nâˆ’M+Î½
trace (LTSÎ“Î½L) (101)
where OÎ½,Nâˆ’M+Î½ is the set of (N âˆ’M + Î½)Ã— Î½ matrices whoses columns are orthonormal. Thus,
our approach is to construct an appropriate LÎ½ for every Î½ with `Î½ > 1 +
âˆš
Î³ such that the lower
bound trace (LTÎ½ SÎ“Î½LÎ½) is close to
âˆ‘Î½
k=1 Ïk.
Thereafter by utilizing (21) we have
Î½âˆ’1âˆ‘
k=1
Î»1(SÎ“kâˆ’1) + Î»Î½(SÎ“Î½ ) â‰¥
Î½âˆ’1âˆ‘
k=1
Ì‚Ì€
k + Î»Î½(SÎ“Î½ ) â‰¥
Î½âˆ‘
k=1
Î»k(SÎ“Î½ ) (102)
We construct the (N âˆ’M + Î½)Ã— Î½ matrix LÎ½ as follows. Let RÌƒk, k = 1, . . . , Î½ be numbers between
0 and 1 to be specified. Write
LÎ½ =
[
LA,Î½
LB,Î½
]
where LA,Î½ = diag(
âˆš
1âˆ’ RÌƒ21, . . . ,
âˆš
1âˆ’ RÌƒ2Î½)
and LB,Î½ = V ÎÌƒDÌƒ where DÌƒ = diag(RÌƒ1, . . . , RÌƒÎ½), V is as in (13) and the matrix ÎÌƒ = (Î¶Ìƒ1 : . . . : Î¶ÌƒÎ½)
is obtained by Gram-Schmidt orthonormalization of the matrix Î whose columns are Î¶k/ â€– Î¶k â€–
where
Î¶k =
âˆš
`kM1/2(ÏkI âˆ’M)âˆ’1tk, k = 1, . . . , Î½ (103)
To be specific, we set Î¶ÌƒÎ½ = Î¶Î½/ â€– Î¶Î½ â€– and assume that the orthonormalization is carried out
backwards (w.r.t. the columns of Î). First thing to notice is that
â€– Î¶k â€–2= `ktTkM(ÏkI âˆ’M)âˆ’2tk
and
Î¶
T
j Î¶k =
âˆš
`j`kt
T
jM(ÏkI âˆ’M)âˆ’1(ÏjI âˆ’M)âˆ’1tk, for 1 â‰¤ j 6= k â‰¤ Î½
35
With JÎ³() = {Âµ1 < ÎºÎ³ + }, we have, due to Lemma A.1 (taking Î´ = 13 in the lemma),
P(Î¶Tj Î¶k â‰¥
âˆš
`j`kÎ´0, JÎ³(/2))
= 2 exp
(
âˆ’ n
N âˆ’M
(Ïj âˆ’ /2âˆ’ ÎºÎ³)2(Ïk âˆ’ /2âˆ’ ÎºÎ³)2nÎ´20
12(ÎºÎ³ + /2)2
)
= 2 exp
(
âˆ’1
Î³
(Ïj âˆ’ /2âˆ’ ÎºÎ³)2(Ïk âˆ’ /2âˆ’ ÎºÎ³)2nÎ´20
12(ÎºÎ³ + /2)2
(1 + o(1))
)
,
for 0 < Î´0 <
2(ÎºÎ³ + /2)
(Ïj âˆ’ /2âˆ’ ÎºÎ³)(Ïk âˆ’ /2âˆ’ ÎºÎ³)
for 1 â‰¤ j 6= k â‰¤ Î½ (104)
We now choose RÌƒk as
RÌƒk =
â€– Î¶k â€–âˆš
1+ â€– Î¶k â€–2
or
âˆš
1âˆ’ RÌƒ2k =
1âˆš
1+ â€– Î¶k â€–2
(105)
Our aim is to prove the following proposition.
Proposition B.1 : With this choice of LÎ½ , given  > 0, âˆƒ n6(,Î›, Î³) such that for n â‰¥ n6(,Î›, Î³),
P(trace(LTÎ½ SÎ“Î½LÎ½) â‰¤
Î½âˆ‘
k=1
Ïk âˆ’ /2, Âµ1 < ÎºÎ³ + /2) â‰¤ Îµ7(n, ,Î›, Î³) (106)
where
âˆ‘âˆ
n=n6(,Î›,Î³)
Îµ7(n, ,Î›, Î³) <âˆ.
Once we have this result, we apply Proposition 3 (with ÏÎ½ +  replaced by Ïk + /(2Î½) and Ì‚Ì€Î½
replaced by Î»1(SÎ“kâˆ’1), k = 1, . . . , Î½ âˆ’ 1; the validity of this is readily cheked by following the first
step of the proof), utilize (101), (102), and the inequality Ì‚Ì€Î½ â‰¥ Î»Î½(SÎ“Î½ ), in combination with (106)
to prove the following.
Proposition B.2 : Given  > 0, âˆƒ n7(,Î›, Î³) such that for n â‰¥ n7(,Î›, Î³),
P(Ì‚Ì€Î½ â‰¤ ÏÎ½ âˆ’ , Âµ1 < ÎºÎ³ + /2) â‰¤ Îµ8(n, ,Î›, Î³)
where
âˆ‘âˆ
n=n7(,Î›,Î³)
Îµ8(n, ,Î›, Î³) <âˆ, provided  is small enough so that ÏÎ½ > ÎºÎ³ + 2.
The rest of the section is devoted to giving an outline of the proof of Proposition B.1. First step
in that direction is to express trace(LTÎ½ SÎ“Î½LÎ½) as
trace(LTÎ½ SÎ“Î½LÎ½) = trace(L
T
A,Î½SAA,Î½LA,Î½) + 2 trace(L
T
A,Î½SAB,Î½LB,Î½) + trace(L
T
B,Î½SBBLB,Î½)
(107)
Here, as before, SAA,Î½ denotes the submatrix of SAA consisting of first Î½ rows and first Î½ columns.
SAB,Î½ is analogously defined. By definition of LA,Î½ ,
trace(LTA,Î½SAA,Î½LA,Î½) =
Î½âˆ‘
k=1
(1âˆ’ RÌƒ2k)skk =
Î½âˆ‘
k=1
(1âˆ’ RÌƒ2k)`k
1
n
â€– ZA,k â€–2 (108)
36
Next,
trace(LTA,Î½SAB,Î½LB,Î½) = trace(L
T
A,Î½Î›
1/2
Î½ T
T
Î½ M1/2ÎÌƒDÌƒ) =
Î½âˆ‘
k=1
RÌƒk
âˆš
1âˆ’ RÌƒ2k
âˆš
`kt
T
kM1/2Î¶Ìƒk (109)
Finally,
trace(LTB,Î½SBBLB,Î½) = trace(DÌƒÎÌƒ
TMÎÌƒDÌƒ) =
Î½âˆ‘
k=1
RÌƒ2kÎ¶Ìƒ
T
kMÎ¶Ìƒk (110)
Now let us find an expression for Î¶Ìƒk. By definition, Î¶ÌƒÎ½ = Î¶Î½/ â€– Î¶Î½ â€– and
Î¶Ìƒj =
ï£«ï£­ Î¶j
â€– Î¶j â€–
âˆ’
Î½âˆ‘
k=j+1
cjkÎ¶Ìƒk
ï£¶ï£¸ / â€– Î¶j
â€– Î¶j â€–
âˆ’
Î½âˆ‘
k=j+1
cjkÎ¶Ìƒk â€–, j = Î½ âˆ’ 1, Î½ âˆ’ 2, . . . , 1, (111)
where cjk are determined from the orthogonality relations. Therefore,
cjk =
ã€ˆÎ¶j , Î¶Ìƒkã€‰
â€– Î¶j â€–
, for Î½ â‰¥ k > j.
Thus we can express ÎÌƒ as Îâˆ† where âˆ† is a lower triangular matrix whose entries are given as
follows:
âˆ†jk =
ï£±ï£²ï£³0 if 1 â‰¤ k â‰¤ j âˆ’ 1âˆ’cjkâˆ†jj if j + 1 â‰¤ k â‰¤ Î½ with âˆ†jj =
ï£«ï£­â€– Î¶j
â€– Î¶j â€–
âˆ’
Î½âˆ‘
k=j+1
cjkÎ¶Ìƒk â€–
ï£¶ï£¸âˆ’1
Note that
âˆ†âˆ’2jj =
â€– Î¶k â€–2
â€– Î¶k â€–2
âˆ’ 2
âˆ‘
k>j
cjk
ã€ˆÎ¶j , Î¶Ìƒkã€‰
â€– Î¶j â€–
+
âˆ‘
k>j
c2jk = 1âˆ’
âˆ‘
k>j
c2jk
This implies that for k > j,
cjk =
ã€ˆÎ¶j , Î¶Ìƒkã€‰
â€– Î¶j â€–
=
âˆ‘
kâ€²â‰¥k
ã€ˆÎ¶j , Î¶kâ€²ã€‰
â€– Î¶j â€– Â· â€– Î¶kâ€² â€–
âˆ†kkâ€²
= âˆ†kk
(
ã€ˆÎ¶j , Î¶kã€‰
â€– Î¶j â€– Â· â€– Î¶k â€–
âˆ’
âˆ‘
kâ€²>k
ã€ˆÎ¶j , Î¶kâ€²ã€‰
â€– Î¶j â€– Â· â€– Î¶kâ€² â€–
ckkâ€²
)
= âˆ†kk(Ï„jk âˆ’
âˆ‘
kâ€²>k
Ï„jkâ€²ckkâ€²) (112)
where Ï„jk =
ã€ˆÎ¶j ,Î¶kã€‰
â€–Î¶jâ€–Â·â€–Î¶kâ€–
. Moreover,
|1âˆ’âˆ†kk| =
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£1âˆ’ 1âˆš1âˆ’âˆ‘kâ€²>k c2kkâ€²
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ â‰¤
âˆ‘
kâ€²>k c
2
kkâ€²
1âˆ’
âˆ‘
kâ€²>k c
2
kkâ€²
=â‡’ âˆ†kk â‰¤
1
1âˆ’
âˆ‘
kâ€²>k c
2
kkâ€²
(113)
37
We aim to show that Î¶Ìƒj is close to
Î¶j
â€–Î¶jâ€–
. The following lemma helps us make such a statement.
Lemma B.3 : Let A > 1 be arbitrary. Suppose Î´ > 0 is such that Î´ < Aâˆ’1
Î½A2
. If |Ï„jk| â‰¤ Î´ for all
Î½ â‰¥ k > j â‰¥ 1, then
|cjk| â‰¤ AÎ´, and âˆ†jj â‰¤
1
1âˆ’ (Î½ âˆ’ j)A2Î´2
, Î½ â‰¥ k > j â‰¥ 1. (114)
Proof : We prove the result by backward induction on j, k. First note that cjÎ½ = Ï„jÎ½ for j =
1, . . . , Î½ âˆ’ 1. Thus |cjÎ½ | â‰¤ Î´ for j = 1, . . . , Î½ âˆ’ 1. And by (113), âˆ†Î½âˆ’1,Î½âˆ’1 â‰¤ 11âˆ’Î´2 . So the induction
hypothesis is satisfied for j = Î½ âˆ’ 1, k = Î½. Suppose the hypothesis holds for all Î½ â‰¥ k > j â‰¥ J + 1
where J â‰¥ 1. Want to show that the same holds for j = J . Evidently |cJÎ½ | â‰¤ Î´. From (112) we
have, for k = J + 1, . . . , Î½ âˆ’ 1
|cJk| â‰¤ âˆ†kk(|Ï„Jk|+
âˆ‘
kâ€²>k
|Ï„Jk||ckkâ€² |) â‰¤
1
1âˆ’ (Î½ âˆ’ k)A2Î´2
(Î´ +
Î½âˆ‘
kâ€²=k+1
Î´ Â·AÎ´) (by hypothesis)
=
Î´(1 + (Î½ âˆ’ k)AÎ´)
1âˆ’ (Î½ âˆ’ k)A2Î´2
â‰¤ Î´(1 + (Î½ âˆ’ k)AÎ´)
1âˆ’ (Î½ âˆ’ k)2A2Î´2
=
Î´
1âˆ’ (Î½ âˆ’ k)AÎ´
â‰¤ AÎ´
Here the last inequality follows from the fact
1
1âˆ’ (Î½ âˆ’ k)AÎ´
â‰¤ A â‡” 1âˆ’ 1
A
â‰¥ (Î½ âˆ’ k)AÎ´ â‡” Î´ â‰¤ Aâˆ’ 1
(Î½ âˆ’ k)A2
and the last condition holds since Î´ < Aâˆ’1
Î½A2
. The assertion about âˆ†JJ follows easily from this and
(113).
We are now in a position to finish the proof of Proposition B.1. We avoid all the messy details
since most of it is mere repitition of the analysis we carried out in Section 4. We just show how
the three terms behave asymptotically as n â†’ âˆ. Proposition 6 shows that for large n, â€– Î¶k â€–2
concentrates around `kÎ³
âˆ«
x
(Ïkâˆ’x)2
dFÎ³(x) = `kÎ³(`kâˆ’1)2âˆ’1 . This and (104) imply that for every pair
j 6= k, Ï„jk concentrates about 0. Therefore by Lemma B.3 we see that Î¶Ìƒj âˆ’ Î¶j/ â€– Î¶j â€– is a
vector whose norm concentrates around zero. With this piece of information, we can strip off the
insignificant terms in (109) and (110) to claim that
trace(LTA,Î½SAB,Î½LB,Î½) âˆ¼
Î½âˆ‘
k=1
1
â€– Î¶k â€–
RÌƒk
âˆš
1âˆ’ RÌƒ2k
âˆš
`kt
T
kM1/2Î¶k
and trace(LTB,Î½SBBLB,Î½) âˆ¼
Î½âˆ‘
k=1
1
â€– Î¶k â€–2
RÌƒ2kÎ¶
T
kMÎ¶k
where âˆ¼ means that the difference between the LHS and RHS concentrates around 0 as n â†’ âˆ.
Recalling (103) and (105), and using Proposition 6 and parts of the proof of Proposition 3, it is
easy to show that
trace(LTA,Î½SAA,Î½LA,Î½) âˆ¼
Î½âˆ‘
k=1
1
1+ â€– Î¶k â€–2
`k âˆ¼
Î½âˆ‘
k=1
`k
(
1 +
`kÎ³
(`k âˆ’ 1)2 âˆ’ Î³
)âˆ’1
(115)
38
trace(LTA,Î½SAB,Î½LB,Î½) âˆ¼
Î½âˆ‘
k=1
1
1+ â€– Î¶k â€–2
`kt
T
kM(ÏkI âˆ’M)âˆ’1tk
âˆ¼
Î½âˆ‘
k=1
`kÎ³
`k âˆ’ 1
(
1 +
`kÎ³
(`k âˆ’ 1)2 âˆ’ Î³
)âˆ’1
(116)
trace(LTA,Î½SAB,Î½LB,Î½) âˆ¼
Î½âˆ‘
k=1
1
1+ â€– Î¶k â€–2
`kt
T
kM2(ÏkI âˆ’M)âˆ’2tk
âˆ¼
Î½âˆ‘
k=1
`kÎ³
(
1 +
`kÎ³
(`k âˆ’ 1)2 âˆ’ Î³
)âˆ’1 [ Ïk
(`k âˆ’ 1)2 âˆ’ 1
âˆ’ 1
`k âˆ’ 1
]
(117)
Substituing (115), (116) and (117) in (107), after some simplification we get
trace(LTÎ½ SÎ“Î½LÎ½) âˆ¼
Î½âˆ‘
k=1
Ïk
Formalizing this argument we can prove (106).
8.3 Proof of Proposition 6
We simply give an outline. First consider the expansion
tTjM(ÏI âˆ’M)âˆ’2tj âˆ’ Î³
âˆ«
x
(Ïâˆ’ x)2
dFÎ³(x)
=
[
tTjM(ÏI âˆ’M)âˆ’2tj âˆ’
1
n
trace(M(ÏI âˆ’M)âˆ’2)
]
+
[
1
n
trace(M(ÏI âˆ’M)âˆ’2)âˆ’ Î³
âˆ«
x
(Ïâˆ’ x)2
dFÎ³(x)
]
For the first square-bracketed term use Lemma A.1 restricting to the set {Âµ1 < ÎºÎ³+/2}. Subdivide
the second term further as
Ï
[
1
n
trace((ÏI âˆ’M)âˆ’2)âˆ’ Î³
âˆ«
1
(Ïâˆ’ x)2
dFÎ³(x)
]
âˆ’
[
1
n
trace((ÏI âˆ’M)âˆ’1)âˆ’ Î³
âˆ«
1
Ïâˆ’ x
dFÎ³(x)
]
= I âˆ’ II, say.
Bounds for I and II on the set {Âµ1 < ÎºÎ³ + /2} are derived by imitating the arguments leading to
(47) and (45). Only notable difference is that for I we need to use the functionG2(Â·; Ï, Î³, /2) instead
of G1(Â·; Ï, Î³, /2). Keeping track of the constants we derive the upper bound in the statement of
Proposition 6.
Acknowledgement
I wish to thank Professor Iain Johnstone, who is my thesis advisor, for his support and thoughtful
discussions, and also for bringing to my notice two important references. I would also like to thank
Dr. Jinho Baik for some helpful communications.
39
Reference
1. Anderson, T. W. (1963) : Asymptotic theory of principal component analysis, Annals of
Mathematical Statistics, 34, p. 122-148.
2. Bai, Z. D. (1993) : Convergence rate of expected spectral distributions of large random
matrices. Part II. Sample covariance matrices, Annals of Probability, 21, 649-672.
3. Bai, Z. D. (1999) : Methodologies in spectral analysis of large dimensional random matrices,
a review, Statistica Sinica, 9, 611-677.
4. Bai, Z. D. & Silvertein, J. W. (1999) : Exact separation of eigenvalues of large dimensional
sample covariance matrices, Annals of Probability, 27, 1536-1555.
5. Bai, Z. D. & Silvertein, J. W. (2004) : CLT for linear spectral statistics of large dimensional
sample covariance matrix, (to appear in the Annals of Probability).
6. Bai, Z. D. & Yin, Y. Q. (1993) : Limit of the smallest eigenvalue of a large dimensional
sample covariance matrix, Annals of Probability, 21, 1275-1294.
7. Baik, J., Ben Arous, G. & PeÌcheÌ (2004) : Phase transition of the largest eigenvalue for
non-null complex covariance matrices, arXiv:math.PR/0403022 v1.
8. Baik, J. & Silverstein, J. W. (2004) : Eigenvalues of large sample covariance matrices of
spiked population models, arXiv:math.ST/048165 v1.
9. Bogachev, V. I. (1998) : Gaussian Measures, American Mathematical Society.
10. Buja, A., Hastie, T. & Tibshirani, R. (1995) : Penalized discriminant analysis, Annals of
Statistics, 23, 73-102.
11. Guionnet, A. & Zeitouni, O. (2000) : Concentration of the spectral measure for large matrices,
Electronic Communications in Probability, 5, 119-136.
12. Hoyle, D. & Rattray, M. (2003) : Limiting form of the sample covariance eigenspectrum in
PCA and kernel PCA, Advances in Neural Information Processing Systems (NIPS 16).
13. Hoyle, D. & Rattray, M. (2004) : Principal component analysis eigenvalue spectra from data
with symmetry breaking structure, Physical Review E, 69, 026124.
14. Johnstone, I. M. (2001) : On the distribution of the largest principal component, Annals of
Statistics, 29, 295-327.
15. Johnstone, I. M. & Lu, A. Y. (2004) : Sparse principal component analysis, (to appear in
Journal of American Statistical Association).
40
16. Kneip, A. & Utikal, K. J. (2001) : Inference for density families using functional principal
component analysis. Journal of American Statistical Association, 96, 519-542.
17. Kato, T. (1980) : Perturbation theory of linear operators, Springer-Verlag.
18. Ledoit, O. & Wolf, M. (2002) : Some hypothesis test for the covariance matrix when the
dimension is large compared to the sample size, Annals of Statistics, 30, 1081-1102.
19. Ledoux, M. (2001) : The concentration of measure phenomenon, Mathematical Surveys and
Monographs 89, American Mathematical Society.
20. Muirhead, R. J. (1982) : Aspects of multivariate statistical theory, John Wiley & Sons, Inc.
21. PeÌcheÌ, S. (2003) : Universality of local eigenvalue statistics for random sample covariance
matrices. Ph. D. Thesis, Ecole Polytechnique FeÌdeÌrale de Lausanne.
22. Rao, C. R. (1973) : Linear statistical inference and its applications, Wiley Eastern.
23. Reimann, P., Van den Broeck, C. & Bex, G. J. (1996) : A Gaussian scenario for unsupervised
learning, Journal of Physics A : Mathematical and General, 29, 3521-3535.
24. Roy, S. N. (1953) : On a heuristic method of test construction and its use in Multivariate
analysis, Annals of Mathematical Statistics, 24, 220-238.
25. Silverstein, J. W. & Choi, S. (1995) : Analysis of the limiting spectral distribution of large
dimensional random matrices, Journal of Multivariate Analysis, 54, 295-309.
26. Soshnikov, A. (2002) : A note on universality of the distribution of the largest eigenvalues
in certain sample covariance matrices, Journal of Statistical Physics, 108(5/6), 1033-1056.
(Also arXiv:math.PR/0104113 v2 ).
27. Telatar, E. (1999) : Capacity of multi-antenna Gaussian channels, European Transactions on
Telecommunications, 10, 585-595.
28. Tracy, C. & Widom, H. (1994) : Level-spacing distribution and Airy kernel, Communications
in Mathematical Physics, 159, 151-174.
29. Tracy, C. & Widom, H. (1996) : On orthogonal and symplectic matrix ensembles, Commu-
nicat
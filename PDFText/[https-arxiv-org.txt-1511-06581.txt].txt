Dueling Network Architectures for Deep Reinforcement Learning
Ziyu Wang ZIYU@GOOGLE.COM
Tom Schaul SCHAUL@GOOGLE.COM
Matteo Hessel MTTHSS@GOOGLE.COM
Hado van Hasselt HADO@GOOGLE.COM
Marc Lanctot LANCTOT@GOOGLE.COM
Nando de Freitas NANDODEFREITAS@GMAIL.COM
Google DeepMind, London, UK
Abstract
In recent years there have been many successes
of using deep representations in reinforcement
learning. Still, many of these applications use
conventional architectures, such as convolutional
networks, LSTMs, or auto-encoders. In this pa-
per, we present a new neural network architec-
ture for model-free reinforcement learning. Our
dueling network represents two separate estima-
tors: one for the state value function and one for
the state-dependent action advantage function.
The main benefit of this factoring is to general-
ize learning across actions without imposing any
change to the underlying reinforcement learning
algorithm. Our results show that this architec-
ture leads to better policy evaluation in the pres-
ence of many similar-valued actions. Moreover,
the dueling architecture enables our RL agent to
outperform the state-of-the-art on the Atari 2600
domain.
1. Introduction
Over the past years, deep learning has contributed to dra-
matic advances in scalability and performance of machine
learning (LeCun et al., 2015). One exciting application
is the sequential decision-making setting of reinforcement
learning (RL) and control. Notable examples include deep
Q-learning (Mnih et al., 2015), deep visuomotor policies
(Levine et al., 2015), attention with recurrent networks (Ba
et al., 2015), and model predictive control with embeddings
(Watter et al., 2015). Other recent successes include mas-
sively parallel frameworks (Nair et al., 2015) and expert
move prediction in the game of Go (Maddison et al., 2015),
which produced policies matching those of Monte Carlo
tree search programs, and squarely beaten a professional
player when combined with search (Silver et al., 2016).
In spite of this, most of the approaches for RL use standard
neural networks, such as convolutional networks, MLPs,
LSTMs and autoencoders. The focus in these recent ad-
vances has been on designing improved control and RL al-
gorithms, or simply on incorporating existing neural net-
work architectures into RL methods. Here, we take an al-
ternative but complementary approach of focusing primar-
ily on innovating a neural network architecture that is better
suited for model-free RL. This approach has the benefit that
the new network can be easily combined with existing and
future algorithms for RL. That is, this paper advances a new
network (Figure 1), but uses already published algorithms.
The proposed network architecture, which we name the du-
eling architecture, explicitly separates the representation of
state values and (state-dependent) action advantages. The
dueling architecture consists of two streams that represent
the value and advantage functions, while sharing a common
Figure 1. A popular single stream Q-network (top) and the duel-
ing Q-network (bottom). The dueling network has two streams
to separately estimate (scalar) state-value and the advantages for
each action; the green output module implements equation (9) to
combine them. Both networks output Q-values for each action.
ar
X
iv
:1
51
1.
06
58
1v
3 
 [
cs
.L
G
] 
 5
 A
pr
 2
01
6
Dueling Network Architectures for Deep Reinforcement Learning
convolutional feature learning module. The two streams
are combined via a special aggregating layer to produce an
estimate of the state-action value function Q as shown in
Figure 1. This dueling network should be understood as a
single Q network with two streams that replaces the popu-
lar single-stream Q network in existing algorithms such as
Deep Q-Networks (DQN; Mnih et al., 2015). The dueling
network automatically produces separate estimates of the
state value function and advantage function, without any
extra supervision.
Intuitively, the dueling architecture can learn which states
are (or are not) valuable, without having to learn the effect
of each action for each state. This is particularly useful
in states where its actions do not affect the environment in
any relevant way. To illustrate this, consider the saliency
maps shown in Figure 21. These maps were generated by
computing the Jacobians of the trained value and advan-
tage streams with respect to the input video, following the
method proposed by Simonyan et al. (2013). (The experi-
mental section describes this methodology in more detail.)
The figure shows the value and advantage saliency maps for
two different time steps. In one time step (leftmost pair of
images), we see that the value network stream pays atten-
tion to the road and in particular to the horizon, where new
cars appear. It also pays attention to the score. The advan-
tage stream on the other hand does not pay much attention
to the visual input because its action choice is practically
irrelevant when there are no cars in front. However, in the
second time step (rightmost pair of images) the advantage
stream pays attention as there is a car immediately in front,
making its choice of action very relevant.
In the experiments, we demonstrate that the dueling archi-
tecture can more quickly identify the correct action during
policy evaluation as redundant or similar actions are added
to the learning problem.
We also evaluate the gains brought in by the dueling archi-
tecture on the challenging Atari 2600 testbed. Here, an RL
agent with the same structure and hyper-parameters must
be able to play 57 different games by observing image pix-
els and game scores only. The results illustrate vast im-
provements over the single-stream baselines of Mnih et al.
(2015) and van Hasselt et al. (2015). The combination of
prioritized replay (Schaul et al., 2016) with the proposed
dueling network results in the new state-of-the-art for this
popular domain.
1.1. Related Work
The notion of maintaining separate value and advantage
functions goes back to Baird (1993). In Baird’s original
1https://www.youtube.com/playlist?list=
PLVFXyCSfS2Pau0gBh0mwTxDmutywWyFBP
VALUE ADVANTAGE
VALUE ADVANTAGE
Figure 2. See, attend and drive: Value and advantage saliency
maps (red-tinted overlay) on the Atari game Enduro, for a trained
dueling architecture. The value stream learns to pay attention to
the road. The advantage stream learns to pay attention only when
there are cars immediately in front, so as to avoid collisions.
advantage updating algorithm, the shared Bellman resid-
ual update equation is decomposed into two updates: one
for a state value function, and one for its associated ad-
vantage function. Advantage updating was shown to con-
verge faster than Q-learning in simple continuous time do-
mains in (Harmon et al., 1995). Its successor, the advan-
tage learning algorithm, represents only a single advantage
function (Harmon & Baird, 1996).
The dueling architecture represents both the value V (s)
and advantage A(s, a) functions with a single deep model
whose output combines the two to produce a state-action
value Q(s, a). Unlike in advantage updating, the represen-
tation and algorithm are decoupled by construction. Con-
sequently, the dueling architecture can be used in combina-
tion with a myriad of model free RL algorithms.
There is a long history of advantage functions in policy gra-
dients, starting with (Sutton et al., 2000). As a recent ex-
ample of this line of work, Schulman et al. (2015) estimate
advantage values online to reduce the variance of policy
gradient algorithms.
There have been several attempts at playing Atari with deep
reinforcement learning, including Mnih et al. (2015); Guo
et al. (2014); Stadie et al. (2015); Nair et al. (2015); van
Hasselt et al. (2015); Bellemare et al. (2016) and Schaul
Dueling Network Architectures for Deep Reinforcement Learning
et al. (2016). The results of Schaul et al. (2016) are the
current published state-of-the-art.
2. Background
We consider a sequential decision making setup, in which
an agent interacts with an environment E over discrete time
steps, see Sutton & Barto (1998) for an introduction. In the
Atari domain, for example, the agent perceives a video st
consisting of M image frames: st = (xt−M+1, . . . , xt) ∈
S at time step t. The agent then chooses an action from a
discrete set at ∈ A = {1, . . . , |A|} and observes a reward
signal rt produced by the game emulator.
The agent seeks maximize the expected discounted re-
turn, where we define the discounted return as Rt =∑∞
τ=t γ
τ−trτ . In this formulation, γ ∈ [0, 1] is a discount
factor that trades-off the importance of immediate and fu-
ture rewards.
For an agent behaving according to a stochastic policy π,
the values of the state-action pair (s, a) and the state s are
defined as follows
Qπ(s, a) = E [Rt| st = s, at = a, π] , and
V π(s) = Ea∼π(s) [Qπ(s, a)] . (1)
The preceding state-action value function (Q function for
short) can be computed recursively with dynamic program-
ming:
Qπ(s, a) = Es′
[
r + γEa′∼π(s′) [Qπ(s′, a′)] | s, a, π
]
.
We define the optimal Q∗(s, a) = maxπ Qπ(s, a). Un-
der the deterministic policy a = argmaxa′∈AQ
∗(s, a′),
it follows that V ∗(s) = maxaQ∗(s, a). From this, it also
follows that the optimal Q function satisfies the Bellman
equation:
Q∗(s, a) = Es′
[
r + γmax
a′
Q∗(s′, a′) | s, a
]
. (2)
We define another important quantity, the advantage func-
tion, relating the value and Q functions:
Aπ(s, a) = Qπ(s, a)− V π(s). (3)
Note that Ea∼π(s) [Aπ(s, a)] = 0. Intuitively, the value
function V measures the how good it is to be in a particular
state s. The Q function, however, measures the the value
of choosing a particular action when in this state. The ad-
vantage function subtracts the value of the state from the Q
function to obtain a relative measure of the importance of
each action.
2.1. Deep Q-networks
The value functions as described in the preceding section
are high dimensional objects. To approximate them, we can
use a deep Q-network: Q(s, a; θ) with parameters θ. To
estimate this network, we optimize the following sequence
of loss functions at iteration i:
Li(θi) = Es,a,r,s′
[(
yDQNi −Q(s, a; θi)
)2]
, (4)
with
yDQNi = r + γmax
a′
Q(s′, a′; θ−), (5)
where θ− represents the parameters of a fixed and sepa-
rate target network. We could attempt to use standard Q-
learning to learn the parameters of the network Q(s, a; θ)
online. However, this estimator performs poorly in prac-
tice. A key innovation in (Mnih et al., 2015) was to freeze
the parameters of the target network Q(s′, a′; θ−) for a
fixed number of iterations while updating the online net-
work Q(s, a; θi) by gradient descent. (This greatly im-
proves the stability of the algorithm.) The specific gradient
update is
∇θiLi(θi) = Es,a,r,s′
[(
yDQNi −Q(s, a; θi)
)
∇θiQ(s, a; θi)
]
This approach is model free in the sense that the states and
rewards are produced by the environment. It is also off-
policy because these states and rewards are obtained with
a behavior policy (epsilon greedy in DQN) different from
the online policy that is being learned.
Another key ingredient behind the success of DQN is expe-
rience replay (Lin, 1993; Mnih et al., 2015). During learn-
ing, the agent accumulates a dataset Dt = {e1, e2, . . . , et}
of experiences et = (st, at, rt, st+1) from many episodes.
When training the Q-network, instead only using the
current experience as prescribed by standard temporal-
difference learning, the network is trained by sampling
mini-batches of experiences from D uniformly at random.
The sequence of losses thus takes the form
Li(θi) = E(s,a,r,s′)∼U(D)
[(
yDQNi −Q(s, a; θi)
)2]
.
Experience replay increases data efficiency through re-use
of experience samples in multiple updates and, importantly,
it reduces variance as uniform sampling from the replay
buffer reduces the correlation among the samples used in
the update.
2.2. Double Deep Q-networks
The previous section described the main components of
DQN as presented in (Mnih et al., 2015). In this paper,
Dueling Network Architectures for Deep Reinforcement Learning
we use the improved Double DQN (DDQN) learning al-
gorithm of van Hasselt et al. (2015). In Q-learning and
DQN, the max operator uses the same values to both select
and evaluate an action. This can therefore lead to overopti-
mistic value estimates (van Hasselt, 2010). To mitigate this
problem, DDQN uses the following target:
yDDQNi = r + γQ(s
′, argmax
a′
Q(s′, a′; θi); θ
−). (6)
DDQN is the same as for DQN (see Mnih et al. (2015)), but
with the target yDQNi replaced by y
DDQN
i . The pseudo-
code for DDQN is presented in Appendix A.
2.3. Prioritized Replay
A recent innovation in prioritized experience re-
play (Schaul et al., 2016) built on top of DDQN and
further improved the state-of-the-art. Their key idea was
to increase the replay probability of experience tuples
that have a high expected learning progress (as measured
via the proxy of absolute TD-error). This led to both
faster learning and to better final policy quality across
most games of the Atari benchmark suite, as compared to
uniform experience replay.
To strengthen the claim that our dueling architecture is
complementary to algorithmic innovations, we show that
it improves performance for both the uniform and the pri-
oritized replay baselines (for which we picked the easier
to implement rank-based variant), with the resulting priori-
tized dueling variant holding the new state-of-the-art.
3. The Dueling Network Architecture
The key insight behind our new architecture, as illustrated
in Figure 2, is that for many states, it is unnecessary to es-
timate the value of each action choice. For example, in
the Enduro game setting, knowing whether to move left or
right only matters when a collision is eminent. In some
states, it is of paramount importance to know which action
to take, but in many other states the choice of action has no
repercussion on what happens. For bootstrapping based al-
gorithms, however, the estimation of state values is of great
importance for every state.
To bring this insight to fruition, we design a single Q-
network architecture, as illustrated in Figure 1, which we
refer to as the dueling network. The lower layers of the
dueling network are convolutional as in the original DQNs
(Mnih et al., 2015). However, instead of following the con-
volutional layers with a single sequence of fully connected
layers, we instead use two sequences (or streams) of fully
connected layers. The streams are constructed such that
they have they have the capability of providing separate es-
timates of the value and advantage functions. Finally, the
two streams are combined to produce a single output Q
function. As in (Mnih et al., 2015), the output of the net-
work is a set of Q values, one for each action.
Since the output of the dueling network is a Q function,
it can be trained with the many existing algorithms, such
as DDQN and SARSA. In addition, it can take advantage
of any improvements to these algorithms, including better
replay memories, better exploration policies, intrinsic mo-
tivation, and so on.
The module that combines the two streams of fully-
connected layers to output a Q estimate requires very
thoughtful design.
From the expressions for advantage Qπ(s, a) = V π(s) +
Aπ(s, a) and state-value V π(s) = Ea∼π(s) [Qπ(s, a)], it
follows that Ea∼π(s) [Aπ(s, a)] = 0. Moreover, for a de-
terministic policy, a∗ = argmaxa′∈AQ(s, a
′), it follows
that Q(s, a∗) = V (s) and hence A(s, a∗) = 0.
Let us consider the dueling network shown in Figure 1,
where we make one stream of fully-connected layers out-
put a scalar V (s; θ, β), and the other stream output an |A|-
dimensional vector A(s, a; θ, α). Here, θ denotes the pa-
rameters of the convolutional layers, while α and β are the
parameters of the two streams of fully-connected layers.
Using the definition of advantage, we might be tempted to
construct the aggregating module as follows:
Q(s, a; θ, α, β) = V (s; θ, β) +A(s, a; θ, α), (7)
Note that this expression applies to all (s, a) instances; that
is, to express equation (7) in matrix form we need to repli-
cate the scalar, V (s; θ, β), |A| times.
However, we need to keep in mind that Q(s, a; θ, α, β)
is only a parameterized estimate of the true Q-function.
Moreover, it would be wrong to conclude that V (s; θ, β)
is a good estimator of the state-value function, or likewise
that A(s, a; θ, α) provides a reasonable estimate of the ad-
vantage function.
Equation (7) is unidentifiable in the sense that given Q
we cannot recover V and A uniquely. To see this, add a
constant to V (s; θ, β) and subtract the same constant from
A(s, a; θ, α). This constant cancels out resulting in the
same Q value. This lack of identifiability is mirrored by
poor practical performance when this equation is used di-
rectly.
To address this issue of identifiability, we can force the ad-
vantage function estimator to have zero advantage at the
chosen action. That is, we let the last module of the net-
work implement the forward mapping
Q(s, a; θ, α, β) = V (s; θ, β) +(
A(s, a; θ, α)− max
a′∈|A|
A(s, a′; θ, α)
)
. (8)
Dueling Network Architectures for Deep Reinforcement Learning
Now, for a∗ = argmaxa′∈AQ(s, a
′; θ, α, β) =
argmaxa′∈AA(s, a
′; θ, α), we obtain Q(s, a∗; θ, α, β) =
V (s; θ, β). Hence, the stream V (s; θ, β) provides an esti-
mate of the value function, while the other stream produces
an estimate of the advantage function.
An alternative module replaces the max operator with an
average:
Q(s, a; θ, α, β) = V (s; θ, β) +(
A(s, a; θ, α)− 1
|A|
∑
a′
A(s, a′; θ, α)
)
. (9)
On the one hand this loses the original semantics of V and
A because they are now off-target by a constant, but on
the other hand it increases the stability of the optimization:
with (9) the advantages only need to change as fast as the
mean, instead of having to compensate any change to the
optimal action’s advantage in (8). We also experimented
with a softmax version of equation (8), but found it to de-
liver similar results to the simpler module of equation (9).
Hence, all the experiments reported in this paper use the
module of equation (9).
Note that while subtracting the mean in equation (9) helps
with identifiability, it does not change the relative rank of
the A (and hence Q) values, preserving any greedy or -
greedy policy based on Q values from equation (7). When
acting, it suffices to evaluate the advantage stream to make
decisions.
It is important to note that equation (9) is viewed and im-
plemented as part of the network and not as a separate algo-
rithmic step. Training of the dueling architectures, as with
standard Q networks (e.g. the deep Q-network of Mnih
et al. (2015)), requires only back-propagation. The esti-
mates V (s; θ, β) and A(s, a; θ, α) are computed automati-
cally without any extra supervision or algorithmic modifi-
cations.
As the dueling architecture shares the same input-output in-
terface with standard Q networks, we can recycle all learn-
ing algorithms with Q networks (e.g., DDQN and SARSA)
to train the dueling architecture.
4. Experiments
We now show the practical performance of the dueling net-
work. We start with a simple policy evaluation task and
then show larger scale results for learning policies for gen-
eral Atari game-playing.
4.1. Policy evaluation
We start by measuring the performance of the dueling ar-
chitecture on a policy evaluation task. We choose this par-
ticular task because it is very useful for evaluating network
architectures, as it is devoid of confounding factors such as
the choice of exploration strategy, and the interaction be-
tween policy improvement and policy evaluation.
In this experiment, we employ temporal difference learning
(without eligibility traces, i.e., λ = 0) to learn Q values.
More specifically, given a behavior policy π, we seek to
estimate the state-action value Qπ(·, ·) by optimizing the
sequence of costs of equation (4), with target
yi = r + γEa′∼π(s′) [Q(s′, a′; θi)] .
The above update rule is the same as that of Expected
SARSA (van Seijen et al., 2009). We, however, do not
modify the behavior policy as in Expected SARSA.
To evaluate the learned Q values, we choose a simple envi-
ronment where the exact Qπ(s, a) values can be computed
separately for all (s, a) ∈ S ×A. This environment, which
we call the corridor is composed of three connected cor-
ridors. A schematic drawing of the corridor environment
is shown in Figure 3, The agent starts from the bottom left
corner of the environment and must move to the top right
to get the largest reward. A total of 5 actions are available:
go up, down, left, right and no-op. We also have the free-
dom of adding an arbitrary number of no-op actions. In our
setup, the two vertical sections both have 10 states while
the horizontal section has 50.
We use an -greedy policy as the behavior policy π, which
chooses a random action with probability  or an action
according to the optimal Q function argmaxa∈AQ
∗(s, a)
with probability 1 − . In our experiments,  is chosen to
be 0.001.
We compare a single-stream Q architecture with the duel-
ing architecture on three variants of the corridor environ-
ment with 5, 10 and 20 actions respectively. The 10 and 20
action variants are formed by adding no-ops to the original
environment. We measure performance by Squared Error
(SE) against the true state values:
∑
s∈S,a∈A(Q(s, a; θ)−
Qπ(s, a))2. The single-stream architecture is a three layer
MLP with 50 units on each hidden layer. The dueling ar-
chitecture is also composed of three layers. After the first
hidden layer of 50 units, however, the network branches off
into two streams each of them a two layer MLP with 25 hid-
den units. The results of the comparison are summarized in
Figure 3.
The results show that with 5 actions, both architectures
converge at about the same speed. However, when we in-
crease the number of actions, the dueling architecture per-
forms better than the traditional Q-network. In the dueling
network, the stream V (s; θ, β) learns a general value that
is shared across many similar actions at s, hence leading
to faster convergence. This is a very promising result be-
Dueling Network Architectures for Deep Reinforcement Learning
CORRIDOR ENVIRONMENT 5 ACTIONS 10 ACTIONS 20 ACTIONS
103 104
No. Iterations
100
101
102
103
S
E
Single
Duel
103 104
No. Iterations
100
101
102
103
103 104
No. Iterations
100
101
102
103
(a) (b) (c) (d)
Figure 3. (a) The corridor environment. The star marks the starting state. The redness of a state signifies the reward the agent receives
upon arrival. The game terminates upon reaching either reward state. The agent’s actions are going up, down, left, right and no action.
Plots (b), (c) and (d) shows squared error for policy evaluation with 5, 10, and 20 actions on a log-log scale. The dueling network
(Duel) consistently outperforms a conventional single-stream network (Single), with the performance gap increasing with the number of
actions.
cause many control tasks with large action spaces have this
property, and consequently we should expect that the du-
eling network will often lead to much faster convergence
than a traditional single stream network. In the following
section, we will indeed see that the dueling network results
in substantial gains in performance in a wide-range of Atari
games.
4.2. General Atari Game-Playing
We perform a comprehensive evaluation of our proposed
method on the Arcade Learning Environment (Bellemare
et al., 2013), which is composed of 57 Atari games. The
challenge is to deploy a single algorithm and architecture,
with a fixed set of hyper-parameters, to learn to play all
the games given only raw pixel observations and game re-
wards. This environment is very demanding because it is
both comprised of a large number of highly diverse games
and the observations are high-dimensional.
We follow closely the setup of van Hasselt et al. (2015) and
compare to their results using single-stream Q-networks.
We train the dueling network with the DDQN algorithm
as presented in Appendix A. At the end of this section,
we incorporate prioritized experience replay (Schaul et al.,
2016).
Our network architecture has the same low-level convolu-
tional structure of DQN (Mnih et al., 2015; van Hasselt
et al., 2015). There are 3 convolutional layers followed by
2 fully-connected layers. The first convolutional layer has
32 8×8 filters with stride 4, the second 64 4×4 filters with
stride 2, and the third and final convolutional layer consists
64 3 × 3 filters with stride 1. As shown in Figure 1, the
dueling network splits into two streams of fully connected
layers. The value and advantage streams both have a fully-
connected layer with 512 units. The final hidden layers of
the value and advantage streams are both fully-connected
with the value stream having one output and the advantage
as many outputs as there are valid actions2. We combine the
value and advantage streams using the module described by
Equation (9). Rectifier non-linearities (Fukushima, 1980)
are inserted between all adjacent layers.
We adopt the optimizers and hyper-parameters of van Has-
selt et al. (2015), with the exception of the learning rate
which we chose to be slightly lower (we do not do this for
double DQN as it can deteriorate its performance). Since
both the advantage and the value stream propagate gradi-
ents to the last convolutional layer in the backward pass,
we rescale the combined gradient entering the last convo-
lutional layer by 1/
√
2. This simple heuristic mildly in-
creases stability. In addition, we clip the gradients to have
their norm less than or equal to 10. This clipping is not
standard practice in deep RL, but common in recurrent net-
work training (Bengio et al., 2013).
To isolate the contributions of the dueling architecture, we
re-train DDQN with a single stream network using exactly
the same procedure as described above. Specifically, we
apply gradient clipping, and use 1024 hidden units for the
first fully-connected layer of the network so that both archi-
tectures (dueling and single) have roughly the same number
of parameters. We refer to this re-trained model as Single
Clip, while the original trained model of van Hasselt et al.
(2015) is referred to as Single.
As in (van Hasselt et al., 2015), we start the game with up
to 30 no-op actions to provide random starting positions for
the agent. To evaluate our approach, we measure improve-
ment in percentage (positive or negative) in score over the
better of human and baseline agent scores:
ScoreAgent − ScoreBaseline
max{ScoreHuman, ScoreBaseline} − ScoreRandom
. (10)
We took the maximum over human and baseline agent
scores as it prevents insignificant changes to appear as
2The number of actions ranges between 3-18 actions in the
ALE environment.
Dueling Network Architectures for Deep Reinforcement Learning
Freeway
Video Pinball
Breakout
Assault
Beam Rider
Solaris
James Bond
Tutankham
Bowling
Private Eye
Montezuma's Revenge
Pong
Robotank
Pitfall!
Skiing
H.E.R.O.
Asteroids
Demon Attack
Wizard Of Wor
Gravitar
Gopher
Boxing
Berzerk
Alien
Kangaroo
Kung-Fu Master
Battle Zone
Name This Game
Defender
Centipede
Crazy Climber
Ice Hockey
Zaxxon
Q*Bert
Fishing Derby
Amidar
Venture
River Raid
Double Dunk
Surround 
Star Gunner
Ms. Pac-Man
Krull
Bank Heist
Road Runner
Asterix
Time Pilot
Frostbite
Yars' Revenge
Seaquest
Chopper Command
Enduro
Phoenix
Up and Down
Space Invaders
Tennis
Atlantis
-100.00%
-68.31%
-17.56%
-14.93%
-9.71%
-7.37%
-3.42%
-3.38%
-1.89%
-0.04%
0.00%
0.24%
0.32%
0.45%
1.29%
2.31%
4.51%
4.78%
5.24%
5.54%
6.02%
8.52%
9.86%
10.34%
14.39%
15.56%
15.65%
16.28%
21.18%
21.68%
24.68%
26.45%
27.45%
27.68%
28.82%
31.40%
33.60%
39.79%
42.75%
44.24%
48.92%
53.76%
55.85%
57.19%
57.57%
63.17%
69.73%
70.02%
73.63%
80.51%
82.20%
86.35%
94.33%
97.90%
164.11%
180.00%
296.67%
Figure 4. Improvements of dueling architecture over the baseline
Single network of van Hasselt et al. (2015), using the metric de-
scribed in Equation (10). Bars to the right indicate by how much
the dueling network outperforms the single-stream network.
large improvements when neither the agent in question nor
the baseline are doing well. For example, an agent that
achieves 2% human performance should not be interpreted
as two times better when the baseline agent achieves 1%
human performance. We also chose not to measure perfor-
mance in terms of percentage of human performance alone
because a tiny difference relative to the baseline on some
games can translate into hundreds of percent in human per-
formance difference.
The results for the wide suite of 57 games are summarized
in Table 1. Detailed results are presented in the Appendix.
Using this 30 no-ops performance measure, it is clear that
the dueling network (Duel Clip) does substantially better
than the Single Clip network of similar capacity. It also
does considerably better than the baseline (Single) of van
Hasselt et al. (2015). For comparison we also show results
for the deepQ-network of Mnih et al. (2015), referred to as
Nature DQN.
Figure 4 shows the improvement of the dueling network
over the baseline Single network of van Hasselt et al.
(2015). Again, we seen that the improvements are often
very dramatic.
As shown in Table 1, Single Clip performs better than Sin-
gle. We verified that this gain was mostly brought in by
gradient clipping. For this reason, we incorporate gradient
clipping in all the new approaches.
Table 1. Mean and median scores across all 57 Atari games, mea-
sured in percentages of human performance.
30 no-ops Human Starts
Mean Median Mean Median
Prior. Duel Clip 591.9% 172.1% 567.0% 115.3%
Prior. Single 434.6% 123.7% 386.7% 112.9%
Duel Clip 373.1% 151.5% 343.8% 117.1%
Single Clip 341.2% 132.6% 302.8% 114.1%
Single 307.3% 117.8% 332.9% 110.9%
Nature DQN 227.9% 79.1% 219.6% 68.5%
Duel Clip does better than Single Clip on 75.4% of the
games (43 out of 57). It also achieves higher scores com-
pared to the Single baseline on 80.7% (46 out of 57) of the
games. Of all the games with 18 actions, Duel Clip is better
86.6% of the time (26 out of 30). This is consistent with the
findings of the previous section. Overall, our agent (Duel
Clip) achieves human level performance on 42 out of 57
games. Raw scores for all the games, as well as measure-
ments in human performance percentage, are presented in
the Appendix.
Robustness to human starts. One shortcoming of the 30
no-ops metric is that an agent does not necessarily have to
generalize well to play the Atari games. Due to the deter-
ministic nature of the Atari environment, from an unique
starting point, an agent could learn to achieve good perfor-
mance by simply remembering sequences of actions.
To obtain a more robust measure, we adopt the methodol-
ogy of Nair et al. (2015). Specifically, for each game, we
use 100 starting points sampled from a human expert’s tra-
jectory. From each of these points, an evaluation episode
is launched for up to 108,000 frames. The agents are eval-
uated only on rewards accrued after the starting point. We
refer to this metric as Human Starts.
As shown in Table 1, under the Human Starts metric, Duel
Clip once again outperforms the single stream variants. In
particular, our agent does better than the Single baseline on
70.2% (40 out of 57) games and on games of 18 actions,
Duel Clip is 83.3% better (25 out of 30).
Combining with Prioritized Experience Replay. The du-
eling architecture can be easily combined with other algo-
rithmic improvements. In particular, prioritization of the
experience replay has been shown to significantly improve
performance of Atari games (Schaul et al., 2016). Further-
more, as prioritization and the dueling architecture address
very different aspects of the learning process, their combi-
nation is promising. So in our final experiment, we inves-
tigate the integration of the dueling architecture with pri-
oritized experience replay. We use the prioritized variant
of DDQN (Prior. Single) as the new baseline algorithm,
which replaces with the uniform sampling of the experi-
Dueling Network Architectures for Deep Reinforcement Learning
Kangaroo
James Bond
Double Dunk
Skiing
Seaquest
Robotank
Ms. Pac-Man
Surround 
Solaris
Time Pilot
Ice Hockey
Gravitar
H.E.R.O.
Alien
Asteroids
Breakout
Freeway
Bowling
Venture
Tennis
Montezuma's Revenge
Private Eye
Pong
Fishing Derby
Demon Attack
Boxing
Pitfall!
Road Runner
Krull
Enduro
Atlantis
Battle Zone
Q*Bert
Crazy Climber
Tutankham
Kung-Fu Master
Amidar
Beam Rider
Centipede
Zaxxon
Name This Game
Defender
River Raid
Bank Heist
Assault
Chopper Command
Video Pinball
Frostbite
Berzerk
Star Gunner
Yars' Revenge
Up and Down
Wizard Of Wor
Gopher
Phoenix
Space Invaders
Asterix 1097.02%
-89.22%
-84.70%
-83.56%
-77.99%
-60.56%
-58.11%
-48.03%
-40.74%
-37.65%
-29.21%
-13.60%
-9.77%
-6.72%
-3.81%
-3.13%
-2.12%
-2.08%
-0.87%
-0.51%
0.00%
0.00%
0.01%
0.73%
1.37%
1.44%
3.46%
5.33%
7.89%
7.95%
10.20%
11.16%
11.46%
15.56%
16.16%
21.38%
22.36%
24.98%
29.94%
32.48%
32.74%
33.09%
35.33%
38.56%
43.11%
51.07%
58.87%
69.92%
70.29%
83.91%
98.69%
113.16%
113.47%
178.13%
223.03%
281.56%
457.93%
Figure 5. Improvements of dueling architecture over Prioritized
DDQN baseline, using the same metric as Figure 4. Again, the
dueling architecture leads to significant improvements over the
single-stream baseline on the majority of games.
ence tuples by rank-based prioritized sampling. We keep
all the parameters of the prioritized replay as described
in (Schaul et al., 2016), namely a priority exponent of 0.7,
and an annealing schedule on the importance sampling ex-
ponent from 0.5 to 1. We combine this baseline with our
dueling architecture (as above), and again use gradient clip-
ping (Prior. Duel Clip).
Note that, although orthogonal in their objectives, these
extensions (prioritization, dueling and gradient clipping)
interact in subtle ways. For example, prioritization inter-
acts with gradient clipping, as sampling transitions with
high absolute TD-errors more often leads to gradients with
higher norms. To avoid adverse interactions, we roughly
re-tuned the learning rate and the gradient clipping norm on
a subset of 9 games. As a result of rough tuning, we settled
on 6.25×10−5 for the learning rate and 10 for the gradient
clipping norm (the same as in the previous section).
When evaluated on all 57 Atari games, our prioritized du-
eling agent performs significantly better than both the pri-
oritized baseline agent and the dueling agent alone. The
full mean and median performance against the human per-
formance percentage is shown in Table 1. When initializ-
ing the games using up to 30 no-ops action, we observe
mean and median scores of 591% and 172% respectively.
The direct comparison between the prioritized baseline and
prioritized dueling versions, using the metric described in
Equation 10, is presented in Figure 5.
The combination of prioritized replay and the dueling net-
work results in vast improvements over the previous state-
of-the-art in the popular ALE benchmark.
Saliency maps. To better understand the roles of the value
and the advantage streams, we compute saliency maps (Si-
monyan et al., 2013). More specifically, to visualize the
salient part of the image as seen by the value stream, we
compute the absolute value of the Jacobian of V̂ with re-
spect to the input frames: |∇sV̂ (s; θ)|. Similarly, to visu-
alize the salient part of the image as seen by the advan-
tage stream, we compute |∇sÂ(s, argmaxa′ Â(s, a′); θ)|.
Both quantities are of the same dimensionality as the input
frames and therefore can be visualized easily alongside the
input frames.
Here, we place the gray scale input frames in the green and
blue channel and the saliency maps in the red channel. All
three channels together form an RGB image. Figure 2 de-
picts the value and advantage saliency maps on the Enduro
game for two different time steps. As observed in the in-
troduction, the value stream pays attention to the horizon
where the appearance of a car could affect future perfor-
mance. The value stream also pays attention to the score.
The advantage stream, on the other hand, cares more about
cars that are on an immediate collision course.
5. Discussion
The advantage of the dueling architecture lies partly in its
ability to learn the state-value function efficiently. With
every update of the Q values in the dueling architecture,
the value stream V is updated – this contrasts with the up-
dates in a single-stream architecture where only the value
for one of the actions is updated, the values for all other
actions remain untouched. This more frequent updating of
the value stream in our approach allocates more resources
to V , and thus allows for better approximation of the state
values, which in turn need to be accurate for temporal-
difference-based methods like Q-learning to work (Sutton
& Barto, 1998). This phenomenon is reflected in the ex-
periments, where the advantage of the dueling architecture
over single-stream Q networks grows when the number of
actions is large.
Furthermore, the differences between Q-values for a given
state are often very small relative to the magnitude of Q.
For example, after training with DDQN on the game of
Seaquest, the average action gap (the gap between the Q
values of the best and the second best action in a given
state) across visited states is roughly 0.04, whereas the av-
erage state value across those states is about 15. This differ-
ence in scales can lead to small amounts of noise in the up-
dates can lead to reorderings of the actions, and thus make
the nearly greedy policy switch abruptly. The dueling ar-
Dueling Network Architectures for Deep Reinforcement Learning
chitecture with its separate advantage stream is robust to
such effects.
6. Conclusions
We introduced a new neural network architecture that de-
couples value and advantage in deep Q-networks, while
sharing a common feature learning module. The new duel-
ing architecture, in combination with some algorithmic im-
provements, leads to dramatic improvements over existing
approaches for deep RL in the challenging Atari domain.
The results presented in this paper are the new state-of-the-
art in this popular domain.
References
Ba, J., Mnih, V., and Kavukcuoglu, K. Multiple object
recognition with visual attention. In ICLR, 2015.
Baird, L.C. Advantage updating. Technical Report WL-
TR-93-1146, Wright-Patterson Air Force Base, 1993.
Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
The arcade learning environment: An evaluation plat-
form for general agents. Journal of Artificial Intelligence
Research, 47:253–279, 2013.
Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S.,
and Munos, R. Increasing the action gap: New operators
for reinforcement learning. In AAAI, 2016. To appear.
Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R.
Advances in optimizing recurrent networks. In ICASSP,
pp. 8624–8628, 2013.
Fukushima, K. Neocognitron: A self-organizing neural
network model for a mechanism of pattern recognition
unaffected by shift in position. Biological Cybernetics,
36:193–202, 1980.
Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X.
Deep learning for real-time Atari game play using offline
Monte-Carlo tree search planning. In NIPS, pp. 3338–
3346. 2014.
Harmon, M.E. and Baird, L.C. Multi-player residual ad-
vantage learning with general function approximation.
Technical Report WL-TR-1065, Wright-Patterson Air
Force Base, 1996.
Harmon, M.E., Baird, L.C., and Klopf, A.H. Advantage
updating applied to a differential game. In G. Tesauro,
D.S. Touretzky and Leen, T.K. (eds.), NIPS, 1995.
LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-
ture, 521(7553):436–444, 2015.
Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-
end training of deep visuomotor policies. arXiv preprint
arXiv:1504.00702, 2015.
Lin, L.J. Reinforcement learning for robots using neu-
ral networks. PhD thesis, School of Computer Science,
Carnegie Mellon University, 1993.
Maddison, C. J., Huang, A., Sutskever, I., and Silver, D.
Move Evaluation in Go Using Deep Convolutional Neu-
ral Networks. In ICLR, 2015.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-
ness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,
Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-
stra, D., Legg, S., and Hassabis, D. Human-level con-
trol through deep reinforcement learning. Nature, 518
(7540):529–533, 2015.
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C.,
Fearon, R., Maria, A. De, Panneershelvam, V., Suley-
man, M., Beattie, C., Petersen, S., Legg, S., Mnih,
V., Kavukcuoglu, K., and Silver, D. Massively paral-
lel methods for deep reinforcement learning. In Deep
Learning Workshop, ICML, 2015.
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-
tized experience replay. In ICLR, 2016.
Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and
Abbeel, P. High-dimensional continuous control us-
ing generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015.
Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L.,
van den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,
D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
T., Leach, M., Kavukcuoglu, K., Graepel, T., and Has-
sabis, D. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484–489,
01 2016.
Simonyan, K., Vedaldi, A., and Zisserman, A. Deep in-
side convolutional networks: Visualising image clas-
sification models and saliency maps. arXiv preprint
arXiv:1312.6034, 2013.
Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing ex-
ploration in reinforcement learning with deep predictive
models. arXiv preprint arXiv:1507.00814, 2015.
Sutton, R. S. and Barto, A. G. Introduction to reinforce-
ment learning. MIT Press, 1998.
Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y.
Policy gradient methods for reinforcement learning with
function approximation. In NIPS, pp. 1057–1063, 2000.
van Hasselt, H. Double Q-learning. NIPS, 23:2613–2621,
2010.
van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
ment learning with double Q-learning. arXiv preprint
Dueling Network Architectures for Deep Reinforcement Learning
arXiv:1509.06461, 2015.
van Seijen, H., van Hasselt, H., Whiteson, S., and Wier-
ing, M. A theoretical and empirical analysis of Expected
Sarsa. In IEEE Symposium on Adaptive Dynamic Pro-
gramming and Reinforcement Learning, pp. 177–184.
2009.
Watter, M., Springenberg, J. T., Boedecker, J., and Ried-
miller, M. A. Embed to control: A locally linear latent
dynamics model for control from raw images. In NIPS,
2015.
Dueling Network Architectures for Deep Reinforcement Learning
A. Double DQN Algorithm
Algorithm 1: Double DQN Algorithm.
input : D – empty replay buffer; θ – initial network parameters, θ− – copy of θ
input : Nr – replay buffer maximum size; Nb – training batch size; N− – target network replacement freq.
for episode e ∈ {1, 2, . . . ,M } do
Initialize frame sequence x← ()
for t ∈ {0, 1, . . .} do
Set state s← x, sample action a ∼ πB
Sample next frame xt from environment E given (s, a) and receive reward r, and append xt to x
if |x| > Nf then delete oldest frame xtmin from x end
Set s′ ← x, and add transition tuple (s, a, r, s′) to D,
replacing the oldest tuple if |D| ≥ Nr
Sample a minibatch of Nb tuples (s, a, r, s′) ∼ Unif(D)
Construct target values, one for each of the Nb tuples:
Define amax (s′; θ) = argmaxa′ Q(s
′, a′; θ)
yj =
{
r if s′ is terminal
r + γQ(s′, amax (s′; θ); θ−), otherwise.
Do a gradient descent step with loss ‖yj −Q(s, a; θ)‖2
Replace target parameters θ− ← θ every N− steps
end
end
Dueling Network Architectures for Deep Reinforcement Learning
Table 2. Raw scores across all games. Starting with 30 no-op actions.
GAMES NO. ACTIONS RANDOM HUMAN DQN DDQN DUEL PRIOR. PRIOR. DUEL.
Alien 18 227.8 7,127.7 1,620.0 3,747.7 4,461.4 4,203.8 3,941.0
Amidar 10 5.8 1,719.5 978.0 1,793.3 2,354.5 1,838.9 2,296.8
Assault 7 222.4 742.0 4,280.4 5,393.2 4,621.0 7,672.1 11,477.0
Asterix 9 210.0 8,503.3 4,359.0 17,356.5 28,188.0 31,527.0 375,080.0
Asteroids 14 719.1 47,388.7 1,364.5 734.7 2,837.7 2,654.3 1,192.7
Atlantis 4 12,850.0 29,028.1 279,987.0 106,056.0 382,572.0 357,324.0 395,762.0
Bank Heist 18 14.2 753.1 455.0 1,030.6 1,611.9 1,054.6 1,503.1
Battle Zone 18 2,360.0 37,187.5 29,900.0 31,700.0 37,150.0 31,530.0 35,520.0
Beam Rider 9 363.9 16,926.5 8,627.5 13,772.8 12,164.0 23,384.2 30,276.5
Berzerk 18 123.7 2,630.4 585.6 1,225.4 1,472.6 1,305.6 3,409.0
Bowling 6 23.1 160.7 50.4 68.1 65.5 47.9 46.7
Boxing 18 0.1 12.1 88.0 91.6 99.4 95.6 98.9
Breakout 4 1.7 30.5 385.5 418.5 345.3 373.9 366.0
Centipede 18 2,090.9 12,017.0 4,657.7 5,409.4 7,561.4 4,463.2 7,687.5
Chopper Command 18 811.0 7,387.8 6,126.0 5,809.0 11,215.0 8,600.0 13,185.0
Crazy Climber 9 10,780.5 35,829.4 110,763.0 117,282.0 143,570.0 141,161.0 162,224.0
Defender 18 2,874.5 18,688.9 23,633.0 35,338.5 42,214.0 31,286.5 41,324.5
Demon Attack 6 152.1 1,971.0 12,149.4 58,044.2 60,813.3 71,846.4 72,878.6
Double Dunk 18 -18.6 -16.4 -6.6 -5.5 0.1 18.5 -12.5
Enduro 9 0.0 860.5 729.0 1,211.8 2,258.2 2,093.0 2,306.4
Fishing Derby 18 -91.7 -38.7 -4.9 15.5 46.4 39.5 41.3
Freeway 3 0.0 29.6 30.8 33.3 0.0 33.7 33.0
Frostbite 18 65.2 4,334.7 797.4 1,683.3 4,672.8 4,380.1 7,413.0
Gopher 8 257.6 2,412.5 8,777.4 14,840.8 15,718.4 32,487.2 104,368.2
Gravitar 18 173.0 3,351.4 473.0 412.0 588.0 548.5 238.0
H.E.R.O. 18 1,027.0 30,826.4 20,437.8 20,130.2 20,818.2 23,037.7 21,036.5
Ice Hockey 18 -11.2 0.9 -1.9 -2.7 0.5 1.3 -0.4
James Bond 18 29.0 302.8 768.5 1,358.0 1,312.5 5,148.0 812.0
Kangaroo 18 52.0 3,035.0 7,259.0 12,992.0 14,854.0 16,200.0 1,792.0
Krull 18 1,598.0 2,665.5 8,422.3 7,920.5 11,451.9 9,728.0 10,374.4
Kung-Fu Master 14 258.5 22,736.3 26,059.0 29,710.0 34,294.0 39,581.0 48,375.0
Montezuma’s Revenge 18 0.0 4,753.3 0.0 0.0 0.0 0.0 0.0
Ms. Pac-Man 9 307.3 6,951.6 3,085.6 2,711.4 6,283.5 6,518.7 3,327.3
Name This Game 6 2,292.3 8,049.0 8,207.8 10,616.0 11,971.1 12,270.5 15,572.5
Phoenix 8 761.4 7,242.6 8,485.2 12,252.5 23,092.2 18,992.7 70,324.3
Pitfall! 18 -229.4 6,463.7 -286.1 -29.9 0.0 -356.5 0.0
Pong 3 -20.7 14.6 19.5 20.9 21.0 20.6 20.9
Private Eye 18 24.9 69,571.3 146.7 129.7 103.0 200.0 206.0
Q*Bert 6 163.9 13,455.0 13,117.3 15,088.5 19,220.3 16,256.5 18,760.3
River Raid 18 1,338.5 17,118.0 7,377.6 14,884.5 21,162.6 14,522.3 20,607.6
Road Runner 18 11.5 7,845.0 39,544.0 44,127.0 69,524.0 57,608.0 62,151.0
Robotank 18 2.2 11.9 63.9 65.1 65.3 62.6 27.5
Seaquest 18 68.4 42,054.7 5,860.6 16,452.7 50,254.2 26,357.8 931.6
Skiing 3 -17,098.1 -4,336.9 -13,062.3 -9,021.8 -8,857.4 -9,996.9 -19,949.9
Solaris 18 1,236.3 12,326.7 3,482.8 3,067.8 2,250.8 4,309.0 133.4
Space Invaders 6 148.0 1,668.7 1,692.3 2,525.5 6,427.3 2,865.8 15,311.5
Star Gunner 18 664.0 10,250.0 54,282.0 60,142.0 89,238.0 63,302.0 125,117.0
Surround 5 -10.0 6.5 -5.6 -2.9 4.4 8.9 1.2
Tennis 18 -23.8 -8.3 12.2 -22.8 5.1 0.0 0.0
Time Pilot 10 3,568.0 5,229.2 4,870.0 8,339.0 11,666.0 9,197.0 7,553.0
Tutankham 8 11.4 167.6 68.1 218.4 211.4 204.6 245.9
Up and Down 6 533.4 11,693.2 9,989.9 22,972.2 44,939.6 16,154.1 33,879.1
Venture 18 0.0 1,187.5 163.0 98.0 497.0 54.0 48.0
Video Pinball 9 16,256.9 17,667.9 196,760.4 309,941.9 98,209.5 282,007.3 479,197.0
Wizard Of Wor 10 563.5 4,756.5 2,704.0 7,492.0 7,855.0 4,802.0 12,352.0
Yars’ Revenge 18 3,092.9 54,576.9 18,098.9 11,712.6 49,622.1 11,357.0 69,618.1
Zaxxon 18 32.5 9,173.3 5,363.0 10,163.0 12,944.0 10,469.0 13,886.0
Dueling Network Architectures for Deep Reinforcement Learning
Table 3. Raw scores across all games. Starting with Human starts.
GAMES NO. ACTIONS RANDOM HUMAN DQN DDQN DUEL PRIOR. PRIOR. DUEL.
Alien 18 128.3 6,371.3 634.0 1,033.4 1,486.5 1,334.7 823.7
Amidar 10 11.8 1,540.4 178.4 169.1 172.7 129.1 238.4
Assault 7 166.9 628.9 3,489.3 6,060.8 3,994.8 6,548.9 10,950.6
Asterix 9 164.5 7,536.0 3,170.5 16,837.0 15,840.0 22,484.5 364,200.0
Asteroids 14 871.3 36,517.3 1,458.7 1,193.2 2,035.4 1,745.1 1,021.9
Atlantis 4 13,463.0 26,575.0 292,491.0 319,688.0 445,360.0 330,647.0 423,252.0
Bank Heist 18 21.7 644.5 312.7 886.0 1,129.3 876.6 1,004.6
Battle Zone 18 3,560.0 33,030.0 23,750.0 24,740.0 31,320.0 25,520.0 30,650.0
Beam Rider 9 254.6 14,961.0 9,743.2 17,417.2 14,591.3 31,181.3 37,412.2
Berzerk 18 196.1 2,237.5 493.4 1,011.1 910.6 865.9 2,178.6
Bowling 6 35.2 146.5 56.5 69.6 65.7 52.0 50.4
Boxing 18 -1.5 9.6 70.3 73.5 77.3 72.3 79.2
Breakout 4 1.6 27.9 354.5 368.9 411.6 343.0 354.6
Centi